{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ij64Arie4Pc"
   },
   "source": [
    "# 10 - Introduction to Machine Translation\n",
    "Prepared by Jan Christian Blaise Cruz\n",
    "\n",
    "DLSU Machine Learning Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMvPAJrUfNrk"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oQtAeqoe9R3"
   },
   "source": [
    "First, let's make sure that we have an active GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "2VJ9VF_nsdzk",
    "outputId": "2486dbe3-521b-4e6a-c723-c530c07ec0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  3 11:58:07 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kbAJmEKfAay"
   },
   "source": [
    "Then we'll download the Flickr Multi30k dataset, as well as some tokenizer models from Spacy. After you do this, make sure to **restart the runtime** by clicking Runtime > Restart Runtime in the menu bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Birxagpss2UJ"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/translation/multi30k.zip\n",
    "!unzip multi30k.zip && rm multi30k.zip\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgYnTmOMfL_I"
   },
   "source": [
    "Let's include our standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erLqo1e-vzpr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as datautils\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F90hDE5KfS1L"
   },
   "source": [
    "Then load the dataset. There's no need to shuffle and split as the dataset already has predefined training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGMWVZMRuABB"
   },
   "outputs": [],
   "source": [
    "with open('multi30k/train.en', 'r') as f:\n",
    "    train_en = [line.strip() for line in f]\n",
    "with open('multi30k/train.de', 'r') as f:\n",
    "    train_de = [line.strip() for line in f]\n",
    "with open('multi30k/val.en', 'r') as f:\n",
    "    valid_en = [line.strip() for line in f]\n",
    "with open('multi30k/val.de', 'r') as f:\n",
    "    valid_de = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFyjhZ6RfYeo"
   },
   "source": [
    "We'll tokenize our data. We'll add in a start of sequence and an end of sequence token. This will make it easier for the model later to learn when to stop generating tokens for the translations. We'll also make everything lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "PH6sf85ixBiM",
    "outputId": "39c4ef2d-a20b-4bdc-c030-4055b6c84b8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29000/29000 [00:01<00:00, 15642.70it/s]\n",
      "100%|██████████| 29000/29000 [00:02<00:00, 10186.42it/s]\n",
      "100%|██████████| 1014/1014 [00:00<00:00, 20026.39it/s]\n",
      "100%|██████████| 1014/1014 [00:00<00:00, 11979.41it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return ['<sos>'] + [tok.text.lower() for tok in spacy_de.tokenizer(text)] + ['<eos>']\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return ['<sos>'] + [tok.text.lower() for tok in spacy_en.tokenizer(text)] + ['<eos>']\n",
    "\n",
    "# Tokenize the text\n",
    "train_en = [tokenize_en(text) for text in tqdm(train_en)]\n",
    "train_de = [tokenize_de(text) for text in tqdm(train_de)]\n",
    "valid_en = [tokenize_en(text) for text in tqdm(valid_en)]\n",
    "valid_de = [tokenize_de(text) for text in tqdm(valid_de)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2xVJoIyfkFI"
   },
   "source": [
    "Next up, we'll pad and cut the samples in the dataset. The maximum sequence length is the largest length in the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzOGXZGa0esD"
   },
   "outputs": [],
   "source": [
    "def process(dataset):\n",
    "    max_len = max([len(text) for text in dataset])\n",
    "    temp = []\n",
    "    for text in dataset:\n",
    "        if len(text) < max_len:\n",
    "            text += ['<pad>' for _ in range(max_len - len(text))]\n",
    "        temp.append(text)\n",
    "    return temp\n",
    "\n",
    "# Pad to maximum length of the dataset\n",
    "train_en_proc, valid_en_proc = process(train_en), process(valid_en)\n",
    "train_de_proc, valid_de_proc = process(train_de), process(valid_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvbZglfQfvBd"
   },
   "source": [
    "Here's the first training example in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "x8xEN4IJ0jkS",
    "outputId": "f7a23f4e-651e-417d-b7ce-414a005e0e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(train_en_proc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BwP-Ul2fxZt"
   },
   "source": [
    "And it's corresponding sentence in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YYuCfTwE2adv",
    "outputId": "14a1091e-d104-4840-85fe-8b1409edcb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(train_de_proc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-eQjk68f0a6"
   },
   "source": [
    "Our vocabulary generation scheme remains largely the same, but with some minor improvements. We'll use ```collections.Counter``` to get token frequency counts and remove rare tokens. This ensures that the vocabulary and our embeddings don't become too sparse.\n",
    "\n",
    "We'll generate two sets of vocabularies and index-word converters: one for English and one of German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4m4MSByk3APp"
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset, min_freq=2):\n",
    "    # Add all tokens to the list\n",
    "    special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "    vocab = []\n",
    "    for line in dataset: vocab.extend(line)\n",
    "\n",
    "    # Remove words that are below the minimum frequency, the enforce set\n",
    "    counts = Counter(vocab)\n",
    "    vocab = special_tokens + [word for word in counts.keys() if counts[word] > min_freq]\n",
    "    vocab_set = set(vocab)\n",
    "\n",
    "    # Push all special tokens to the front\n",
    "    idx2word = list(vocab_set)\n",
    "    for token in special_tokens[::-1]:\n",
    "        idx2word.insert(0, idx2word.pop(idx2word.index(token)))\n",
    "\n",
    "    # Produce word2idx then return\n",
    "    word2idx = {idx2word[i]: i for i in range(len(idx2word))}\n",
    "    return vocab_set, idx2word, word2idx\n",
    "\n",
    "# Get vocabulary and references\n",
    "vocab_set_en, idx2word_en, word2idx_en = get_vocab(train_en_proc, min_freq=2)\n",
    "vocab_set_de, idx2word_de, word2idx_de = get_vocab(train_de_proc, min_freq=2)\n",
    "\n",
    "# Convert unknown tokens\n",
    "train_en_proc = [[token if token in vocab_set_en else '<unk>' for token in line] for line in train_en_proc]\n",
    "train_de_proc = [[token if token in vocab_set_de else '<unk>' for token in line] for line in train_de_proc]\n",
    "valid_en_proc = [[token if token in vocab_set_en else '<unk>' for token in line] for line in valid_en_proc]\n",
    "valid_de_proc = [[token if token in vocab_set_de else '<unk>' for token in line] for line in valid_de_proc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgoLmxvYgG88"
   },
   "source": [
    "Here's the number of words in both vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jD6Gc1sf3RWQ",
    "outputId": "32802730-16e5-4e54-e8ce-75f2cbc66a27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4556, 5376)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_set_en), len(vocab_set_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52XyFCJlgIp5"
   },
   "source": [
    "Next, we'll convert every token into its corresponding index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_NH7ob43dQo"
   },
   "outputs": [],
   "source": [
    "def serialize(dataset, word2idx):\n",
    "    temp = []\n",
    "    for line in dataset: temp.append([word2idx[token] for token in line])\n",
    "    return torch.LongTensor(temp)\n",
    "\n",
    "# Convert to idx\n",
    "y_train = serialize(train_en_proc, word2idx_en)\n",
    "X_train = serialize(train_de_proc, word2idx_de)\n",
    "y_valid = serialize(valid_en_proc, word2idx_en)\n",
    "X_valid = serialize(valid_de_proc, word2idx_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cn0fpWCQgLsL"
   },
   "source": [
    "Then produce our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsRWBPXs5FlM"
   },
   "outputs": [],
   "source": [
    "bs = 128\n",
    "\n",
    "train_dataset = datautils.TensorDataset(X_train, y_train)\n",
    "valid_dataset = datautils.TensorDataset(X_valid, y_valid)\n",
    "train_sampler = datautils.RandomSampler(train_dataset)\n",
    "train_loader = datautils.DataLoader(train_dataset, batch_size=bs, sampler=train_sampler)\n",
    "valid_loader = datautils.DataLoader(valid_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jm4S0XOpgNIq"
   },
   "source": [
    "Now, since this is a text generation task and we'll be using the outputs of our RNNs, we'll have to use top-down sequentiality (like in language modeling). \n",
    "\n",
    "Left-right sequentiality is only useful for sequence classification tasks (like sentiment classification) as it treats one sentence as a batch. In text generation, we treat a number of tokens as a batch and the task is to generate the following tokens, which is why we use top-down.\n",
    "\n",
    "To easily convert our left-right batches, we can use ```.rot90()```. Here's an example showing the first batch in the trainings set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "WDJqxX815ICH",
    "outputId": "e521e34e-1d76-40a8-84ff-39fc3eeaf0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 128]) torch.Size([43, 128])\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [ 816, 2818,  883,  ...,  816,  883,  883],\n",
      "        [1601, 3547, 5046,  ...,  490, 5046, 5046],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]])\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [1916, 1722, 1916,  ..., 1916, 1916, 1916],\n",
      "        [ 685, 3055, 1576,  ..., 3304, 1576, 1576],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.rot90(k=3), y.rot90(k=3)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TAR4DNJ9Tng"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3tep4bugovz"
   },
   "source": [
    "First up, we have our encoder.\n",
    "\n",
    "\"Encoder\" is common parlance in deep learning, especially in NLP. When we say encoder, we usually refer to a way to embed sequential input. In this case, it's an embedding layer + an RNN layer that we use to \"encode\" the source sentence into a feature representation.\n",
    "\n",
    "When we pass our source sentence to the encoder, it gives us hidden and cell states that we use to inform the decoder about the source sentence.\n",
    "\n",
    "The architecture here is simple and it's something that we've seen before already in language modeling, minus the projection layer. The one improvement we'll add here is the inclusion of ```pack_padded_sequences``` which allows our RNN to disregard padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVwLVd267_-y"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, embedding_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_sz, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.vocab_sz = vocab_sz\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        weight = next(self.parameters())\n",
    "        hidden_dim = self.rnn.hidden_size\n",
    "        layers = self.rnn.num_layers\n",
    "\n",
    "        h = weight.new_zeros(layers, bs, hidden_dim)\n",
    "        c = weight.new_zeros(layers, bs, hidden_dim)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, x, pad_idx=None):\n",
    "        msl, bs = x.shape\n",
    "        out = self.embedding(x)\n",
    "        out = self.dropout(out)\n",
    "        hidden, cell = self.init_hidden(bs)\n",
    "\n",
    "        if pad_idx is not None:\n",
    "            lens = ((x.rot90() == pad_idx) == False).int().sum(dim=1)\n",
    "            out = nn.utils.rnn.pack_padded_sequence(out, lens, enforce_sorted=False)\n",
    "        \n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3TB7GUVhHzU"
   },
   "source": [
    "Our decoder is something a little different from our encoder. Instead of getting a batch of sequences, it will get a batch of tokens.\n",
    "\n",
    "The idea of sequence-to-sequence learning is that we encode the source sentence and use it to produce token after token of translations. We'll write the decoder to be used for this function.\n",
    "\n",
    "Nothing is too special here. We pass it the hidden and cell states from the encoder, then it produces logits that predict the most likely next token. We output the hidden and cell states again to be used for the next round of decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFaPtjl59H45"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, embedding_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_sz, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(hidden_dim, vocab_sz)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.vocab_sz = vocab_sz\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x.unsqueeze(0))\n",
    "        out = self.dropout(out)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc1(out.squeeze(0))\n",
    "\n",
    "        return out, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Rr-0OBthnre"
   },
   "source": [
    "We'll wrap everything together in a Seq2Seq wrapper module.\n",
    "\n",
    "Again, nothing too flashy. The module wraps together the encoder and decoder and defines the sequence-to-sequence generation/training scheme.\n",
    "\n",
    "First, we encode the source sentence. Second, we set the ```<sos>``` token as the initial \"start generation\" token. Decode using this token and the initial hidden and cell states. Update the token to the new \"predction\" then loop until the entire length of the target sentence is generated.\n",
    "\n",
    "We'll introduce a technique used in machine translation called **teacher forcing**. This technique essentially means that there is a chance that the correct answer is fed as the next token instead of the predicted token. In cases where the model strays from the correct translation, teacher forcing will let it learn back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSNQdB-D_H31"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, initrange=0.08):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.init_weights(initrange)\n",
    "\n",
    "    def init_weights(self, initrange=0.08):\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param.data, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing=0.5, pad_idx=None):\n",
    "        src_len, bs = x.shape\n",
    "        trg_len, bs = y.shape\n",
    "\n",
    "        # Make container for outputs\n",
    "        weight = next(self.encoder.parameters())\n",
    "        outputs = weight.new_zeros(trg_len, bs, self.decoder.vocab_sz)\n",
    "\n",
    "        # Encode source then prep first input\n",
    "        hidden, cell = self.encoder(x, pad_idx=pad_idx)\n",
    "        input_ids = y[0,:]\n",
    "\n",
    "        # Decode per input token\n",
    "        for i in range(1, trg_len):\n",
    "            out, hidden, cell = self.decoder(input_ids, hidden, cell)\n",
    "            outputs[i] = out\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing\n",
    "            input_ids = y[i] if teacher_force else out.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qKGxjxYiV-k"
   },
   "source": [
    "Let's test an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQ3iBPtE8-g8"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_sz=len(vocab_set_de), embedding_dim=100, hidden_dim=256)\n",
    "decoder = Decoder(vocab_sz=len(vocab_set_en), embedding_dim=100, hidden_dim=256)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4wF8ygLiXPb"
   },
   "source": [
    "Get some initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjLUsXjo9X9n"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.rot90(k=3), y.rot90(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPggUw8LiYr3"
   },
   "source": [
    "And test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sY006fLfAsIU",
    "outputId": "eb3563f2-a7ec-424e-fdbf-a9647dd0f04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 128, 4556])\n"
     ]
    }
   ],
   "source": [
    "out = model(x, y, pad_idx=word2idx_de['<pad>'])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pqi2789eiZ3y"
   },
   "source": [
    "To get the loss, we disregard the ```<sos>``` token. We do the same as we do in language modeling, which is flattening the outputs into 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YP1OJcsDCDok"
   },
   "outputs": [],
   "source": [
    "loss = criterion(out[1:].flatten(0, 1), y[1:].flatten(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SiziUB-EigHM"
   },
   "source": [
    "Here's our initial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-EraqazqD0Bz",
    "outputId": "21ab3a3a-9c8f-4351-ddf1-8739b7c19edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3815, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qy9genNiEnQu"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdVkkQWKiiFY"
   },
   "source": [
    "Time to put everything together.\n",
    "\n",
    "We'll initialize an encoder and decoder with the same hyperparameters (we need to do this to ensure that the hidden and cell shapes remain the same). Wrap them up in a Seq2Seq wrapper, then pass to the GPU.\n",
    "\n",
    "Adam is still our optimizer of choice. We'll use cosine annealing as a basic scheduler for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N84NV2YPEHqN"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_sz=len(vocab_set_de), embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.5)\n",
    "decoder = Decoder(vocab_sz=len(vocab_set_en), embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.5)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx_en['<pad>'])\n",
    "\n",
    "epochs = 10\n",
    "iters = epochs * len(train_loader)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=iters, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UG8BLyAGixI8"
   },
   "source": [
    "Here's the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A_ylCakfND_a",
    "outputId": "652b7ea5-56af-4418-d935-9ebc215caebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,236,236 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-087_qOvizI0"
   },
   "source": [
    "We'll train our model for 10 epochs, setting gradient clipping to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "3JkYUN21FDSd",
    "outputId": "98f69f05-9376-43f7-b0f7-be6c4cd850f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.82it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 4.8932 | Train Ppl 133.3773 | Valid Loss 4.3267 | Valid Ppl 75.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.66it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 4.2014 | Train Ppl 66.7812 | Valid Loss 3.9796 | Valid Ppl 53.4944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.95it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.62it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 3.9506 | Train Ppl 51.9646 | Valid Loss 3.8309 | Valid Ppl 46.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.95it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.56it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 3.7697 | Train Ppl 43.3692 | Valid Loss 3.6327 | Valid Ppl 37.8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.95it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.64it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 3.6098 | Train Ppl 36.9603 | Valid Loss 3.5103 | Valid Ppl 33.4571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.95it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.64it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   6 | Train Loss 3.4949 | Train Ppl 32.9486 | Valid Loss 3.4151 | Valid Ppl 30.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.62it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Train Loss 3.4229 | Train Ppl 30.6586 | Valid Loss 3.4263 | Valid Ppl 30.7632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.75it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   8 | Train Loss 3.3689 | Train Ppl 29.0469 | Valid Loss 3.3817 | Valid Ppl 29.4221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.79it/s]\n",
      "  0%|          | 0/227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   9 | Train Loss 3.3143 | Train Ppl 27.5032 | Valid Loss 3.3889 | Valid Ppl 29.6347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Train Loss 3.3131 | Train Ppl 27.4696 | Valid Loss 3.4424 | Valid Ppl 31.2607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = x.rot90(k=3).to(device), y.rot90(k=3).to(device)\n",
    "\n",
    "        out = model(x, y, pad_idx=word2idx_de['<pad>'])\n",
    "        loss = criterion(out[1:].flatten(0, 1), y[1:].flatten(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    valid_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(valid_loader):\n",
    "            x, y = x.rot90(k=3).to(device), y.rot90(k=3).to(device)\n",
    "\n",
    "            out = model(x, y, pad_idx=word2idx_de['<pad>'])\n",
    "            loss = criterion(out[1:].flatten(0, 1), y[1:].flatten(0))\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3} | Train Loss {:.4f} | Train Ppl {:.4f} | Valid Loss {:.4f} | Valid Ppl {:.4f}\".format(e, train_loss, np.exp(train_loss), valid_loss, np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrIgpk1bi3Zv"
   },
   "source": [
    "If you trained the model yourself, you can save the weights to resue it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzLuKS4F-YMV"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'seq2seq.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kb6D49yRFcY0"
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQv8_8cpi_9p"
   },
   "source": [
    "Next, we want to see our translation model in action.\n",
    "\n",
    "Let's load the model and see it in action. Put it in the CPU to prevent GPU overallocation, then set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7W6v5uTTHYJ"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_sz=len(vocab_set_de), embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.5)\n",
    "decoder = Decoder(vocab_sz=len(vocab_set_en), embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.5)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('seq2seq.pt'))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Wf8Ve_GxCFH0",
    "outputId": "7342066b-293f-4d9c-a95e-fccd4c2f2c31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Loss 3.3153 | Valid Ppl 27.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "valid_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(valid_loader):\n",
    "        x, y = x.rot90(k=3).to(device), y.rot90(k=3).to(device)\n",
    "\n",
    "        out = model(x, y, pad_idx=word2idx_de['<pad>'])\n",
    "        loss = criterion(out[1:].flatten(0, 1), y[1:].flatten(0))\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "valid_loss /= len(valid_loader)\n",
    "\n",
    "print(\"\\nValid Loss {:.4f} | Valid Ppl {:.4f}\".format(valid_loss, np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgM43jYTjJHU"
   },
   "source": [
    "For this example, we'll use the first example in the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HebxEeYSBBj"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(valid_loader))\n",
    "x, y = x.rot90(k=3), y.rot90(k=3)\n",
    "sample = x[:, 0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6EJVoBvjRAO"
   },
   "source": [
    "This is the source sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "syUAo3hjIBSL",
    "outputId": "511f7c85-3b21-40ec-db66-2efb161c338d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> ein kleines rothaariges mädchen in einem <unk> reitet auf einem spielzeugpferd . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([idx2word_de[idx] for idx in list(sample.squeeze(1).numpy())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgWiw2QTjT3h"
   },
   "source": [
    "Here's the correct translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "JdyEwI1dINez",
    "outputId": "1fa3fefe-c9cd-4f8c-e85f-917fa8079e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> a little redheaded girl wears a spider - man suit while riding a play horse . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([idx2word_en[idx] for idx in list(y[:, 0].numpy())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enxsNlwijVnU"
   },
   "source": [
    "We'll use basic multinomial sampling for this.\n",
    "\n",
    "Our sampling scheme is similar to seq2seq learning, minus the teacher forcing. We first encode the source sequence, then greedily take the maximum of the logits as the next token. Feed that next token over and over until we hit the max number of tokens, or we hit the end of sequence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rwp7HGVm9zUH",
    "outputId": "4efa98df-90ed-4014-86b1-fe545b63a540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a smiling blond girl wearing a yellow sweater is sitting on a green bench slide . <eos>\n"
     ]
    }
   ],
   "source": [
    " predictions = []\n",
    " max_words = 20\n",
    " temperature = 0.8\n",
    " torch.manual_seed(1111)\n",
    "\n",
    " model = model.cpu()\n",
    "\n",
    " with torch.no_grad():\n",
    "    hidden, cell = model.encoder(sample)\n",
    "    token = sample[0]\n",
    "\n",
    "    for _ in range(max_words):\n",
    "        out, hidden, cell = model.decoder(token, hidden, cell)\n",
    "        weights = torch.softmax(out / temperature, dim=-1)\n",
    "        token = torch.multinomial(weights, 1).squeeze(0)\n",
    "\n",
    "        predictions.append(token.item())\n",
    "\n",
    "        if token.item() == word2idx_en['<eos>']:\n",
    "            break\n",
    "\n",
    "print(' '.join([idx2word_en[ix] for ix in predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKIvswqsjoY3"
   },
   "source": [
    "Our model seems to know that there's a little girl and she is wearing a certain type of clothing. The other details weren't translated, but hey, that's pretty good for our first translation model!\n",
    "\n",
    "The basic seq2seq scheme has problems:\n",
    "1. It's highly unlikely the the decoder will retain the information learned by the encoder about the source sentence, especially for long sequences.\n",
    "2. The model has no way to learn alignment.\n",
    "3. Once the model has overwritten the hidden and cell states, there is no way for it to refer back to the source information.\n",
    "\n",
    "In the next notebook, we'll learn how to leverage the attention mechanism to solve these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CJP1kTI6_Qo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08a - Introduction to Machine Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
