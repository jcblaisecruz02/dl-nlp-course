{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL_7uK2x8zP7"
   },
   "source": [
    "# 07 - Optimizers, Activation Functions, Regularization, and Learning Rate Schedulers\n",
    "Prepared by Jan Christian Blaise Cruz\n",
    "\n",
    "DLSU Machine Learning Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diAqj7he89id"
   },
   "source": [
    "This time, we'll accelerate computation using a GPU. \n",
    "\n",
    "Make sure you're using a GPU instance by going to Runtime > Change Runtime Type, and make sure that GPU is chosen in the dropdown box.\n",
    "\n",
    "You can see the type of GPU that you have at the moment by using the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "hnxAiTe8aq4j",
    "outputId": "071135d1-306b-4fe8-bcec-ed3965e565c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 21 02:22:29 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHmfSoaK9LpX"
   },
   "source": [
    "Alright, let's dive right in. Let's import our (starting today) standard imports. PyTorch is our main tool of choice, so we'll bring in all the standard submodules. We'll also bring in some helpers for our dataset.\n",
    "\n",
    "Lastly, we'll seed the random number generator, then tell PyTorch what device we'll be using (in this case, a reference to our GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ot8EGojtcEDW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wMcCWb88G2j"
   },
   "source": [
    "# Data and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Nl8arG79g4M"
   },
   "source": [
    "We will once again use the MNIST dataset for our examples. Let's load the dataset, then convert them to PyTorch tensors. Lastly, we'll create dataloaders out of our data using batch size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BcxfT8JdST5"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data.astype(np.float32), \n",
    "                                                    mnist.target.astype(np.int), \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test  = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test  = torch.from_numpy(y_test)\n",
    "\n",
    "# Produce Dataloaders\n",
    "bs = 32\n",
    "\n",
    "train_data = data_utils.TensorDataset(X_train, y_train)\n",
    "test_data  = data_utils.TensorDataset(X_test,  y_test)\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=bs)\n",
    "test_loader  = data_utils.DataLoader(test_data,  batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IviIK8Ou9pMR"
   },
   "source": [
    "Let's check for shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "n7X_1_1ng8xu",
    "outputId": "f54d7d1f-b14f-4776-b97a-c5b1a911541e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) torch.Size([32])\n",
      "1641\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape, y.shape)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mshYhx1R9q2U"
   },
   "source": [
    "Let's code up a simple one-hidden layer neural network model so we can test things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmE0kYxohGtO"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwbsACGw9u7P"
   },
   "source": [
    "Then code a helper function for checking accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ufMLDhIhluv"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return torch.sum(torch.max(y_pred, 1)[1] == y).item() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8MZo2dUhCm6"
   },
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-fZHIcc9454"
   },
   "source": [
    "Let's start by taking a look at vanilla gradient descent. For the rest of this section of the notebook, we'll look at different optimization methods (based on gradient descent) and see how to implement them in PyTorch.\n",
    "\n",
    "Vanilla gradient descent is the backbone of our work, so we'll start with that. Remember the update rule:\n",
    "\n",
    "$$\\theta_{t + 1} = \\theta_t - \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "Let's instantiate a model and pass it to our GPU. We'll also use cross entropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grrinW_7g_Y0"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBmMD51U-Oqv"
   },
   "source": [
    "Let's train for 10 epochs using vanilla gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Xb-R4IOFh5H9",
    "outputId": "de0126a5-8661-433d-8aef-59a688ca449b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss 2.1359 | Test Loss 1.9407 | Train Acc 31.00% | Test Acc 51.05%\n",
      "Epoch   2 | Train Loss 1.8183 | Test Loss 1.7030 | Train Acc 59.20% | Test Acc 65.27%\n",
      "Epoch   3 | Train Loss 1.6171 | Test Loss 1.5309 | Train Acc 69.08% | Test Acc 72.33%\n",
      "Epoch   4 | Train Loss 1.4635 | Test Loss 1.3967 | Train Acc 74.27% | Test Acc 75.97%\n",
      "Epoch   5 | Train Loss 1.3418 | Test Loss 1.2891 | Train Acc 77.21% | Test Acc 77.97%\n",
      "Epoch   6 | Train Loss 1.2428 | Test Loss 1.1993 | Train Acc 79.19% | Test Acc 79.73%\n",
      "Epoch   7 | Train Loss 1.1586 | Test Loss 1.1217 | Train Acc 80.78% | Test Acc 80.90%\n",
      "Epoch   8 | Train Loss 1.0855 | Test Loss 1.0539 | Train Acc 82.04% | Test Acc 82.06%\n",
      "Epoch   9 | Train Loss 1.0205 | Test Loss 0.9943 | Train Acc 82.99% | Test Acc 83.17%\n",
      "Epoch  10 | Train Loss 0.9637 | Test Loss 0.9427 | Train Acc 83.89% | Test Acc 84.13%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"Epoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBojoy_3-Rpe"
   },
   "source": [
    "A shorthand for that would be to use the torch.optim submodule, which contains an implementation of standard gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nK3SixbSy_Gj"
   },
   "outputs": [],
   "source": [
    "# optim.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Io3lZUZk-XXp"
   },
   "source": [
    "While gradient descent is effective, it does have it's problems:\n",
    "1. It's slow. It takes a lot of time and iterations to achieve a good solution.\n",
    "2. It's sensitive to changes in the hyperparameters. You need to do a lot of runs to tune the learning rate, for example.\n",
    "3. For neural networks (and other non-convex optimization) settings, it has trouble getting out of local minima and saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFV8za0PjVyF"
   },
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxZvBpP8-xW2"
   },
   "source": [
    "Let's solve the first problem with SGD.\n",
    "\n",
    "Instead of relying purely on the gradient signals on the update step, why not use previous information to aid direction? In this case, we can say that if SGD is ramping up **momentum** during it's traversal of the loss surface, then we can keep pushing it towards directions that it's confident in.\n",
    "\n",
    "We call this method **SGD with Momentum**. You can picture this optimizer like a ball rolling down a hill. As it moves further down, it speeds up, until it meets a bump that changes its direction with inertia. Mathematically, we represent the update step like this:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\alpha \\nabla_\\theta J(\\theta) \\\\\n",
    "\\theta_{t + 1} = \\theta_t - v_t\n",
    "$$\n",
    "\n",
    "In this equation, $\\gamma$ represents our \"momentum\" parameter. The higher the momentum parameter, the more informtion the optimizer will use from the previous update step.\n",
    "\n",
    "Let's instantiate our neural network and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7bT4GVwisvG"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijXxqBHs_iQM"
   },
   "source": [
    "To keep track of which theta parameter matrices belong to which, we'll rely on PyTorch's built in parameter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "rEHs-lvYj4vh",
    "outputId": "a765e661-ecef-4dde-b467-1c3336998898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEw_TKK4_nIN"
   },
   "source": [
    "We'll keep a dictionary that contains information from the previous theta updates. We set our learning rate to 1e-4, and use a momentum value of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSqH9hzCkzyt"
   },
   "outputs": [],
   "source": [
    "v_t = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "YOPXdoNc5UBZ",
    "outputId": "0ed85246-8642-41e9-c798-5c1f0a26a5d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_t['fc1.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxYgLPMa_vtj"
   },
   "source": [
    "And then we code up the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "5Nj2Za6MjsdG",
    "outputId": "ddd21405-59a8-4450-dc83-9f7675ec498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss 1.3846 | Test Loss 0.9348 | Train Acc 71.11% | Test Acc 83.88%\n",
      "Epoch   2 | Train Loss 0.7660 | Test Loss 0.6427 | Train Acc 86.25% | Test Acc 87.89%\n",
      "Epoch   3 | Train Loss 0.5664 | Test Loss 0.5147 | Train Acc 88.81% | Test Acc 89.48%\n",
      "Epoch   4 | Train Loss 0.4678 | Test Loss 0.4463 | Train Acc 89.97% | Test Acc 90.18%\n",
      "Epoch   5 | Train Loss 0.4090 | Test Loss 0.4008 | Train Acc 90.85% | Test Acc 90.64%\n",
      "Epoch   6 | Train Loss 0.3687 | Test Loss 0.3699 | Train Acc 91.42% | Test Acc 91.27%\n",
      "Epoch   7 | Train Loss 0.3390 | Test Loss 0.3469 | Train Acc 91.93% | Test Acc 91.50%\n",
      "Epoch   8 | Train Loss 0.3166 | Test Loss 0.3284 | Train Acc 92.33% | Test Acc 91.87%\n",
      "Epoch   9 | Train Loss 0.2982 | Test Loss 0.3137 | Train Acc 92.72% | Test Acc 92.11%\n",
      "Epoch  10 | Train Loss 0.2832 | Test Loss 0.3036 | Train Acc 93.05% | Test Acc 92.18%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                v_t[name] = (gamma * v_t[name]) + (learning_rate * param.grad)\n",
    "                param -= v_t[name]\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"Epoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOqKnw_D_ydJ"
   },
   "source": [
    "As we can see, our model learned faster from using Momentum than with just pure gradient descent. To use momentum in PyTorch optimizers, we use the following shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONXt1wQTkgGv"
   },
   "outputs": [],
   "source": [
    "# optim.SGD(momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Jq5bhAq_4yE"
   },
   "source": [
    "Now, let's work on problem 2. SGD requires you to tune your hyperparameters, which can be costly. Learning rate is the most important hyperparameter. If it's too small, training can proceed very slowly. If it's too big, training can diverge or fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8v7z5of-oPy1"
   },
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTRdY9Q6AKBY"
   },
   "source": [
    "The RMSProp (Hinton, 2012) algorithm was proposed by Geoffrey Hinton on session 6 of his Coursera MOOC (it didn't come from a paper!)\n",
    "\n",
    "The idea of RMSProp is essentially scaling the learning rate based on the weighted moving average of our gradients. If our gradients are moving slowly (meaning, little changes are made), then we do smaller updates. If the gradients are moving faster (meaning, the parameters haven't \"found their place\" yet), then we do larger updates.\n",
    "\n",
    "the equation for $E[g^2]_t$ represents this moving average. We use a $\\beta$ parameter to essentially control how much information we take from the previous update step and how much information to take from the current one. Afterwards, we then take the root mean square of that moving average to scale the learning rate. This is where RMSProp gets its name.\n",
    "\n",
    "The equations for the update step are as follows:\n",
    "\n",
    "$$\n",
    "g_t = \\nabla_\\theta J(\\theta) \\\\\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g^2_t \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "$$\n",
    "\n",
    "Now to see it in action, we'll create a new instance of our neural network and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BO3KTBaoRFB"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qvd9xWLCl2R"
   },
   "source": [
    "We'll again use a dictionary to store our previous averages. Hinton recommends a $\\beta$ of 0.9 and a default learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1uZK2fHpCR8"
   },
   "outputs": [],
   "source": [
    "history = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "learning_rate = 0.001\n",
    "beta = 0.9\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lgw3JuOsCvIq"
   },
   "source": [
    "Let's implement it to train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "A0wuFmFEnL1O",
    "outputId": "3f78237c-c0cf-470d-9e51-ce9df07b4d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss 0.4608 | Test Loss 0.4077 | Train Acc 87.73% | Test Acc 90.58%\n",
      "Epoch   2 | Train Loss 0.3721 | Test Loss 0.3606 | Train Acc 91.34% | Test Acc 91.85%\n",
      "Epoch   3 | Train Loss 0.3318 | Test Loss 0.3415 | Train Acc 92.51% | Test Acc 92.38%\n",
      "Epoch   4 | Train Loss 0.3067 | Test Loss 0.3283 | Train Acc 93.15% | Test Acc 93.02%\n",
      "Epoch   5 | Train Loss 0.2853 | Test Loss 0.3268 | Train Acc 93.64% | Test Acc 93.16%\n",
      "Epoch   6 | Train Loss 0.2726 | Test Loss 0.3027 | Train Acc 94.05% | Test Acc 93.84%\n",
      "Epoch   7 | Train Loss 0.2547 | Test Loss 0.2859 | Train Acc 94.41% | Test Acc 94.20%\n",
      "Epoch   8 | Train Loss 0.2397 | Test Loss 0.2918 | Train Acc 94.70% | Test Acc 94.13%\n",
      "Epoch   9 | Train Loss 0.2332 | Test Loss 0.2758 | Train Acc 94.93% | Test Acc 94.35%\n",
      "Epoch  10 | Train Loss 0.2227 | Test Loss 0.2830 | Train Acc 95.11% | Test Acc 94.26%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                history[name] = (beta * torch.square(history[name])) + ((1 - beta) * torch.square(param.grad))\n",
    "                param -= learning_rate / torch.sqrt(history[name] + epsilon) * param.grad\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"Epoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELfUc9_fCzvp"
   },
   "source": [
    "To use RMSProp in PyTorch, we simply use the shorthand from the torch.optim module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf2anbNrptOo"
   },
   "outputs": [],
   "source": [
    "# optim.RMSprop(lr=0.001, alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yxl1xmSEC4KZ"
   },
   "source": [
    "Now, we can mash Momentum and RMSProp together to make a better optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMMXmjQTtQWG"
   },
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19Jdls36DI3o"
   },
   "source": [
    "Adaptive Moments Estimation (Kingma and Ba, 2014) is considered the \"de facto\" standard optimizer for Neural Networks, simply because it's very effective. It combines Momentum's idea of...well... momentum, as well as RMSProp's idea of scaling learning rates depending on the current and previous gradients.\n",
    "\n",
    "Here are the equations for Adam's update rule:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_{t + 1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v_t}} + \\epsilon} \\hat{m_t}\n",
    "$$\n",
    "\n",
    "In this equation, $m_t$ is an approximation of the first moment (the expected mean of the distribution) and $v_t$ is an approximation of the second moment (the uncentered variance of the distribution). This is how the algorithm gets its name.\n",
    "\n",
    "In easier words, we're essentially taking the weighted moving averages of both the gradient and the gradient squared. Since we initialize $m_t$ and $v_t$ to be zero at start, these estimates will be biased during the initial steps. To correct these, we'll make bias-corrected values $\\hat{m_t}$ and $\\hat{v_t}$.\n",
    "\n",
    "We'll then use the variance (as a root mean square) to scale the learning rate, and the mean as a \"momentum-influenced parameter update.\" Researchers describe Adam as like a \"ball with friction\" moving down a slope.\n",
    "\n",
    "**In mathematics, a moment is a measure that refers to a function's graph. For probability distributions, these are (from zeroth to fourth): total probability, expected value/arithmetic mean, variance, skewness, and kurtosis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mPXqwHhtRCC"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ae5S81fVGQWR"
   },
   "source": [
    "We'll use two dictionaries to keep track of our mean and variances. We'll use Karpathy's Constant 3e-4 as a base learning rate for Adam. The authors propose 0.9 and 0.999 as good values for our betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIU2Q3jAuqNx"
   },
   "outputs": [],
   "source": [
    "m_t = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "v_t = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "learning_rate = 3e-4\n",
    "b1, b2 = 0.9, 0.999\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kU7vF9JGa2l"
   },
   "source": [
    "We'll implement Adam and train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "9O2Lyd8musUT",
    "outputId": "a76bf61a-d719-49d4-9fef-e25c5f210f07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss 0.5908 | Test Loss 0.3306 | Train Acc 86.61% | Test Acc 91.35%\n",
      "Epoch   2 | Train Loss 0.2868 | Test Loss 0.2655 | Train Acc 92.18% | Test Acc 92.44%\n",
      "Epoch   3 | Train Loss 0.2417 | Test Loss 0.2424 | Train Acc 93.27% | Test Acc 93.27%\n",
      "Epoch   4 | Train Loss 0.2179 | Test Loss 0.2304 | Train Acc 93.87% | Test Acc 93.37%\n",
      "Epoch   5 | Train Loss 0.2018 | Test Loss 0.2101 | Train Acc 94.12% | Test Acc 93.83%\n",
      "Epoch   6 | Train Loss 0.1868 | Test Loss 0.2043 | Train Acc 94.52% | Test Acc 94.04%\n",
      "Epoch   7 | Train Loss 0.1785 | Test Loss 0.1909 | Train Acc 94.86% | Test Acc 94.34%\n",
      "Epoch   8 | Train Loss 0.1690 | Test Loss 0.1913 | Train Acc 95.09% | Test Acc 94.52%\n",
      "Epoch   9 | Train Loss 0.1631 | Test Loss 0.1836 | Train Acc 95.25% | Test Acc 94.59%\n",
      "Epoch  10 | Train Loss 0.1573 | Test Loss 0.1803 | Train Acc 95.38% | Test Acc 94.64%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for t, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                m_t[name] = b1 * m_t[name] + (1 - b1) * param.grad\n",
    "                v_t[name] = b2 * v_t[name] + (1 - b2) * torch.square(param.grad)\n",
    "                hat_m = m_t[name] / (1 - b1 ** (t + 1))\n",
    "                hat_v = v_t[name] / (1 - b2 ** (t + 1))\n",
    "                param -= (learning_rate / (torch.sqrt(hat_v) + epsilon)) * hat_m\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"Epoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLUaqXeRGd71"
   },
   "source": [
    "Great results! To use Adam in PyTorch, we can use the convenience function from torch.optim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bx4ZA-pGvm3R"
   },
   "outputs": [],
   "source": [
    "# optim.Adam(betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jv3bkuKGGu2m"
   },
   "source": [
    "While Adam performs well, there are a lot of cases where it is outperformed by vanilla SGD with a well tuned learning rate.\n",
    "\n",
    "Researchers pinpoint that the problem lies with the exponential weighted moving average that scales the learning rate. In learning settings, a handful of minibatches will provide very informative gradients. However, since these are rare, scaling through exponential averaging will diminish their influence. This leads to suboptimal solutions, especially in tasks that have sparse data or in tasks that have a lot of rare vocabulary, like machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-pslIaWz4PC"
   },
   "source": [
    "# AMSGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn4QKdd2HbwD"
   },
   "source": [
    "AMSGrad (Reddi et al., 2018) solves the problems with Adam by using the maximum of the past squared gradients instead of an exponential average. They also removed the debiasing step to enforce simplicity. Here is the update step:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{v_t} = \\max(\\hat{v_{t-1}}, v_t) \\\\\n",
    "\\theta_{t + 1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v_t}} + \\epsilon} m_t\n",
    "$$\n",
    "\n",
    "To see it in action, let's create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zy5EOJ_nz6K6"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7h1B6JAH0mt"
   },
   "source": [
    "We'll use three dictionaries to keep track of our previous updates. We'll also use the defaults that were used by Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W7i_aRlvuBP"
   },
   "outputs": [],
   "source": [
    "m_t = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "v_t = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "hat_v = {n: torch.zeros(p.shape).to(device) for n, p in model.named_parameters()}\n",
    "learning_rate = 3e-4\n",
    "b1, b2 = 0.9, 0.999\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "197Gm_t5H6iX"
   },
   "source": [
    "And then we'll implement AMSGrad to train our model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "m-t7n00O0jYk",
    "outputId": "35b23727-fcb3-44fb-d744-45186ba1b15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss 0.5237 | Test Loss 0.3316 | Train Acc 87.08% | Test Acc 90.92%\n",
      "Epoch   2 | Train Loss 0.3012 | Test Loss 0.2792 | Train Acc 91.59% | Test Acc 92.22%\n",
      "Epoch   3 | Train Loss 0.2601 | Test Loss 0.2613 | Train Acc 92.67% | Test Acc 92.39%\n",
      "Epoch   4 | Train Loss 0.2399 | Test Loss 0.2362 | Train Acc 93.11% | Test Acc 93.28%\n",
      "Epoch   5 | Train Loss 0.2192 | Test Loss 0.2274 | Train Acc 93.65% | Test Acc 93.31%\n",
      "Epoch   6 | Train Loss 0.2080 | Test Loss 0.2189 | Train Acc 93.90% | Test Acc 93.71%\n",
      "Epoch   7 | Train Loss 0.1982 | Test Loss 0.2151 | Train Acc 94.33% | Test Acc 93.77%\n",
      "Epoch   8 | Train Loss 0.1945 | Test Loss 0.2084 | Train Acc 94.39% | Test Acc 93.82%\n",
      "Epoch   9 | Train Loss 0.1863 | Test Loss 0.2019 | Train Acc 94.56% | Test Acc 94.14%\n",
      "Epoch  10 | Train Loss 0.1794 | Test Loss 0.1991 | Train Acc 94.83% | Test Acc 94.10%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                m_t[name] = b1 * m_t[name] + (1 - b1) * param.grad\n",
    "                v_t[name] = b2 * v_t[name] + (1 - b2) * torch.square(param.grad)\n",
    "                hat_v[name] = torch.max(hat_v[name], v_t[name])\n",
    "                param -= (learning_rate / (torch.sqrt(hat_v[name]) + epsilon)) * m_t[name]\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"Epoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y--JdOu4IBXW"
   },
   "source": [
    "AMSGrad achieves very similar results to Adam. While it may outperform Adam in certain tasks, it is more computationally expensive space-wise to implement.\n",
    "\n",
    "PyTorch does not have a standard implementation of AMSGrad since it is not widely used. While it's not used as much as Adam, it is worth learning that the most accepted techniques still have their flaws. In this case, this keeps research on optimization techniques alive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSZN1E5Q9QFu"
   },
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfOh_bCTIT79"
   },
   "source": [
    "Next up, we'll look at activation functions.\n",
    "\n",
    "So far, we've been using Sigmoid as an activation function. While it does do the job (and proofs for the universal approximation theorem exists for sigmoid), it does have a very unwelcome problem called vanishing gradients.\n",
    "\n",
    "This occurs when a pre-activation has a value that is very large or very small, which causes the sigmoid's gradients to be effectively zero. Very small values, when backpropagated, only result in smaller and smaller values. This causes the gradient to \"vanish\" and the model to stop learning. This phenomenon where sigmoids saturate is called the vanishing gradient problem.\n",
    "\n",
    "Here's a simple model to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KADjG5-I-Cd0"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4CAG1tWmIuf9"
   },
   "source": [
    "We'll initialize the weights to be rather large to simulate the effects of vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78EaAJ4X-CkM"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.data.uniform_(9.95, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHUdSlu6I2fy"
   },
   "source": [
    "We can check the weights in a layer using the following convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "rm1q25zv-Ci6",
    "outputId": "3348b607-95e5-47cd-b21d-88549772757b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[9.9852, 9.9618, 9.9592,  ..., 9.9686, 9.9737, 9.9777],\n",
       "        [9.9680, 9.9919, 9.9893,  ..., 9.9855, 9.9912, 9.9811],\n",
       "        [9.9730, 9.9750, 9.9915,  ..., 9.9982, 9.9527, 9.9506],\n",
       "        ...,\n",
       "        [9.9890, 9.9690, 9.9991,  ..., 9.9718, 9.9665, 9.9530],\n",
       "        [9.9760, 9.9799, 9.9917,  ..., 9.9505, 9.9579, 9.9583],\n",
       "        [9.9579, 9.9732, 9.9645,  ..., 9.9937, 9.9909, 9.9790]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKkznRY-I44u"
   },
   "source": [
    "Let's check the initial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S_RYrlmp9SZi",
    "outputId": "bd002f02-bf38-4f3c-a0ce-da28e303ca4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.365257978439331\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPBcBdNqI8su"
   },
   "source": [
    "Then let's run backpropagation so we can see the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Prd2G9gR-fai"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFbWzzQHI7wA"
   },
   "source": [
    "Then inspect the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9iJACTiJ-fgK",
    "outputId": "19672194-9bc6-4e9d-f1fc-1785c5d1df48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_an6lKMJJB-Y"
   },
   "source": [
    "As you can see, underflow pushed all of the gradients that are near zero to zero. This causes this specific layer to not learn anything for this minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WINmuPc79SYj"
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_K_4xGJJIm-"
   },
   "source": [
    "Let's do another check after that update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lNC3p87y9SVB",
    "outputId": "c9f05c5d-39f1-4a2d-bdfb-655932336e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3418242931365967\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH_qYf1bJMYa"
   },
   "source": [
    "Now we see a very miniscule change in the gradients. This may be due to features that when multiplied to weights result in values where the sigmoid's gradient is not near zero.\n",
    "\n",
    "While *some* of the weights still learn, we can't deny that most of the weights have stopped learning. To solve the vanishing gradient problem, we introduce a different kind of activation function, the ReLU.\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "Due to bounding the pre-activations from zero to positive infinity, this ensures that even if the activations become very large, they will not cause the gradients to vanish. If the pre-activation is less than zero, it turns that weight off, causing some level of \"feature selection.\"\n",
    "\n",
    "Let's reimplement our classifier to take any activation we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjpv0ljTB8Ce"
   },
   "outputs": [],
   "source": [
    "fn = getattr(torch, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pxlTDFE-B8N7",
    "outputId": "5ee46515-8f8c-4b7f-f57b-d4a58a41511c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _VariableFunctionsClass.relu>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "cxQ4ZbY9B8aD",
    "outputId": "829da6eb-3592-46ac-b478-bc62ac2f1e64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8106, 0.2778, 0.0237, 0.3264, 0.6076, 0.4387, 0.2737, 0.3026, 0.8402,\n",
       "         0.0229],\n",
       "        [0.5996, 0.5519, 0.0750, 0.2495, 0.5378, 0.9661, 0.2393, 0.4775, 0.4872,\n",
       "         0.8691],\n",
       "        [0.3350, 0.9564, 0.9675, 0.7527, 0.0995, 0.9526, 0.7401, 0.8962, 0.7106,\n",
       "         0.3155],\n",
       "        [0.2837, 0.0387, 0.6324, 0.1396, 0.3486, 0.8905, 0.3037, 0.4479, 0.2570,\n",
       "         0.8080],\n",
       "        [0.7167, 0.5765, 0.3893, 0.2680, 0.3339, 0.2481, 0.7888, 0.6254, 0.1747,\n",
       "         0.0563],\n",
       "        [0.0430, 0.7353, 0.7776, 0.2877, 0.4653, 0.5478, 0.7822, 0.3584, 0.7011,\n",
       "         0.8642],\n",
       "        [0.6823, 0.0591, 0.6562, 0.2958, 0.8072, 0.0123, 0.4818, 0.9800, 0.4406,\n",
       "         0.6441],\n",
       "        [0.5488, 0.9565, 0.7668, 0.5523, 0.0247, 0.0232, 0.9730, 0.6204, 0.3875,\n",
       "         0.2019],\n",
       "        [0.5219, 0.5436, 0.8318, 0.0080, 0.2589, 0.8435, 0.0098, 0.9974, 0.7244,\n",
       "         0.8615],\n",
       "        [0.6922, 0.4097, 0.4689, 0.2491, 0.1071, 0.3233, 0.1136, 0.6408, 0.1049,\n",
       "         0.3313]])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn(torch.rand(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EMqUmCWCCwD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgAvkSED_PDa"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation='sigmoid'):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = getattr(torch, activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80hfxHIwJ6rT"
   },
   "source": [
    "Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0InnXWhK_PJH"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10, activation='relu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-eDkcqRJ78F"
   },
   "source": [
    "Here's the model's first layer weights for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "cE9mmZgj_PQj",
    "outputId": "b99dcbc9-de46-41f2-86a4-f9ab27c8268b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0320,  0.0209, -0.0169,  ...,  0.0234, -0.0106, -0.0191],\n",
       "        [-0.0057,  0.0214,  0.0345,  ..., -0.0140, -0.0036, -0.0297],\n",
       "        [-0.0259,  0.0219, -0.0293,  ..., -0.0345, -0.0024, -0.0150],\n",
       "        ...,\n",
       "        [ 0.0164, -0.0285, -0.0310,  ..., -0.0086,  0.0015, -0.0260],\n",
       "        [ 0.0284,  0.0275,  0.0195,  ..., -0.0239, -0.0293,  0.0049],\n",
       "        [ 0.0257, -0.0315, -0.0113,  ...,  0.0119,  0.0101,  0.0088]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fsqo9fFcKANf"
   },
   "source": [
    "Now let's check the initial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dgMY6hq3_PPV",
    "outputId": "cfb3f164-dd9f-4e7f-97da-e6479aebf264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.931854248046875\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0wlMk6CKCDT"
   },
   "source": [
    "Zeroing out certain values will induce a large loss, which is natural. Let's do one round of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wj9FUTma9ST4"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpzQEw0nKKJZ"
   },
   "source": [
    "And see the loss on the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "epAxLI4S_eqD",
    "outputId": "a6f3922b-9866-4f84-c092-9ec9f9094e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.564491271972656\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qj3r4qI4KM6D"
   },
   "source": [
    "As we can see, the loss diminished very quickly.\n",
    "\n",
    "While ReLUs solve vanishing gradients, they do have one problem, which we call the **dead ReLU problem**. This happens when most of the pre-activations are negative, which cause their gradients to be zero. Anything multiplied to a zero becomes zero. This means that once a weight have been reduced to zero by ReLU, it cannot be used again. Think of it as \"permanent brain damage.\"\n",
    "\n",
    "To illustrate, let's instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pZSGen7ALD8"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=128, output_dim=10, activation='relu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmI2sgftK9rE"
   },
   "source": [
    "Let's make all the weights zero or negative to turn positive features into negatives or cancel them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sjhGGAvALWs"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.data.uniform_(-0.95, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7AgKtGBLDWs"
   },
   "source": [
    "Here's the weights for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "2ls--1_NALeZ",
    "outputId": "e02e8347-a2a5-41a7-a3c3-7e2c4c702b86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7589, -0.1574, -0.7725,  ..., -0.1801, -0.7189, -0.6725],\n",
       "        [-0.6133, -0.0948, -0.1728,  ..., -0.1845, -0.3282, -0.5740],\n",
       "        [-0.4391, -0.7236, -0.4287,  ..., -0.4023, -0.3112, -0.3046],\n",
       "        ...,\n",
       "        [-0.9460, -0.2352, -0.5050,  ..., -0.9281, -0.9049, -0.9400],\n",
       "        [-0.1440, -0.9492, -0.8244,  ..., -0.5679, -0.0645, -0.6139],\n",
       "        [-0.0043, -0.6651, -0.0916,  ..., -0.8562, -0.8520, -0.3418]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqCdFrH5LFFH"
   },
   "source": [
    "Let's take the initial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cTtlY99EALdG",
    "outputId": "6684b3f2-17bc-401c-b323-3295a2c935e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3071532249450684\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2ImS2cXLGp6"
   },
   "source": [
    "Then backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gjlwp7v2AfSA"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKO2EHb3LJVK"
   },
   "source": [
    "Let's check the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "zRhuGSa_Aaez",
    "outputId": "ad135a0f-279b-4b6c-c92f-ea3d6a659788"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRgRiMsyLKvk"
   },
   "source": [
    "Now, since the gradients are zero, we can expect our weights to die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQpw-KzJAay1"
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1dfrGonLWz8"
   },
   "source": [
    "Checking on another iteration shows little change in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-b1nb_OLAanY",
    "outputId": "0107b0e2-7a2d-4467-f42b-1bd407719052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3069703578948975\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjS3fctvLaWJ"
   },
   "source": [
    "This problem is move obvious once training has progressed significantly. In the earlier stages of training, dead neurons may not be as apparent, especially when using optimizers that use information from previous updates (like Adam).\n",
    "\n",
    "In addition to sigmoid and ReLU, other activation functions exist, like the hyperbolic tangent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "UXI4kP5VAvFT",
    "outputId": "2e57ce92-c488-4b8e-b1da-cf62242608b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6939, 0.6895, 0.6189,  ..., 0.3762, 0.5984, 0.7391],\n",
       "        [0.4039, 0.7081, 0.2623,  ..., 0.1109, 0.5687, 0.2029],\n",
       "        [0.0314, 0.2638, 0.1884,  ..., 0.3660, 0.6329, 0.0872],\n",
       "        ...,\n",
       "        [0.6976, 0.1645, 0.3900,  ..., 0.3329, 0.5385, 0.3594],\n",
       "        [0.6184, 0.5461, 0.2193,  ..., 0.5999, 0.3148, 0.4593],\n",
       "        [0.6282, 0.7393, 0.5304,  ..., 0.5910, 0.6962, 0.6380]])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(torch.rand(32, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7P6av8E8Lvn9"
   },
   "source": [
    "Or the scaled exponential linear unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "oZksFmnfA0B4",
    "outputId": "52f0ac2e-bdb2-4c76-ebc3-301383111cba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.1843e-01, 9.9903e-01, 2.9602e-02,  ..., 2.1382e-01, 1.0189e-01,\n",
       "         6.3008e-01],\n",
       "        [8.7486e-01, 8.6671e-01, 3.1738e-02,  ..., 3.6255e-04, 7.7438e-01,\n",
       "         2.8898e-01],\n",
       "        [6.9118e-01, 7.5252e-01, 1.0291e+00,  ..., 6.4152e-01, 7.2487e-01,\n",
       "         2.5264e-01],\n",
       "        ...,\n",
       "        [7.1973e-01, 1.6815e-01, 3.8548e-01,  ..., 5.9593e-01, 3.0547e-01,\n",
       "         7.6764e-01],\n",
       "        [2.5948e-02, 4.7135e-01, 6.4959e-01,  ..., 9.6709e-01, 5.2690e-01,\n",
       "         1.0526e-01],\n",
       "        [2.5942e-02, 1.2759e-01, 4.2285e-01,  ..., 2.8810e-01, 5.4804e-01,\n",
       "         9.8794e-01]])"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.selu(torch.rand(32, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SGD0XSjLzf0"
   },
   "source": [
    "And many more. We'll tackle more advanced activation functions as the need arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoEuhD2i79lV"
   },
   "source": [
    "# Regularization for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwM3Rzo7L37j"
   },
   "source": [
    "Regularization for neural networks, aside from weight decay (L2 normalization), is done through **Dropout** (Srivastava et al., 2014).\n",
    "\n",
    "The idea of dropout is randomly shutting down neurons temporarily during an iteration. This prevents a problem called \"interdependence\" where certain neurons would only improve when nudged by specific other neurons. Interdependence causes a model to overfit by relying on a select (often biased) number of weights. By randomly shutting down neurons during training, any neuron would then be forced to also rely on other neurons for its information.\n",
    "\n",
    "Dropout in PyTorch is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e9hNCnI03mY"
   },
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcySkOOnMfGs"
   },
   "source": [
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "RMYdmmSe8VDY",
    "outputId": "454cc404-df6e-4305-bcdf-8660d8a24b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5683, 0.3716, 0.0402,  ..., 0.4963, 0.2983, 0.8306],\n",
      "        [0.2989, 0.4166, 0.0325,  ..., 0.0400, 0.1299, 0.4955],\n",
      "        [0.2415, 0.0788, 0.8703,  ..., 0.3353, 0.6601, 0.4963],\n",
      "        ...,\n",
      "        [0.7047, 0.3866, 0.6453,  ..., 0.2435, 0.6146, 0.3113],\n",
      "        [0.1023, 0.1380, 0.4004,  ..., 0.5660, 0.7226, 0.8818],\n",
      "        [0.3103, 0.3818, 0.5397,  ..., 0.0287, 0.3997, 0.9811]])\n"
     ]
    }
   ],
   "source": [
    "out = torch.rand(32, 128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prjQEK8RMiqA"
   },
   "source": [
    "Random values will be turned to zero to a probability set by dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "7A4CgEEW8YMv",
    "outputId": "96561b53-2952-4df5-c0cd-85af30027290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.7432, 0.0000,  ..., 0.0000, 0.0000, 1.6612],\n",
      "        [0.5979, 0.0000, 0.0649,  ..., 0.0800, 0.0000, 0.9909],\n",
      "        [0.4831, 0.0000, 0.0000,  ..., 0.6706, 1.3202, 0.9925],\n",
      "        ...,\n",
      "        [1.4093, 0.0000, 0.0000,  ..., 0.4870, 0.0000, 0.6226],\n",
      "        [0.0000, 0.2760, 0.0000,  ..., 0.0000, 1.4453, 1.7637],\n",
      "        [0.0000, 0.0000, 1.0793,  ..., 0.0000, 0.7995, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "out = dropout(out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cPSEs4XMofF"
   },
   "source": [
    "Let's create a new model that combines all of the techniques we've learned so far. We'll generalize it as well to use more than one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBGm28W08aE3"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5, activation='sigmoid', n_layers=1):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = [nn.Linear(hidden_dim, output_dim) if n == n_layers - 1 else nn.Linear(hidden_dim, hidden_dim) for n in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = getattr(torch, activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            out = layer(out)\n",
    "            if n != len(self.layers) - 1:\n",
    "                out = self.activation(out)\n",
    "                out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENPw9WGOMvNl"
   },
   "source": [
    "Let's instantiate our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjF4rvYMEVR-"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=1024, output_dim=10, \n",
    "                      n_layers=2, activation='relu', dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awqPxKypMyA3"
   },
   "source": [
    "We can check the model's \"stack\" by calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "nIQ06xJAFCYf",
    "outputId": "9291713e-ea75-465c-8541-24654ce64ff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "admdbvpsNPSO"
   },
   "source": [
    "Let's check the number of theta parameters in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "1WYY2qX8NCSb",
    "outputId": "00b3a08f-0bbf-43fe-8d81-c2972a19d162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1,863,690\n"
     ]
    }
   ],
   "source": [
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "print(\"Number of parameters: {:,}\".format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P3C1a14M8F8"
   },
   "source": [
    "We'll add train and eval directives to the model to tell it to switch off dropout during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "H72rZy-OD18i",
    "outputId": "b9068dca-cb23-4b84-aa5b-8431b2b4799c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 254.75it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 795.22it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:05, 269.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 0.9393 | Test Loss 0.1787 | Train Acc 85.36% | Test Acc 94.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.61it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 802.05it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 257.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 0.3028 | Test Loss 0.1524 | Train Acc 91.85% | Test Acc 95.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.23it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 823.59it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 268.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 0.2706 | Test Loss 0.1506 | Train Acc 92.68% | Test Acc 95.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.76it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 826.51it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 0.2586 | Test Loss 0.1494 | Train Acc 93.49% | Test Acc 96.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.94it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 796.15it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 262.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 0.2589 | Test Loss 0.1279 | Train Acc 93.68% | Test Acc 96.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.56it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 805.19it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 268.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   6 | Train Loss 0.2489 | Test Loss 0.1478 | Train Acc 94.01% | Test Acc 95.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 256.84it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 786.58it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 263.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Train Loss 0.2399 | Test Loss 0.1313 | Train Acc 94.28% | Test Acc 96.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.58it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 787.00it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 270.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   8 | Train Loss 0.2348 | Test Loss 0.1406 | Train Acc 94.47% | Test Acc 96.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.32it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 820.95it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 257.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   9 | Train Loss 0.2226 | Test Loss 0.1404 | Train Acc 94.83% | Test Acc 96.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.31it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 822.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Train Loss 0.2203 | Test Loss 0.1413 | Train Acc 94.88% | Test Acc 96.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRfBZULTOfSi"
   },
   "source": [
    "Great results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPfBtoRHOix-"
   },
   "source": [
    "# Learning Rate Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8mkGoFnOpVH"
   },
   "source": [
    "The last thing we'll be adding to our toolbox is what we call a **scheduler**. The idea for this is quite simple. As a neural network reaches convergence, our updates need to be slower and slower in order to prevent our model from overshooting. At the same time, if performance stagnates, this usually means the model has encountered a local minima, and a change in learning rate is needed to get out of it.\n",
    "\n",
    "Schedulers are used to perform these tasks. Let's import PyTorch's scheduler submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAgx30qUD9QK"
   },
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVP5piQbPZmG"
   },
   "source": [
    "Let's instantiate another learning setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1ueZXc-PYff"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=1024, output_dim=10, \n",
    "                      n_layers=2, activation='relu', dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56-5WygwPY1b"
   },
   "source": [
    "Let's use a decay scheduler for our learning rate. Cosine Annealing is one popular scheduling method. Let's use it.\n",
    "\n",
    "We'll set the max number of iterations, as well as the lower-bound of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSFk5-RGPNsS"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "iters = epochs * len(train_loader)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=iters, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlrWeYPOQHsh"
   },
   "source": [
    "Using the scheduler itself will only add one more line to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "G9-1BuLuQDAh",
    "outputId": "a3b07d8b-d616-45e1-bb7d-779c5a2841df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 423.04it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1173.41it/s]\n",
      "  3%|▎         | 43/1641 [00:00<00:03, 426.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 0.8899 | Test Loss 0.1743 | Train Acc 85.28% | Test Acc 95.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 424.37it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1140.38it/s]\n",
      "  3%|▎         | 44/1641 [00:00<00:03, 439.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 0.2991 | Test Loss 0.1449 | Train Acc 91.89% | Test Acc 95.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 431.13it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1193.95it/s]\n",
      "  3%|▎         | 45/1641 [00:00<00:03, 446.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 0.2446 | Test Loss 0.1293 | Train Acc 93.39% | Test Acc 96.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 426.81it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1184.38it/s]\n",
      "  3%|▎         | 45/1641 [00:00<00:03, 443.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 0.2002 | Test Loss 0.1198 | Train Acc 94.45% | Test Acc 96.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 432.06it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1205.41it/s]\n",
      "  3%|▎         | 45/1641 [00:00<00:03, 446.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 0.1609 | Test Loss 0.0987 | Train Acc 95.60% | Test Acc 97.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 429.18it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1180.65it/s]\n",
      "  3%|▎         | 46/1641 [00:00<00:03, 457.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   6 | Train Loss 0.1259 | Test Loss 0.0833 | Train Acc 96.45% | Test Acc 97.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 430.55it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1177.86it/s]\n",
      "  3%|▎         | 43/1641 [00:00<00:03, 425.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Train Loss 0.0959 | Test Loss 0.0774 | Train Acc 97.24% | Test Acc 97.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 430.77it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1180.93it/s]\n",
      "  3%|▎         | 44/1641 [00:00<00:03, 437.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   8 | Train Loss 0.0704 | Test Loss 0.0738 | Train Acc 97.84% | Test Acc 97.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 430.12it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1181.93it/s]\n",
      "  3%|▎         | 44/1641 [00:00<00:03, 433.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   9 | Train Loss 0.0588 | Test Loss 0.0725 | Train Acc 98.16% | Test Acc 98.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:03<00:00, 435.20it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1180.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Train Loss 0.0554 | Test Loss 0.0712 | Train Acc 98.34% | Test Acc 98.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = []\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krtk6WQbQma-"
   },
   "source": [
    "We see improvements from our use of a scheduler!\n",
    "\n",
    "Let's plot the change in our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "mnc3Fe0QQops",
    "outputId": "e25eb752-ca6d-4c0c-f838-dca549f60345"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feea21bde10>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data={'learning_rate': learning_rates}).plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUHqtcwoQNWc"
   },
   "source": [
    "Learning rate schedulers make training more stable, they also improve performance on average. While the performance gains in this example may not be as big, the improvements become more apparent the harder the task (and dataset) is.\n",
    "\n",
    "More advanced schedulers will not be shown here as the setup is too simple to warrant their use. They will be discussed as the need arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoqKQO09SPXB"
   },
   "source": [
    "# Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gr8uq6cTL8O"
   },
   "source": [
    "How do I know if my model has converged? Well the only way to find out is to train one. Let's reinstantiate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHhRuCIgQG4g"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(input_dim=784, hidden_dim=1024, output_dim=10, \n",
    "                      n_layers=2, activation='relu', dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "epochs = 50\n",
    "iters = epochs * len(train_loader)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=iters, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPmhRsJuTQpN"
   },
   "source": [
    "We'll be training this for *a lot* of epochs. We'll be taking note of the losses as we'll be plotting the loss curves later. This is the only way we can inspect if our model has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QKIwKY4QSUtC",
    "outputId": "a4c2cf30-ca18-47fd-f314-f55cda02decb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.91it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 819.02it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 274.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 0.9226 | Test Loss 0.1751 | Train Acc 85.32% | Test Acc 94.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.66it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 824.64it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 258.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 0.3049 | Test Loss 0.1505 | Train Acc 91.86% | Test Acc 95.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.42it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 824.06it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:05, 269.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 0.2785 | Test Loss 0.1426 | Train Acc 92.65% | Test Acc 96.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.33it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 812.10it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 268.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 0.2637 | Test Loss 0.1461 | Train Acc 93.31% | Test Acc 95.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.34it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 815.96it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 273.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 0.2499 | Test Loss 0.1319 | Train Acc 93.79% | Test Acc 96.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.28it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 830.24it/s]\n",
      "  2%|▏         | 25/1641 [00:00<00:06, 247.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   6 | Train Loss 0.2372 | Test Loss 0.1392 | Train Acc 94.16% | Test Acc 96.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.11it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 834.73it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 263.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Train Loss 0.2297 | Test Loss 0.1342 | Train Acc 94.50% | Test Acc 96.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 255.19it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 823.93it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 264.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   8 | Train Loss 0.2093 | Test Loss 0.1345 | Train Acc 94.96% | Test Acc 96.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.69it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 828.97it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 265.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   9 | Train Loss 0.2165 | Test Loss 0.1284 | Train Acc 95.02% | Test Acc 96.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.66it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 815.06it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 262.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Train Loss 0.2130 | Test Loss 0.1327 | Train Acc 95.15% | Test Acc 96.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.23it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 827.63it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 266.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  11 | Train Loss 0.1896 | Test Loss 0.1254 | Train Acc 95.58% | Test Acc 97.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.28it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 816.54it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 262.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  12 | Train Loss 0.1908 | Test Loss 0.1408 | Train Acc 95.65% | Test Acc 97.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.94it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 842.94it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 263.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  13 | Train Loss 0.1817 | Test Loss 0.1356 | Train Acc 95.82% | Test Acc 97.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.15it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 852.35it/s]\n",
      "  2%|▏         | 25/1641 [00:00<00:06, 248.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  14 | Train Loss 0.1759 | Test Loss 0.1532 | Train Acc 95.98% | Test Acc 97.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 256.15it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 840.97it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 251.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  15 | Train Loss 0.1667 | Test Loss 0.1343 | Train Acc 96.21% | Test Acc 97.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.83it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 847.37it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:05, 269.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  16 | Train Loss 0.1548 | Test Loss 0.1350 | Train Acc 96.33% | Test Acc 97.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.04it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 834.19it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 265.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  17 | Train Loss 0.1493 | Test Loss 0.1213 | Train Acc 96.51% | Test Acc 97.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 256.84it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 824.91it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 271.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  18 | Train Loss 0.1473 | Test Loss 0.1270 | Train Acc 96.66% | Test Acc 97.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.09it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 841.41it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 266.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  19 | Train Loss 0.1438 | Test Loss 0.1174 | Train Acc 96.88% | Test Acc 97.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.54it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 825.97it/s]\n",
      "  1%|▏         | 24/1641 [00:00<00:06, 237.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  20 | Train Loss 0.1294 | Test Loss 0.1169 | Train Acc 97.10% | Test Acc 97.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.47it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 837.93it/s]\n",
      "  2%|▏         | 25/1641 [00:00<00:06, 248.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  21 | Train Loss 0.1265 | Test Loss 0.1229 | Train Acc 97.18% | Test Acc 97.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.75it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 830.08it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 264.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  22 | Train Loss 0.1148 | Test Loss 0.1236 | Train Acc 97.41% | Test Acc 97.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.74it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 829.82it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 256.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  23 | Train Loss 0.1060 | Test Loss 0.1263 | Train Acc 97.60% | Test Acc 97.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.01it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 829.79it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 267.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  24 | Train Loss 0.1019 | Test Loss 0.1380 | Train Acc 97.61% | Test Acc 97.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.63it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 801.69it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 266.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  25 | Train Loss 0.1011 | Test Loss 0.1230 | Train Acc 97.71% | Test Acc 97.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.51it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 851.89it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 254.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  26 | Train Loss 0.0909 | Test Loss 0.1243 | Train Acc 97.92% | Test Acc 97.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.64it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 847.83it/s]\n",
      "  2%|▏         | 25/1641 [00:00<00:06, 242.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  27 | Train Loss 0.0795 | Test Loss 0.1256 | Train Acc 98.10% | Test Acc 97.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.45it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 837.74it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 264.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  28 | Train Loss 0.0825 | Test Loss 0.1259 | Train Acc 98.11% | Test Acc 97.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.41it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 826.74it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 258.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  29 | Train Loss 0.0718 | Test Loss 0.1333 | Train Acc 98.24% | Test Acc 98.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.16it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 825.12it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 258.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  30 | Train Loss 0.0648 | Test Loss 0.1291 | Train Acc 98.43% | Test Acc 98.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.57it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 814.84it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 273.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  31 | Train Loss 0.0674 | Test Loss 0.1256 | Train Acc 98.34% | Test Acc 98.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.69it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 782.98it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 252.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  32 | Train Loss 0.0587 | Test Loss 0.1397 | Train Acc 98.53% | Test Acc 98.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 256.49it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 833.44it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 256.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  33 | Train Loss 0.0527 | Test Loss 0.1373 | Train Acc 98.76% | Test Acc 98.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.53it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 846.26it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 264.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  34 | Train Loss 0.0488 | Test Loss 0.1459 | Train Acc 98.82% | Test Acc 98.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.88it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 813.76it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 258.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  35 | Train Loss 0.0452 | Test Loss 0.1435 | Train Acc 98.87% | Test Acc 98.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.95it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 819.46it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 276.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  36 | Train Loss 0.0482 | Test Loss 0.1359 | Train Acc 98.91% | Test Acc 98.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.87it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 829.45it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 272.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  37 | Train Loss 0.0418 | Test Loss 0.1452 | Train Acc 98.95% | Test Acc 98.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.17it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 825.85it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 267.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  38 | Train Loss 0.0384 | Test Loss 0.1486 | Train Acc 99.04% | Test Acc 98.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.87it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 798.84it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 264.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  39 | Train Loss 0.0339 | Test Loss 0.1543 | Train Acc 99.15% | Test Acc 98.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.75it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 837.18it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 264.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  40 | Train Loss 0.0357 | Test Loss 0.1545 | Train Acc 99.09% | Test Acc 98.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 253.97it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 833.81it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 265.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  41 | Train Loss 0.0333 | Test Loss 0.1481 | Train Acc 99.13% | Test Acc 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 253.28it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 853.99it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 270.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  42 | Train Loss 0.0295 | Test Loss 0.1526 | Train Acc 99.22% | Test Acc 98.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.29it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 848.72it/s]\n",
      "  2%|▏         | 25/1641 [00:00<00:06, 248.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  43 | Train Loss 0.0307 | Test Loss 0.1543 | Train Acc 99.27% | Test Acc 98.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.43it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 823.31it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 248.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  44 | Train Loss 0.0276 | Test Loss 0.1562 | Train Acc 99.29% | Test Acc 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.86it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 837.17it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 261.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  45 | Train Loss 0.0248 | Test Loss 0.1569 | Train Acc 99.32% | Test Acc 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.04it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 816.81it/s]\n",
      "  2%|▏         | 26/1641 [00:00<00:06, 252.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  46 | Train Loss 0.0255 | Test Loss 0.1574 | Train Acc 99.33% | Test Acc 98.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 256.99it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 824.77it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 268.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  47 | Train Loss 0.0261 | Test Loss 0.1573 | Train Acc 99.30% | Test Acc 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 257.56it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 829.38it/s]\n",
      "  2%|▏         | 27/1641 [00:00<00:06, 267.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  48 | Train Loss 0.0221 | Test Loss 0.1562 | Train Acc 99.38% | Test Acc 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 258.55it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 856.57it/s]\n",
      "  2%|▏         | 28/1641 [00:00<00:05, 272.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  49 | Train Loss 0.0246 | Test Loss 0.1564 | Train Acc 99.34% | Test Acc 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:06<00:00, 259.96it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 824.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  50 | Train Loss 0.0265 | Test Loss 0.1564 | Train Acc 99.37% | Test Acc 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(out, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(\"\\nEpoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Roho2Y7lUK_b"
   },
   "source": [
    "Let's plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "lEb6FDT1Sn9y",
    "outputId": "8a33e80b-a3f1-4dfc-f0ca-c3647e7f1f0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feea1ceb9e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(data={'train_loss': train_losses, 'test_loss': test_losses}).plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4q_LIg11UL3b"
   },
   "source": [
    "Once we see that the testing loss is stable and the training loss has stopped going down, we can say that the model has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e12acJT-UIYb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4wMcCWb88G2j",
    "I8MZo2dUhCm6",
    "xFV8za0PjVyF",
    "8v7z5of-oPy1",
    "fMMXmjQTtQWG",
    "I-pslIaWz4PC",
    "wSZN1E5Q9QFu",
    "xoEuhD2i79lV",
    "ZPfBtoRHOix-",
    "AoqKQO09SPXB"
   ],
   "name": "06 - Optimizers, Activation Functions, Regularization, and LR Schedulers",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
