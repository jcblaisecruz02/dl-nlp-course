{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehIT4T4PVCQ1"
   },
   "source": [
    "# 05 - NGram Language Models \n",
    "Prepared by Jan Christian Blaise Cruz\n",
    "\n",
    "DLSU Machine Learning Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZGcBjmpVLdl"
   },
   "source": [
    "First, we'll download the **WikiText-2** dataset to use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "d3CaLxtJrlgw",
    "outputId": "f9311003-71d3-48e4-d2c1-7110eb476306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-07 03:03:18--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.107.22\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.107.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "wikitext-2-raw-v1.z 100%[===================>]   4.50M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-08-07 03:03:18 (37.1 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n",
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "!unzip wikitext-2-raw-v1.zip && rm wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oxS74dDVRMw"
   },
   "source": [
    "Then we'll import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRBOHy-Su-qc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8YGLhDQVTeS"
   },
   "source": [
    "We'll load the dataset and make every word lowercase. WikiText-2 is pre-tokenized so we only have to split by space. We'll also remove blank lines and the headers per article so we'll only be left with the text bodies themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "woJo1K5m_yZE"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('wikitext-2-raw/wiki.train.raw', 'r') as f:\n",
    "    temp = [l.lower().strip() for l in f]\n",
    "\n",
    "# Remove blanks and headers, and tokenize\n",
    "X_train = []\n",
    "for line in temp:\n",
    "    if line != '' and not line.startswith('='): X_train.append(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FJs_dCGVfz-"
   },
   "source": [
    "Here's the first five lines of the dataset tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-Taw3pMR_Onx",
    "outputId": "910cfe4e-926a-4a7d-f0f4-a784ba3b16c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['senjō', 'no', 'valkyria', '3', ':', 'unrecorded', 'chronicles', '(', 'japanese', ':', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media.vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', 'employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', 'nameless', '\"', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '\"', 'calamaty', 'raven', '\"', '.'], ['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', ',', 'it', 'also', 'underwent', 'multiple', 'adjustments', ',', 'such', 'as', 'making', 'the', 'game', 'more', 'forgiving', 'for', 'series', 'newcomers', '.', 'character', 'designer', 'raita', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', ',', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', '.', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', 'the', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'n\", '.'], ['it', 'met', 'with', 'positive', 'sales', 'in', 'japan', ',', 'and', 'was', 'praised', 'by', 'both', 'japanese', 'and', 'western', 'critics', '.', 'after', 'release', ',', 'it', 'received', 'downloadable', 'content', ',', 'along', 'with', 'an', 'expanded', 'edition', 'in', 'november', 'of', 'that', 'year', '.', 'it', 'was', 'also', 'adapted', 'into', 'manga', 'and', 'an', 'original', 'video', 'animation', 'series', '.', 'due', 'to', 'low', 'sales', 'of', 'valkyria', 'chronicles', 'ii', ',', 'valkyria', 'chronicles', 'iii', 'was', 'not', 'localized', ',', 'but', 'a', 'fan', 'translation', 'compatible', 'with', 'the', 'game', \"'s\", 'expanded', 'edition', 'was', 'released', 'in', '2014', '.', 'media.vision', 'would', 'return', 'to', 'the', 'franchise', 'with', 'the', 'development', 'of', 'valkyria', ':', 'azure', 'revolution', 'for', 'the', 'playstation', '4', '.'], ['as', 'with', 'previous', 'valkyira', 'chronicles', 'games', ',', 'valkyria', 'chronicles', 'iii', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'game', 'where', 'players', 'take', 'control', 'of', 'a', 'military', 'unit', 'and', 'take', 'part', 'in', 'missions', 'against', 'enemy', 'forces', '.', 'stories', 'are', 'told', 'through', 'comic', 'book', '@-@', 'like', 'panels', 'with', 'animated', 'character', 'portraits', ',', 'with', 'characters', 'speaking', 'partially', 'through', 'voiced', 'speech', 'bubbles', 'and', 'partially', 'through', 'unvoiced', 'text', '.', 'the', 'player', 'progresses', 'through', 'a', 'series', 'of', 'linear', 'missions', ',', 'gradually', 'unlocked', 'as', 'maps', 'that', 'can', 'be', 'freely', 'scanned', 'through', 'and', 'replayed', 'as', 'they', 'are', 'unlocked', '.', 'the', 'route', 'to', 'each', 'story', 'location', 'on', 'the', 'map', 'varies', 'depending', 'on', 'an', 'individual', 'player', \"'s\", 'approach', ':', 'when', 'one', 'option', 'is', 'selected', ',', 'the', 'other', 'is', 'sealed', 'off', 'to', 'the', 'player', '.', 'outside', 'missions', ',', 'the', 'player', 'characters', 'rest', 'in', 'a', 'camp', ',', 'where', 'units', 'can', 'be', 'customized', 'and', 'character', 'growth', 'occurs', '.', 'alongside', 'the', 'main', 'story', 'missions', 'are', 'character', '@-@', 'specific', 'sub', 'missions', 'relating', 'to', 'different', 'squad', 'members', '.', 'after', 'the', 'game', \"'s\", 'completion', ',', 'additional', 'episodes', 'are', 'unlocked', ',', 'some', 'of', 'them', 'having', 'a', 'higher', 'difficulty', 'than', 'those', 'found', 'in', 'the', 'rest', 'of', 'the', 'game', '.', 'there', 'are', 'also', 'love', 'simulation', 'elements', 'related', 'to', 'the', 'game', \"'s\", 'two', 'main', 'heroines', ',', 'although', 'they', 'take', 'a', 'very', 'minor', 'role', '.'], ['the', 'game', \"'s\", 'battle', 'system', ',', 'the', 'blitz', 'system', ',', 'is', 'carried', 'over', 'directly', 'from', 'valkyira', 'chronicles', '.', 'during', 'missions', ',', 'players', 'select', 'each', 'unit', 'using', 'a', 'top', '@-@', 'down', 'perspective', 'of', 'the', 'battlefield', 'map', ':', 'once', 'a', 'character', 'is', 'selected', ',', 'the', 'player', 'moves', 'the', 'character', 'around', 'the', 'battlefield', 'in', 'third', '@-@', 'person', '.', 'a', 'character', 'can', 'only', 'act', 'once', 'per', '@-@', 'turn', ',', 'but', 'characters', 'can', 'be', 'granted', 'multiple', 'turns', 'at', 'the', 'expense', 'of', 'other', 'characters', \"'\", 'turns', '.', 'each', 'character', 'has', 'a', 'field', 'and', 'distance', 'of', 'movement', 'limited', 'by', 'their', 'action', 'gauge', '.', 'up', 'to', 'nine', 'characters', 'can', 'be', 'assigned', 'to', 'a', 'single', 'mission', '.', 'during', 'gameplay', ',', 'characters', 'will', 'call', 'out', 'if', 'something', 'happens', 'to', 'them', ',', 'such', 'as', 'their', 'health', 'points', '(', 'hp', ')', 'getting', 'low', 'or', 'being', 'knocked', 'out', 'by', 'enemy', 'attacks', '.', 'each', 'character', 'has', 'specific', '\"', 'potentials', '\"', ',', 'skills', 'unique', 'to', 'each', 'character', '.', 'they', 'are', 'divided', 'into', '\"', 'personal', 'potential', '\"', ',', 'which', 'are', 'innate', 'skills', 'that', 'remain', 'unaltered', 'unless', 'otherwise', 'dictated', 'by', 'the', 'story', 'and', 'can', 'either', 'help', 'or', 'impede', 'a', 'character', ',', 'and', '\"', 'battle', 'potentials', '\"', ',', 'which', 'are', 'grown', 'throughout', 'the', 'game', 'and', 'always', 'grant', 'boons', 'to', 'a', 'character', '.', 'to', 'learn', 'battle', 'potentials', ',', 'each', 'character', 'has', 'a', 'unique', '\"', 'masters', 'table', '\"', ',', 'a', 'grid', '@-@', 'based', 'skill', 'table', 'that', 'can', 'be', 'used', 'to', 'acquire', 'and', 'link', 'different', 'skills', '.', 'characters', 'also', 'have', 'special', 'abilities', 'that', 'grant', 'them', 'temporary', 'boosts', 'on', 'the', 'battlefield', ':', 'kurt', 'can', 'activate', '\"', 'direct', 'command', '\"', 'and', 'move', 'around', 'the', 'battlefield', 'without', 'depleting', 'his', 'action', 'point', 'gauge', ',', 'the', 'character', 'reila', 'can', 'shift', 'into', 'her', '\"', 'valkyria', 'form', '\"', 'and', 'become', 'invincible', ',', 'while', 'imca', 'can', 'target', 'multiple', 'enemy', 'units', 'with', 'her', 'heavy', 'weapon', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSAqgi8OVia6"
   },
   "source": [
    "We'll write code to produce ngrams from a series of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QckuXbhmtfrB"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(tokens, n):\n",
    "    temp = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        temp.append(tuple(tokens[i:i+n]))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTvCdQpJVmtL"
   },
   "source": [
    "Let's test it on the first line of the dataset and make bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "liTPAbhLugcN",
    "outputId": "71492268-33b6-4ef8-d149-5a78b9fd8001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('senjō', 'no'), ('no', 'valkyria'), ('valkyria', '3'), ('3', ':'), (':', 'unrecorded'), ('unrecorded', 'chronicles'), ('chronicles', '('), ('(', 'japanese'), ('japanese', ':'), (':', '戦場のヴァルキュリア3')]\n"
     ]
    }
   ],
   "source": [
    "print(make_ngrams(X_train[0], n=2)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ve7fCFVVpe0"
   },
   "source": [
    "Let's turn every line in the dataset into a stream of ngrams, then flatten them into one list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aA2sG1RSu3Y2",
    "outputId": "0d18ce72-4b43-48ba-b106-cd7ef070433b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17556/17556 [00:00<00:00, 21600.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('senjō', 'no', 'valkyria')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "# Produce then flatten the ngrams\n",
    "temp = [make_ngrams(t, n) for t in tqdm(X_train)]\n",
    "ngrams = []\n",
    "for t in temp: ngrams.extend(t)\n",
    "\n",
    "print(ngrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGU_L8xGVu0Y"
   },
   "source": [
    "Here's the number of ngrams in the entire stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q58t6BEjvKQz",
    "outputId": "1dfeab71-3cbe-4a8e-bb3b-dbd6dc4aad45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1972156"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPZFdl7q0Ijy"
   },
   "source": [
    "# Co-Occurence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY9onQK6Vy9v"
   },
   "source": [
    "Next, we'll have to construct the co-occurence matrix. For bigrams, this will be 2d, for trigrams, this will be 3d, and so on. This matrix will stand as our Maximum Likelihood Estimate (MLE) model.\n",
    "\n",
    "First, let's count the frequency of each unique ngram and the frequency of each word in the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgseITXcui5o"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count ngram frequencies\n",
    "ngram_counts = dict(Counter(ngrams))\n",
    "\n",
    "# Get word frequencies\n",
    "all_text = []\n",
    "for t in X_train: all_text.extend(t)\n",
    "word_counts = dict(Counter(all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fbSoTNAWTrH"
   },
   "source": [
    "Then we'll produce our co-occurence matrix using Pandas MultiIndexing (which makes querying and constructing our matrix much *much* easier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5cHsWKsxDBk"
   },
   "outputs": [],
   "source": [
    "# Get a list of all unique ngrams\n",
    "ngram_vocab = list(ngram_counts.keys())\n",
    "ngram_vocab_set = set(ngram_vocab)\n",
    "\n",
    "# Produce a blank co-occurence matrix\n",
    "indices = pd.MultiIndex.from_tuples(ngram_vocab)\n",
    "matrix = pd.Series(np.zeros(len(indices)), index=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nL8yuwEAWbbC"
   },
   "source": [
    "We'll then compute for the MLE of each word given its previous $n-1$ words. Again, this is described as:\n",
    "\n",
    "$$P(W_n | W_{n-1}) = \\frac{C( W_{n-1} W_n )}{C(W_{n-1})}$$\n",
    "\n",
    "Which essentially says that the probability of a word given $n-1$ context words is the frequency of the ngram (context, word) normalized by the frequency of the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QWNMzWjGzR6W",
    "outputId": "a753d12d-937e-4728-9a34-7139146a1586"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1406280/1406280 [00:29<00:00, 48024.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Copy ngram frequencies into the matrix and normalize\n",
    "for ngram in tqdm(ngram_vocab):\n",
    "    matrix[ngram] = (ngram_counts[ngram] / word_counts[ngram[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9rtR03P7AZR"
   },
   "source": [
    "# Using the Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDlvPN7OXBeI"
   },
   "source": [
    "We can then use our co-occurence matrix (our MLE model) to assign probabilities to each sentence. We use the Markov Assumption to calculate this probability:\n",
    "\n",
    "$$P(W_n | W_1^{n-1}) \\approx P(W_n | W^{n-1}_{n - N + 1})$$\n",
    "\n",
    "Which essentially means that the probability of the sequence so far can be summarized as the probability of the current token and the $n-1$ previous tokens in the sequence, based on the $n$gram used.\n",
    "\n",
    "We'll code this up by using *log probabilities* to prevent numerical instability. Remember that:\n",
    "\n",
    "$$p_1 \\times p_2 \\times ... \\times p_n = \\exp(\\log p_1 + \\log p_2 + ... + \\log p_n)$$\n",
    "\n",
    "We'll also add in a small epsilon value of 1e-18 in ``np.log()`` to prevent underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7vsKiru4Vf4"
   },
   "outputs": [],
   "source": [
    "def get_probability(sentence, n):\n",
    "    s = sentence.split()\n",
    "    ngrams = make_ngrams(s, n)\n",
    "    return np.exp(sum([np.log(matrix[ngram]) if ngram in ngram_vocab_set else 0 for ngram in ngrams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DNCyMtvYoEq"
   },
   "source": [
    "Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uIrQ6i8X4J4Y",
    "outputId": "c5838ab3-7293-4ccf-981e-afc7fbc5e229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.061936903452163e-05\n"
     ]
    }
   ],
   "source": [
    "s = \"i went to the kitchen\"\n",
    "o = get_probability(s, n=n)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1c86fc6YpOy"
   },
   "source": [
    "To handle unknown tokens, instead of passing 0 outright (which will cause numerical instability), we'll instead pass a very small epsilon value 1e-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0GwBiEf64lQ7",
    "outputId": "320b9b85-3b5a-444b-a00f-4ae361928776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.061936903452163e-05\n"
     ]
    }
   ],
   "source": [
    "s = \"i went to the unknowntoken\"\n",
    "o = get_probability(s, n=n)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDRi7kol7Cp8"
   },
   "source": [
    "# Predict the Next Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOxPVu99Y0KA"
   },
   "source": [
    "We can use a language model to predict the next word. Essentially, we compute the probability of the entire sequence, then find the maximum probability when multiplied to all possible next tokens. This is called **Greedy Sampling**. In the future we will look at better sampling techniques that produce better sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNGo2LbF5_R7"
   },
   "outputs": [],
   "source": [
    "def predict_next_token(s, n):\n",
    "    last_token = s.split()[-1]\n",
    "    prob = get_probability(s, n)\n",
    "    return (matrix[last_token] * prob).sort_values().index[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWRCowA-ZBuw"
   },
   "source": [
    "Let's test by predicting the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ty60U2lj57UP",
    "outputId": "9d5f327a-74e3-4c67-84f5-355dd494c19e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'first')"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token('energizer is a brand that manufactures batteries for', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMjWXt9eZDsk"
   },
   "source": [
    "Another test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "owIPvFQfHLGY",
    "outputId": "f5d0a303-44f0-48f0-fa83-07d031f30292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('your', 'face')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token('my friend and i went to see', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbnroYi-ZE0J"
   },
   "source": [
    "Let's write a helper function to generate the next $n$ tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPyXztTe6Rav"
   },
   "outputs": [],
   "source": [
    "def predict_text(s, n, n_words=10):\n",
    "    print(s, end=' ')\n",
    "    for _ in range(n_words):\n",
    "        ntok = predict_next_token(s, n)\n",
    "        if type(ntok) is tuple: ntok = ' '.join(ntok)\n",
    "        print(ntok, end=' ')\n",
    "        s = s + ' ' + ntok\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TtFh2xWZInb"
   },
   "source": [
    "And test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "safc5owj6S5V",
    "outputId": "32757dd6-1718-4e31-ed71-5d10c97f7869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my friend and i went to see your face of the united states . the united states . the united states . the united states . the \n"
     ]
    }
   ],
   "source": [
    "predict_text('my friend and i went to see', n=n, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "0ypKu2ab6p0Q",
    "outputId": "781ec11c-2931-464c-8117-b987cb05c3f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duty       ,            0.002336\n",
       "official   authority    0.002336\n",
       "knowledge  ,            0.002336\n",
       "magazine   ,            0.002336\n",
       "sister     ran          0.002336\n",
       "                          ...   \n",
       "senior     ...          0.002336\n",
       "...        enquiry      0.002336\n",
       "situation  makes        0.002336\n",
       "window     \"            0.002336\n",
       "tongue     to           0.002336\n",
       "Length: 343, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix['my']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLUlH7iO6qSb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfrNpanW-C0f"
   },
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-ClE5lPZKAN"
   },
   "source": [
    "To evaluate language models, we use **Perplexity**, which is the inverse of the probability of the testing set, normalized by the number of words.\n",
    "\n",
    "First, we'll load the test set of WikiText-2, then preprocess it like the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECLMScYNJjcP"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('wikitext-2-raw/wiki.test.raw', 'r') as f:\n",
    "    temp = [l.lower().strip() for l in f]\n",
    "\n",
    "# Remove blanks and headers, and tokenize\n",
    "X_test = []\n",
    "for line in temp:\n",
    "    if line != '' and not line.startswith('='): X_test.append(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vy0Ra_c7ZXYR"
   },
   "source": [
    "Then we code perplexity. Remember:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PP(W) = P(w_1 w_2 ... w_N)^{\\frac{1}{N}} \\\\\n",
    "= \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_N)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We'll add a small epsilon value to the denominator to prevent numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wlvtMVEFCqA"
   },
   "outputs": [],
   "source": [
    "def get_perplexity(s, n):\n",
    "    n_words = len(s.split())\n",
    "    prob = get_probability(s, n)\n",
    "    return np.power(1/(prob + 1e-18), 1/n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvfhccQKaaRA"
   },
   "source": [
    "Here's the perplexity on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4NOHeFVHITaS",
    "outputId": "7f08e561-e580-4f46-8f5c-57f8a3197e0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17556/17556 [00:42<00:00, 412.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.140256454834692"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_perplexity(' '.join(l), n) for l in tqdm(X_train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvyYX8BgacF5"
   },
   "source": [
    "And the perplexity on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8Hkbc8Z_F2ye",
    "outputId": "cd948d0c-4450-4971-ba51-759252a37976"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2183/2183 [00:01<00:00, 1157.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0040106231448584"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_perplexity(' '.join(l), n) for l in tqdm(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Y2Qkqq6G1Kv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04b - NGram Language Models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
