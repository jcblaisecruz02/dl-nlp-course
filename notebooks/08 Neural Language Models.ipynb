{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDe6wWrYM5TR"
   },
   "source": [
    "# 08 - Neural Language Models\n",
    "Prepared by Jan Christian Blaise Cruz\n",
    "\n",
    "DLSU Machine Learning Group\n",
    "\n",
    "In this notebook, we'll learn how to use RNNs for one of their most common use cases: language modeling in NLP. We'll start by processing our data then move on to an RNN walkthrough. At the end of the notebook, we'll implement ideas from a handful of papers, along with some de facto standard practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xH0_wEDD7dRe"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Goe_3FdDTjno",
    "outputId": "0c6dcc59-46fa-4b42-dc30-436de88201e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 25 06:58:45 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thewE05-SEyx"
   },
   "source": [
    "For this notebook, we'll use the WikiText-2 language modeling dataset (Merity et al., 2016). We first download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "KmxdeV56PVjm",
    "outputId": "a07cf5a3-a381-4115-b322-e1db63918556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-25 07:00:14--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.96.134\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.96.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4475746 (4.3M) [application/zip]\n",
      "Saving to: ‘wikitext-2-v1.zip’\n",
      "\n",
      "wikitext-2-v1.zip   100%[===================>]   4.27M  7.96MB/s    in 0.5s    \n",
      "\n",
      "2020-08-25 07:00:15 (7.96 MB/s) - ‘wikitext-2-v1.zip’ saved [4475746/4475746]\n",
      "\n",
      "Archive:  wikitext-2-v1.zip\n",
      "   creating: wikitext-2/\n",
      "  inflating: wikitext-2/wiki.test.tokens  \n",
      "  inflating: wikitext-2/wiki.valid.tokens  \n",
      "  inflating: wikitext-2/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip wikitext-2-v1.zip && rm wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMuBSwPFSOEY"
   },
   "source": [
    "Then we'll import some preliminary packages and set the random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9P7jglFrtkE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bz3PoPALzInr"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXWeWLtZSURl"
   },
   "source": [
    "We load the dataset as follows. No further preprocessing is needed as the dataset comes pre-preprocessed already, however we have to replace the newline characters with end of sequence characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OibqgGSfPbgM"
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "with open('wikitext-2/wiki.train.tokens', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if len(line) > 0 and not line.startswith('='):\n",
    "            train.extend(line.split() + ['<eos>'])\n",
    "\n",
    "valid = []\n",
    "with open('wikitext-2/wiki.valid.tokens', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if len(line) > 0 and not line.startswith('='):\n",
    "            valid.extend(line.split() + ['<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1290hrtuSgLJ"
   },
   "source": [
    "Let's see how the first twenty characters look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WDwALBzLppz9",
    "outputId": "aff8b5f4-0e87-4955-b379-fbd816dd8964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senjō', 'no', 'Valkyria', '3', ':', '<unk>', 'Chronicles', '(', 'Japanese', ':']\n"
     ]
    }
   ],
   "source": [
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5k7iYScSmCK"
   },
   "source": [
    "We then construct our vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O49o5nlDb4OV"
   },
   "outputs": [],
   "source": [
    "idx2word = ['<unk>', '<pad>', '<eos>']\n",
    "for line in train:\n",
    "    idx2word.append(line)\n",
    "\n",
    "vocab_set = set(idx2word)\n",
    "idx2word = list(vocab_set)\n",
    "word2idx = {idx2word[i]: i for i in range(len(idx2word))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wVDfUPWKcFhs",
    "outputId": "b0e4cca9-3931-4029-eb19-644ff36eaa57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(idx2word[42])\n",
    "print(word2idx['residual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWnOygZ8c_5y"
   },
   "outputs": [],
   "source": [
    "valid = [token if token in vocab_set else '<unk>' for token in valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7dpFGn6Sorr"
   },
   "source": [
    "And convert each token into its corresponding index. We can turn the lists into Tensors afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QlWak-iCrZFU",
    "outputId": "07640dc2-e321-4262-ce8b-20690f2150bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2024702]) torch.Size([211179])\n"
     ]
    }
   ],
   "source": [
    "X_train = [word2idx[word] for word in train]\n",
    "X_valid = [word2idx[word] for word in valid]\n",
    "\n",
    "X_train = torch.LongTensor(X_train)\n",
    "X_valid = torch.LongTensor(X_valid)\n",
    "\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9dyj6tJSu8I"
   },
   "source": [
    "We can see that our training set has about 2 million contiguous tokens in the training set (this is how WikiText-2 gets its name).\n",
    "\n",
    "Our next order of business is to figure out how to batch our data. We want to set a batch size (number of tokens the model will see at one time), then work out how to divide the dataset evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WY8cRULfrZv3"
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njrblBWtTMi9"
   },
   "source": [
    "It's easier to see how that function works in action. Let's produce dividends from our data with a batch size of 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHxPIVuEslJh"
   },
   "outputs": [],
   "source": [
    "bs = 40\n",
    "\n",
    "X_train = batchify(X_train, bs)\n",
    "X_valid = batchify(X_valid, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bz1OH4EvTUnh"
   },
   "source": [
    "Let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "BJvM5Ijzv2MT",
    "outputId": "6eb0e6da-8d24-4d78-bb11-c652dbd46383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26728, 15475, 23915,  ...,  8914, 16368,  3832],\n",
       "        [ 6939, 13704,  2565,  ...,   902, 27749, 26855],\n",
       "        [ 7632, 22425, 17206,  ..., 24026, 22356, 17206],\n",
       "        ...,\n",
       "        [ 2846, 26557,  9952,  ..., 17206,  7066, 22392],\n",
       "        [  550, 17940, 24426,  ..., 26557,  5745, 17206],\n",
       "        [17206, 26407, 17206,  ...,  5627,  8914, 18571]])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_ofz7iWYdpDr",
    "outputId": "78d2b79e-03f9-4582-8731-f7a33abc6fb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Senjō', 'no', 'Valkyria')"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[X_train[0][0]], idx2word[X_train[1][0]], idx2word[X_train[2][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7EEtrE0TXGu"
   },
   "source": [
    "And the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SNuKVwApv3k_",
    "outputId": "37617b85-c419-4cd2-d88e-c4be7a07d20e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50617, 40])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BroGDwk5TYVj"
   },
   "source": [
    "The indexes on the top row are the first tokens in their respective sequences, with the token proceeding it in the row below it.\n",
    "\n",
    "We currently have 40 dividends with about 52 thousand tokens each. We can't feed this much tokens into our model or else the hidden state saturates. We'll further divide our data to a specific \"bptt\" length or sequence length.\n",
    "\n",
    "*Note: BPTT length means \"backpropagation through time length\" which is the number of steps the model needs to backpropagate through to process the sequence. In modern literature we usually just use the term \"maximum sequence length\" or MSL.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R11UOBtewZ5L"
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i, bptt):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aphkxm2TUJT7"
   },
   "source": [
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RssL3Bkwsz0Q"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "\n",
    "x, y = get_batch(X_train, 0, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raFyLi_pULXa"
   },
   "source": [
    "The shapes will be as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GSXRPuCxs9ZO",
    "outputId": "4a8c1a77-8a55-4ad5-e7bd-f2374926919c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 40]), torch.Size([1400]))"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQGF2_CVUQgC"
   },
   "source": [
    "This is our training tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "BcHShXIsws9R",
    "outputId": "a4d3ce75-4937-4dd3-e4d1-90ad4395fc61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26728, 15475, 23915,  ...,  8914, 16368,  3832],\n",
       "        [ 6939, 13704,  2565,  ...,   902, 27749, 26855],\n",
       "        [ 7632, 22425, 17206,  ..., 24026, 22356, 17206],\n",
       "        ...,\n",
       "        [28392, 24026, 26557,  ..., 24026, 19439, 19817],\n",
       "        [23288, 17861,   294,  ..., 28392, 15095, 16316],\n",
       "        [18642, 13175, 28005,  ..., 11057,  9952, 11468]])"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIYkwcYxUSjU"
   },
   "source": [
    "And our target tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Yx_vwJQIyjJx",
    "outputId": "528ef487-7aa9-4912-ffa8-48a52849ae34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6939, 13704,  2565,  ...,  8914,  7498,  5005])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nv7MiNvUUM-"
   },
   "source": [
    "Our target tensor is basically just a flattened version of our training tensor sliced from the first index on the first axis, plus the token targets of the last row in our training tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Zx4sWGQ1ydpo",
    "outputId": "bfb3ae26-3dcf-4f2b-aa53-adaf5771c09b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6939, 13704,  2565,  ...,   902, 27749, 26855],\n",
       "        [ 7632, 22425, 17206,  ..., 24026, 22356, 17206],\n",
       "        [14163, 28005, 20790,  ...,   902,   392, 14230],\n",
       "        ...,\n",
       "        [23288, 17861,   294,  ..., 28392, 15095, 16316],\n",
       "        [18642, 13175, 28005,  ..., 11057,  9952, 11468],\n",
       "        [10854, 23839,   447,  ...,  8914,  7498,  5005]])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iufPQxbUkfb"
   },
   "source": [
    "We can iterate from 0 to the length of the full sequences but it's cleaner to simply put everything into a list we can iterate. We'll call these our \"dataloaders.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyKDTFnqunlk"
   },
   "outputs": [],
   "source": [
    "train_loader = []\n",
    "for i in range(0, X_train.size(0), bptt):\n",
    "    train_loader.append(get_batch(X_train, i, bptt))\n",
    "\n",
    "valid_loader = []\n",
    "for i in range(0, X_valid.size(0), bptt):\n",
    "    valid_loader.append(get_batch(X_valid, i, bptt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkuOCVwGUvI2"
   },
   "source": [
    "We can check the number of training batches we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BhOgK2o0xell",
    "outputId": "6cb03e82-7a7d-4da9-d68e-85b9491ae948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 151)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbS08QVrUyvj"
   },
   "source": [
    "And check the sizes of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cwR-zHwDxogR",
    "outputId": "1a49275a-639f-4653-cf0f-26022cdebf8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 40]) torch.Size([1400])\n",
      "torch.Size([6, 40]) torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "x, y = train_loader[0]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x, y = train_loader[-1]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUegUKvs42G6"
   },
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZlB991LVA-E"
   },
   "source": [
    "Let's take a batch so we can see how forward propagation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MI8HtiK5yPqD"
   },
   "outputs": [],
   "source": [
    "x, y = train_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GI6Wiu1cVGnA"
   },
   "source": [
    "We import our neural networks package from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZZic60a4050"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQPWnrLuVMXR"
   },
   "source": [
    "Our data isn't immediately usable to our model. We have to produce features from our tokens. One way we can do this is by using Embeddings. We assign a vector representation to each token in our training set, which will be trained alongside the neural network.\n",
    "\n",
    "Pretrained embeddings (like GloVe) can be used here to inject more information to the model, but we'll make do with untrained embeddings for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9tiQRV35Btz"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocab_set), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PSiAdgT9VuWu"
   },
   "source": [
    "Modules in PyTorch (subclasses of the ```nn.Module``` class) can be called like functions. To embed our data, we call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMp8WJR55HGS"
   },
   "outputs": [],
   "source": [
    "out = embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDndBQssV5jz"
   },
   "source": [
    "The resulting tensor ```out``` now has in its internal history all the operations that were carried out to result to its current form. This history tracking is how PyTorch does automatic differentiation when we do backprop. More on that later.\n",
    "\n",
    "We can check the resulting size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hRpqsxBS5Iow",
    "outputId": "0c7a9a56-91b9-481c-9ed0-a1b44bbe9390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 40, 100])"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3a5kBgRWGsj"
   },
   "source": [
    "Each of the 40 tokens in our tensor has been represented by a 100-length vector. This results in a three dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3CjFupt35JLZ",
    "outputId": "5ab2c3fd-a073-4d5f-e4f6-88ca64a3171f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGw13UBVWQSh"
   },
   "source": [
    "Instantiating an RNN is likewise straightforward. Here we make an LSTM, passing in the embedding dimensions and our specified hidden dimension for the LSTM's hidden weigh matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtkaTacB5KDV"
   },
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(100, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZApW4ZCWcVN"
   },
   "source": [
    "In PyTorch, we have to manually specify the starting hidden and cell state tensors. They follow the shape (1, batch size, hidden dimensions).\n",
    "\n",
    "*Note: The first dimension of the hidden and cell states can be larger if the LSTM has more recurrent layers or is bidirectional. For now, since we're using a basic LSTM, we just specify 1. More on this in the future.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-D3V8f45lbF"
   },
   "outputs": [],
   "source": [
    "hidden, cell = torch.zeros(1, 40, 128), torch.zeros(1, 40, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2rkpYU0WyMO"
   },
   "source": [
    "We get new hidden and cell states plus our output by calling our LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hd3tT-R5tET"
   },
   "outputs": [],
   "source": [
    "out, (hidden, cell) = rnn(out, (hidden, cell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO-sJqzyW3_c"
   },
   "source": [
    "We can check the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wIlKvTm85y89",
    "outputId": "fb852fec-2126-4e08-a5c8-001b3a6a0b38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 40, 128]), torch.Size([1, 40, 128]), torch.Size([1, 40, 128]))"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6ixGm3WW6-X"
   },
   "source": [
    "After passing our data to the LSTM, we need to pass it through a linear transform to get a distribution over our vocabulary.\n",
    "\n",
    "We make a linear layer, passing the hidden dimension it is expecting, and the output dimension it will result in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qztYr9KV54PT"
   },
   "outputs": [],
   "source": [
    "fc1 = nn.Linear(128, len(vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sz2p7_FaXLN9"
   },
   "source": [
    "Passing our current output is likewise easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulFYXcs26UyR"
   },
   "outputs": [],
   "source": [
    "out = fc1(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHLyUNqTXOai"
   },
   "source": [
    "We can then check our shape.\n",
    "\n",
    "For each of our 40 tokens on every step in the sequence (35 total steps), we have a distribution over 33,279 tokens, the highest of which corresponds to the predicted next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "05hn2fJD6WW6",
    "outputId": "a3ad48b1-4590-451d-91de-e1343174ef78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 40, 33232])"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GumJIfdBXc8_"
   },
   "source": [
    "We can check the loss by instantiating a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idywqzfu6XgN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yqyDfoxXftJ"
   },
   "source": [
    "Let's check our target tensor again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rv_gwFHM63__",
    "outputId": "bc6dd8b5-b075-4113-9095-4611bb9e4bb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1400])"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAHqQWT5XhzH"
   },
   "source": [
    "PyTorch losses do not accept 3D inputs, so we have to manually flatten the first and second dimensions of our logits like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eLElkf_h6zAh",
    "outputId": "e68d746f-2275-414d-98c9-e2b325782996"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1400, 33232])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(-1, len(vocab_set)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbNfP2-LXqns"
   },
   "source": [
    "We calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFD-aauN6nMg"
   },
   "outputs": [],
   "source": [
    "loss = criterion(out.view(-1, len(vocab_set)), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FuKTWlD4XsJ_"
   },
   "source": [
    "And display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6jt6AoYS6rTt",
    "outputId": "76dbedf8-0840-4842-cefe-f402888e6d79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4169, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVW7-TXX7XGv"
   },
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gfzbTgSXvne"
   },
   "source": [
    "Let's construct a simple training loop.\n",
    "\n",
    "First we import the optimizers from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irmzuAs_95Hu"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXPQ2VdRX0cs"
   },
   "source": [
    "We implement a function that detaches a tensor from its history. Remember that any resulting tensor will remember all operations carried from the moment it was instantiated. \n",
    "\n",
    "Our hidden and cell states will be reused per batch to carry information from the previous batch's timesteps to the current one, but we only want to backpropagate through the steps in our current batch. If we don't detach them from history, PyTorch will backpropagate our hidden and cell states *all the way to the start of the sequence* and we don't want that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e3DYJgx8xhb"
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor): \n",
    "        return h.detach()\n",
    "    else: \n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KQXxk7ZYcZu"
   },
   "source": [
    "We can create our model by subclassing the ```nn.Module``` class, overriding the contructor and the ```forward()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6z9Fr7J67KH"
   },
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_dim, hidden_dim):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, vocab_sz)\n",
    "        self.hidden, self.cell = None, None\n",
    "\n",
    "    # Initializes blank hidden and cell states\n",
    "    # creates tensors in the same device as the model's parameters\n",
    "    def init_hidden(self, bs):\n",
    "        weight = next(self.parameters())\n",
    "        hidden_dim = self.rnn.hidden_size\n",
    "\n",
    "        h = weight.new_zeros(1, bs, hidden_dim)\n",
    "        c = weight.new_zeros(1, bs, hidden_dim)\n",
    "        return h, c\n",
    "\n",
    "    # We want to reset the hidden states at the start of every epoch\n",
    "    def reset_hidden(self):\n",
    "        self.hidden, self.cell = None, None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bptt, bs = x.shape\n",
    "\n",
    "        # We initialize hidden states if we have none\n",
    "        # Otherwise, we detach the current ones from history\n",
    "        if self.hidden is None and self.cell is None:\n",
    "            self.hidden, self.cell = self.init_hidden(bs)\n",
    "        else:\n",
    "            self.hidden = repackage_hidden(self.hidden)\n",
    "            self.cell = repackage_hidden(self.cell)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out, (self.hidden, self.cell) = self.rnn(out, (self.hidden, self.cell))\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pwMza4R-ZGr3"
   },
   "source": [
    "Let's instantiate a model, a loss function, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jq3FhCjG9TLi"
   },
   "outputs": [],
   "source": [
    "model = LSTMLanguageModel(vocab_sz=len(vocab_set), emb_dim=100, hidden_dim=128)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOU9f31pZK4g"
   },
   "source": [
    "We can check the behavior of the model per batch. Let's verify if it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PX35vtt09ZE4"
   },
   "outputs": [],
   "source": [
    "x, y = train_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMq-ce3yZP1o"
   },
   "source": [
    "Same thing, except now we only call the model as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "piEPhlXS9ewR",
    "outputId": "a1c7f729-57a5-4358-ffd6-6aba68038369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 40, 33232])\n",
      "tensor(10.4165, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "print(out.shape)\n",
    "loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQ8RAYxTZTeS"
   },
   "source": [
    "We first clear out the optimizer because gradients get accumulated here. We then let our loss function backpropagate (using its operation history). Then, we let the optimizer perform one gradient descent step for each of the parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAIlcCKA9lv4"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqPJAyuDZjR0"
   },
   "source": [
    "Let's feed in the second batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YI71uiNM90n5"
   },
   "outputs": [],
   "source": [
    "x, y = train_loader[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llT39LRsZlCn"
   },
   "source": [
    "Same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "r8ejxu_e-C14",
    "outputId": "ec2121c0-47dd-450e-df5c-d9f14151824b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 40, 33232])\n",
      "tensor(10.1407, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "print(out.shape)\n",
    "loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cetjDyXZmLC"
   },
   "source": [
    "Notice that the loss has gone down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpWdeTjh-DkX"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9zwN6iO-n1W"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9FGbl0AZqFP"
   },
   "source": [
    "Let's train the model for five epochs to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zIHHN6Xn-WDH",
    "outputId": "34bd3117-10da-4134-dcb8-cb265d8237b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,727,888 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_sz=len(vocab_set), emb_dim=100, hidden_dim=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=20)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Crh89TkWZu3m"
   },
   "source": [
    "A basic training loop in PyTorch looks like the following, iterating over training and validation set batches. We set ```torch.no_grad()``` on the validation code as we don't need to backpropagate there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Iu0oNFKh_GNk",
    "outputId": "b23a64a9-6cb5-4797-f416-c45a59b7e897"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1447/1447 [00:19<00:00, 75.77it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 203.36it/s]\n",
      "  1%|          | 8/1447 [00:00<00:18, 76.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 6.9435 | Train Ppl 1036.3765 | Valid Loss 6.5669 | Valid Ppl 711.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1447/1447 [00:19<00:00, 75.63it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 203.15it/s]\n",
      "  1%|          | 8/1447 [00:00<00:18, 77.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 6.2582 | Train Ppl 522.2590 | Valid Loss 6.2196 | Valid Ppl 502.5220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1447/1447 [00:19<00:00, 75.75it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 202.51it/s]\n",
      "  1%|          | 8/1447 [00:00<00:18, 77.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 6.0385 | Train Ppl 419.2703 | Valid Loss 6.0870 | Valid Ppl 440.0840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1447/1447 [00:19<00:00, 75.70it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 204.19it/s]\n",
      "  1%|          | 8/1447 [00:00<00:18, 76.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 5.9082 | Train Ppl 368.0377 | Valid Loss 5.9948 | Valid Ppl 401.3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1447/1447 [00:19<00:00, 75.74it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 202.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 5.8134 | Train Ppl 334.7700 | Valid Loss 5.9822 | Valid Ppl 396.2922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    model.reset_hidden()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    model.reset_hidden()\n",
    "    valid_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3} | Train Loss {:.4f} | Train Ppl {:.4f} | Valid Loss {:.4f} | Valid Ppl {:.4f}\".format(e, train_loss, np.exp(train_loss), valid_loss, np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bP1vEIMeaCVU"
   },
   "source": [
    "Then we can generate a sequence from a starting word to see how the language model performs. Not bad for our first try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "RncCKp9CAWNN",
    "outputId": "21d0a6a1-4774-47fd-f6f2-a07ded56684c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this very could be attended , the Bishop of the kakapo , Uttar crimes on aggregate by an average and division with the closure in the rest of the Union , "
     ]
    }
   ],
   "source": [
    "nwords = 30\n",
    "temp = 1.0\n",
    "\n",
    "# Pick starting word\n",
    "word = 'this'\n",
    "ix = word2idx[word if word in word2idx else '<unk>']\n",
    "inp = torch.LongTensor([ix]).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate\n",
    "print(word, end=' ')\n",
    "model.reset_hidden()\n",
    "with torch.no_grad():\n",
    "    for i in range(nwords):\n",
    "        output = model(inp)\n",
    "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        inp.fill_(word_idx)\n",
    "\n",
    "        word = idx2word[word_idx]\n",
    "        print(word, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig42ELn-Dx5Y"
   },
   "source": [
    "# Better Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bgBFLg2aX36"
   },
   "source": [
    "In this section, we'll improve on our basic setup by adding in standard practices during training, adding some regularization via dropout, and adding in weight tying (Press & Wolf, 2016; Inan et al., 2016). We'll also add in some initialization, which we will see is important for getting better solutions.\n",
    "\n",
    "Weight tying ties the parameters of the embedding layer with the weights of the projection layer. This improves performance as well as reduces the number of parameters we have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqfZthcNAsU5"
   },
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_dim, hidden_dim, dropout=0.5, tie_weights=True, initrange=0.1):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim if tie_weights else hidden_dim, vocab_sz)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden, self.cell = None, None\n",
    "\n",
    "        if tie_weights:\n",
    "            self.fc1.weight = self.embedding.weight\n",
    "\n",
    "        self.init_weights(initrange)\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        weight = next(self.parameters())\n",
    "        hidden_dim = self.rnn.hidden_size\n",
    "\n",
    "        h = weight.new_zeros(1, bs, hidden_dim)\n",
    "        c = weight.new_zeros(1, bs, hidden_dim)\n",
    "        return h, c\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.hidden, self.cell = None, None\n",
    "\n",
    "    # Initialize embedding and projection parameters to a uniform distribution\n",
    "    def init_weights(self, initrange=0.1):\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bptt, bs = x.shape\n",
    "\n",
    "        if self.hidden is None and self.cell is None:\n",
    "            self.hidden, self.cell = self.init_hidden(bs)\n",
    "        else:\n",
    "            self.hidden = repackage_hidden(self.hidden)\n",
    "            self.cell = repackage_hidden(self.cell)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out, (self.hidden, self.cell) = self.rnn(out, (self.hidden, self.cell))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsNnaQMga9vZ"
   },
   "source": [
    "We instantiate a training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aySqUHHqEVpr",
    "outputId": "a379fb91-69c5-4dc4-cc26-2efc1ee8535c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,019,232 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_sz=len(vocab_set), emb_dim=650, hidden_dim=650, tie_weights=True, dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=30)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "en2_zhFsa_tZ"
   },
   "source": [
    "And train the model.\n",
    "\n",
    "We add in gradient clipping to prevent exploding gradients, as well as learning rate annealing when the validation loss fails to improve.\n",
    "\n",
    "*Note: We'll only train the model for 5 epochs to compare it with the earlier setup. The full model is trained for 40 epochs, for around an hour and a half on a Tesla K80 GPU. We'll load a copy of the fully trained weights later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZI8WvgPNEf44"
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "clip = 0.25\n",
    "best_loss = np.inf\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    model.reset_hidden()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    model.reset_hidden()\n",
    "    valid_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    if valid_loss < best_loss: best_loss = valid_loss\n",
    "    else: optimizer.param_groups[0]['lr'] /= 4.0\n",
    "\n",
    "    print(\"\\nEpoch {:3} | Train Loss {:.4f} | Train Ppl {:.4f} | Valid Loss {:.4f} | Valid Ppl {:.4f}\".format(e, train_loss, np.exp(train_loss), valid_loss, np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZqFH2l-m79i"
   },
   "source": [
    "Save the model weights if we train it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDcNkVTSmMn7"
   },
   "outputs": [],
   "source": [
    "#with open('weight-tied-lstm-40e.pt', 'wb') as f:\n",
    "#    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrbhkVwXm98d"
   },
   "source": [
    "I pretrained this model with our current setup so we can just download the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GB_oYJfDqK0K"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.us-east-2.amazonaws.com/blaisecruz.com/pretrained-models/weight-tied-lstm-40e.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-2nq6UEqVVW"
   },
   "source": [
    "And load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PrH_CGHcmZMB"
   },
   "outputs": [],
   "source": [
    "with open('weight-tied-lstm-40e.pt', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtHXgJKUm_hL"
   },
   "source": [
    "We can see how it fares on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "F5oUl39omlEX",
    "outputId": "0ed176e0-0bc1-4f96-eed5-bffca1724f56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:02<00:00, 71.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Loss: 4.7992 | Valid Ppl: 121.4092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.reset_hidden()\n",
    "valid_loss = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.view(-1, len(vocab_set)), y)\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "valid_loss /= len(valid_loader)\n",
    "print(\"\\nValid Loss: {:.4f} | Valid Ppl: {:.4f}\".format(valid_loss, np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZXyT-k8nA7_"
   },
   "source": [
    "Let's try generating a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rOL1XS3QE-xD",
    "outputId": "f4cce2cf-f211-4385-c823-72375cf4623a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow I 've been released in the United States . \" The Dreamscape \" is the sixth episode of the second season of the American science fiction television series The X @-@ Files . The episode was written by series creator Ryan Murphy , directed by <unk> <unk> , who had previously appeared in the episode , with the same name being directed by John <unk> . <eos> The episode received mostly positive reviews from television critics . The episode received mixed reviews from critics . The Edge said that \" The Secret of Monkey Island was a very good episode "
     ]
    }
   ],
   "source": [
    "nwords = 100\n",
    "temp = 0.5\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# Pick starting word\n",
    "word = 'Tomorrow'\n",
    "ix = word2idx[word if word in word2idx else '<unk>'] \n",
    "inp = torch.LongTensor([ix]).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate\n",
    "print(word, end=' ')\n",
    "model.reset_hidden()\n",
    "with torch.no_grad():\n",
    "    for i in range(nwords):\n",
    "        output = model(inp)\n",
    "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        inp.fill_(word_idx)\n",
    "\n",
    "        word = idx2word[word_idx]\n",
    "\n",
    "        print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "tmmw_yPZqb9G",
    "outputId": "e7881eef-11a0-4518-9062-887a653edbdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People 's <unk> , a separate subject of a <unk> full @-@ size : <eos> The novel is a real @-@ life pop and pop culture , for a rest of the era , the red @-@ brick model of one of the surrounding art . The film is believed to "
     ]
    }
   ],
   "source": [
    "nwords = 50\n",
    "temp = 0.8\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Pick starting word\n",
    "word = 'People'\n",
    "ix = word2idx[word if word in word2idx else '<unk>'] \n",
    "inp = torch.LongTensor([ix]).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate\n",
    "print(word, end=' ')\n",
    "model.reset_hidden()\n",
    "with torch.no_grad():\n",
    "    for i in range(nwords):\n",
    "        output = model(inp)\n",
    "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        inp.fill_(word_idx)\n",
    "\n",
    "        word = idx2word[word_idx]\n",
    "\n",
    "        print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSr4hvEOsdVf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07a - Neural Language Models",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
