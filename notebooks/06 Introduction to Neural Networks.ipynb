{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y54K2IajJAEB"
   },
   "source": [
    "# 06 - Introduction to Neural Networks\n",
    "Prepared by Jan Christian Blaise Cruz\n",
    "\n",
    "DLSU Machine Learning Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUjG6JvnTqRY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9inNeJD6BSf"
   },
   "source": [
    "# The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RblDZdT_BV9U"
   },
   "source": [
    "First, we fetch the original MNIST dataset. This will take a while to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fti7ca8hVDc4"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wt1Z6YFBaJv"
   },
   "source": [
    "The dataset has 70k samples of handwritten numbers. Each number is an image that's 28x28 pixels. If you flatten the image into one vector, you get a vector of size 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6ZZxR5RL7C2n",
    "outputId": "048e6e21-ad85-4c66-a0a2-fe12a4fadb82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnEV6-urBo1W"
   },
   "source": [
    "We can reshape a data sample into its original shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nMHQq_sd7I_U",
    "outputId": "00b3f24a-641a-4812-896b-1e9a9a16e2cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[0].reshape(28, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsHN3VjABr6Y"
   },
   "source": [
    "Let's plot some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "r98SIF-37K69",
    "outputId": "ca246644-fb7b-4d6b-87b0-9277ecc5929a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAADSCAYAAABXT0tTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOy9aXBd15Xf+zt3nucRwMUMgvMsUtRA\n0ZblqBXHtpyuxHZ6eF2vyi+VdKqTel31ulL5kI/58lKVD6mXkstOdVQux05aabu71bZli4oGauIo\nEiRAYiDGC9wLXNwRd77nfaDOFkBAEiUCF7jg/lWhMF/ss7DP/6y99lprK6qqIpFIJJLWRbfdA5BI\nJBLJwyGFXCKRSFocKeQSiUTS4kghl0gkkhZHCrlEIpG0OFLIJRKJpMV5KCFXFOV5RVFGFEUZVRTl\nLzZrUK2MtMnGSLusR9pkPdImXw7ly+aRK4qiB24DzwEzwIfA91RVvbl5w2stpE02RtplPdIm65E2\n+fIYHuJ3TwGjqqqOAyiK8t+BbwGfanRFUR6V6qP3VVUNSpusofqgc0XaZGMeFbtIm2zIoqqqwU/7\n5sOEVtqB6VWfz3z8NQlMfvxe2uQTMqs+lna5h7TJZyNt8gmTn/XNh/HIHwhFUX4A/GCr/04rIW2y\nHmmTjZF2WY+0yXoeRshngdiqzzs+/toaVFV9CXgJHqllkIa0ySeYVn28zi7SJnKubIC0yQPyMKGV\nD4EBRVF6FEUxAd8Ffrk5w2p5TNIm67DIubIOaZMNkDb54nxpj1xV1ZqiKH8K/BrQAz9WVXVo00bW\n2uwBbiFtspop5Fy5H2mTjZE2+YI8VIxcVdVXgVc3aSy7iRuqqp7c7kHsMDLSJuuQNtkAVVX3bPcY\nWo0t3+yU7FwURUFRFPR6PYqioNPpUBSFRqOx5k32rJdIdjZSyB9RDAYD4XAYp9PJ0aNHCQaD7Nu3\nD7fbzcjICIlEghs3bjAzM0MqlSKbzW73kCUSyafwyAm55oWu/hwQnqfmlWpvmjeqquqat1ZHr9fj\n9XoJBAIcPXqUrq4unnrqKcLhMBcuXGBiYoJisUipVKJYLEoh3wBtjuh0OrF6edRYbYPV95VczTWX\nXSfk2qQym81rJhaAyWTiwIED+Hw+9Ho9er0ej8eD0Wjkgw8+YGxsjKNHj7Jnzx46Ojpob2+n0WhQ\nr9eZnJzk5s2bzMzMcOXKlZa9aQ0GA8FgkGAwyB/90R/R1dVFb28vLpcLj8eDoigMDAwQiURoa2sj\nkUjwk5/8hF/96lfbPfQdRTgcxuv18thjj3H8+HE++OADzp8/z8rKyiPz0NPpdBw7doz29naOHDlC\nV1eXuOfefvtt3nnnHZaXl0kmk9s80t3PrhFybQLpdDoMBgMWi2WdkNtsNvbt20d7ezsmkwmDwUA0\nGsVms5FMJllYWGDfvn2cPXuWgwcPcvDgQer1OtVqlatXr2KxWNDpdFy7dq2lhdzr9dLe3s7Zs2fZ\ns2cPVqsVvV4vfiYcDhMOh2lvb6dcLvPOO+9s44h3Jk6nk/b2dp544glefPFFdDodV65cQVXVR0bI\n9Xo9PT09HDhwgH/0j/4Rx44dQ6e7l9GsKArj4+PUajUp5E2gpYTcbDZjtVrR6XTodDrsdjuBQACH\nw0EwGMRoNGI2m7Hb7QwMDGAymdb8vsFgoLu7G4fDITb4tM29p59+mmg0ypNPPsmRI0fwer2Uy2Vy\nuRzLy8vcvXuXsbExEolESy4XTSYTgUCAUCjEP/7H/5jOzk6i0Shms1ncfNVqdc2SWFEUzGYzoVCI\n7u5uMpkM2Wx2W5fMiqIQCoVwOBwYjUYMBgPz8/MsLi42dRw+n4/Ozk4ApqenSafTTf3724ler6ev\nr49gMMjZs2c5cuQI0WgUVVVpNBoiJNmK90mr0lJCbjKZcLlcwusOBAL09/cTCAQYHBwUIu7z+Xj8\n8cex2Wyf+XqqqrK8vEyhUODo0aN0dHRw6NAh+vr6qFarVCoVstksCwsLxONxpqenWVpaaskJajQa\nCYVC9PT08PWvf53Ozk68Xi9GoxG4Z4tarSbeVFXFbrdjMpnwer1EIhFUVaVQKABQr9e35ToURcHr\n9RIKhbDb7ZjNZorFYlOFXFEUnE4n4XAYVVWZn58nl8u15Lz4Muh0Orq6uujp6eHYsWMcP34cg+ET\nKZEi3nxaQsgdDgcOh4Njx45x9uxZjEYjJpMJm82Gz+fDZrPh9/vR6/UYjUZsNts6b3wj6vU6169f\n5+7duywtLZHP55mfn+fSpUusrKywsrJCLpcjlUoxNjbGxMQE+Xy+JSapllZoNpuFEH/jG9+go6OD\nSCSC3W5fE05RVZVcLkc+n2dkZISlpSUef/xx+vv7efzxx3E6nXz00Udcv36dqakpxsbGtu26wuEw\n3d3deL1erFYrMzMzTR+DzWbD6/Xi8XjweDxYrdZ1obzdiqIo2O123G43FotFrG53IyaTCb/fj91u\np6enB7vdTjAYxGKx4Ha7xQOsVqtx7do1kskkpVKJWq2G3+/HZrMRj8dJJpNks1lyudyWjLNlhDwS\nifD000/zgx/8QIj4w6B5oNevX+fixYuUy2Wq1ar4fj6fJ5fLiayNdDrNwsLCw15K01AUBaPRiN1u\np6Ojg4GBAb75zW8SDofx+/3CE9doNBpks1lSqRTvvfceo6OjdHR00N/fz6lTpzh16hSvv/46drsd\nnU63rUIeDAbp7u4mFArhdDq5fPlyU/++JuSaiGuC9qigCbnL5cJsNq9xCHYbZrOZaDRKOBzm3Llz\nBINBDhw4gNvtJhaLif97qVTi5ZdfZnh4mOXlZcrlMv39/YTDYS5dusStW7eYmZl5tIVci1UXCgVK\npZKI6X4e9XqdVCpFtVqlVquhKAp+vx+LxUK9XqdSqTA9Pc3Nmzep1WprwgWlUolKpUKtVqNarVIs\nFrfq8raEYDDIyZMnCQaDHDp0iEgkQjAYFEJ8PzqdDq/Xi8FgwG63YzQaSafTzMzM4PV6cTgc2Gw2\n8RrbhU6nIxaLsW/fPur1etNDPGazGZPJRGdnJ4cPH6ZarZJMJllcXCSVSrXcPPki6HQ6gsEgHo9H\nJAN4vV4ACoUC5XKZ69evMzY2xjvvvMPc3FzLbvza7Xai0Sjt7e2cO3eOQCDAvn37cDqdRCIRDAaD\nCLOazWYajQZ79+4lHA5TLBap1WoEg0EcDgeNRgOr1Uq9Xmd2dl0PsE2hJYS8VCqxvLxMNpulUCis\nicd9FtVqlXg8zsrKCoVCAb1ej8ViwWAwUKvVKJfLjI6OcunSpS2+guYTjUb51re+RWdnJ08++SRW\nq/Uzf157yDmdTpxOJ0ajkVQqxfj4OH19fTgcDpxOJ9FoFKfT2aSrWI9Op6O3t5cTJ04wPj7e9FWS\nxWLB4XAwMDDA6dOn+eijj/joo4+Yn58nmUy2RNjty2IwGERa7qlTpzhy5IhYGWtJAX/zN3/DL37x\nC/L5PPl8nlqtts2j/nK4XC4OHTrE3r17+cM//EM8Ho/Yn1MUhVKpxNDQECsrK2JlcvToURwOx7rX\nstlshEIh5ufnuXjx4paMtyWEvFarUSqVmJ6e5v333ycUChGLxahUKuRyOXw+HwMDA2KJV6/XyeVy\nLC4u8vrrr7O4uEixWESv1zM6OorH46GtrQ2DwUA+n9/mq9tczGYzDoeDcDhMZ2cnkUgEo9EoYpi1\nWo3l5WVKpRKLi4uoqsrAwAAOh4NKpSLCSEtLS4yNjVEqlfB6vcRi9zoW319QtV1sxxgURWHPnj30\n9fXR0dEB3HMyMpkMpVJp14q4tlpzOp2cOnWKnp4eQqGQyHhSVZXp6WnGxsaYmZkhm81SLpep1Wot\nlaarZWk5nU76+vp4+umnicViOJ1OdDodyWSSYrHI+Pg4mUyGoaEhCoUCdrsdq9XK008/TUdHB16v\nd03ot1AosLi4SKlU2rKxt4SQV6tVqtUqN27cQFEUuru7OXDgAJlMhqmpKfbt20dPT48Q8nK5zNzc\nHKOjo/z4xz9mcnKScrmMoih0dnbi8Xj4vd/7PaLRKMvLy9t8dZuLzWYjFovR29vLgQMHcLlca1Yw\nlUqFyclJlpaWuHTpEo1Ggz/4gz/A4XCIYpaFhQWmpqZYWVnh9u3b9PT0cPTo0R0h4Brb8UDR6XSc\nOXOGr3/96+zduxe4d5Mmk8ld5xCsxmg00tHRQVtbGy+++CL79u3D4/GI+HC9XufmzZu89dZb3Lp1\ni1Qqtc0j/uJohYQul4vu7m5OnDjBd7/7XZxOJxaLhUKhwOTkJHNzc/z85z9nbm6OW7duUSgUMBqN\nOBwOVFXl+PHjHDx4UAi5lhk3NTW1pVrTEkKukcvlmJ6eplQqUSqVKBQKLCwsiM03j8dDJBIRIZOx\nsTHy+byIdSuKQiaTEZucWjrhbkDbAO7t7eXs2bPs27cPm80mNjWr1SqpVIqlpSUuXLjA0tISS0tL\nGI1GRkZGWF5eZnx8nKWlJUZHR1leXqZarZJOp0Wc0+l00tbWRjAYxOfzUSqVWFlZado12u12nE4n\nDodDZEs0G6PRKMJzcG9TPB6Pt2ws+EHQQipayqq2hwKIcKeWydSq+fROp1Nsop86dYq9e/ditVpR\nVZVkMkkikeDChQvE43HhCBWLRRqNBj6fD7/fTyQSESsVVVWFAzo/P8/4+PiW2qalhDyRSJBMJkVh\nULVapVQqkUwm6e/vp7u7G7/fTyaT4X//7//NxMQEy8vLVCoV4N7TcXFxkcXFRZGytl350JuNw+Gg\nra2Np556in/zb/6NiGlrXuvKygrDw8OMjY3xwx/+kOXlZfbu3YvH4+G1117DZDLxxhtvMDMzw/Ly\nMsViUXgpWhGUVpY+PDxMZ2cni4uLTRNybaMtEAjg9/txuVzrMm+agdFoxGq1CiFPJBIMDQ21VEbT\nF8VisYjWFW1tbbjdbuBeptPc3BwLCwtcunSJd999t6VCKasJhUKcPn2a06dP88d//MeYzWYMBgPp\ndJqRkRFGRkZ46aWXSCaT5HI56vU6qqpisVgYGBigq6uLQ4cOsX//fpFMUCgUKBQKDA8P884772xZ\nxgq0mJBrhQaaMNfrdWq1GpVKhXK5TKVSQVVVjEYj4XB4w41RLY7Zqpsw96Pl1IdCIQYGBujo6BBV\nj1qMslqtsry8zPT0NLOzs+TzeRESWFlZoVarodfrxSTV7Hh/wzAtT9/v9zMwMIBOp2N2drYpsWGd\nTofH4yEcDmOz2Zqeu2wwGMSqx263i3lVrVZZWVkRc3K3oT242tvb6ezsFOEU7b6bnZ1lbGyM5eXl\nlnSKtOKy/fv3c/z4cfr6+rBYLFQqFeLxuKgruXv3LplMhmKxKEQc7lW5xmIxBgYGRCwd7t0zs7Oz\nTE1NiYSLrdSclhJyjXq9vibNq1arifSnRqOBy+XizJkzBAIBfvWrX7G8vLxrN6Lsdjt+v59Dhw7x\n/PPP09vbi91up9FoiLBSOp1mdnaWCxcuMDMzQzqdJpfLcevWLeF1A6Ki89Nspdfr0el0DAwM8MIL\nL/DGG29w9erVpthWKwvfs2cPgUAAk8nU1NCKJuA+n49gMCiygIrFIqlUqqkhpmah1+vF/Dp16hQH\nDx5Er9fTaDREaPPdd9/lvffeY2pqaruH+6UYHBzka1/7GseOHePrX/86BoMBvV4vEiXu3LnDz3/+\nc9LpNJlMZt3DymKxcPbsWU6ePEkoFBJfbzQaXLhwgfPnz3Pt2rUtr/xtSSG/n1KpxNTUFFarlXQ6\njcFgEDecz+cjl8uJ5dBuQlEUPB4PAwMD9Pb20tHRgc/nE42bbt++zcrKColEgsXFRRYWFkin00Kw\nv6w9tFYJzSyC0YpwtNRIuLdxWygUtnx1pdPpRPzT7/eLGGixWBSFZK0aUtgIRVHE6mNwcJCenh5h\nd23eJJNJUqkU8XichYWFlsufd7vdeDweenp6GBgYIBqNYrFYxF7S3NwcIyMjTE5Okk6nWVlZEf9j\nrWra4XDg9/txu924XC7hWCwuLpLL5ZiammJ2drYp7Rt2hZDPzc3xv/7X/+LQoUPs27ePaDRKf38/\nVquVQ4cOYbVauXHjxq7KLNDa8B4+fJh/+k//KT09PRw6dAi4J3C3b98WMb2JiQkhNloPmYfBZrOJ\nZmXNCm9oee5tbW3CG06lUltaLaeh1+t55plnOHnyJIcPH8btdpNKpchkMiKVc3VVcKtjMBjw+/3E\nYjH+1b/6V/T09BCJRMT3K5UKFy9e5Pbt23zwwQcMDQ21nJN04MABzpw5wxNPPMHXv/51dDqdeEAN\nDw9z6dIlfvrTn4qy+tUiroWbDhw4QEdHh3CgzGYz9Xqd999/n+HhYc6fP8/ly5ebEsbdFUJeq9XI\n5/MsLS2J1plarKu7uxtFUUTJvZbfms1mWzquqU0mn89He3s7fr9frEi0DILp6WlSqRTJZJJGo4HR\naFwT3/uybHSQQDNYfSwd3AtrZDKZTf0/ap01rVarSCuz2Wx0dnYSi8VwOBzodDoymYwIU+02j9xg\nMOByufD5fGIlonnjKysr5PN5Ef/NZrMt9RDT5q7P5xNtHmw2m6gHmJ+f59atW0xMTIiYuNbRUeuu\n2t7ejtvtFk6j0+lEr9eLyvOZmRmRpdIsjdkVQt5oNCiXy9y9e5cf/ehHHDhwgIMHD+LxePje975H\nNpvlxo0bLC8vMzk5SSqV4s0339yyctlm4PP5iEaj7N+/nxMnTojNt6GhIf7Lf/kvTE9Pc/XqVZF6\nCYj2ortBdFRVJZFIMDo6umlpXYqiYLFYsFgs7N+/n2AwyKlTp2hra+P48eN0dHSIlLSrV6/y+uuv\nc+XKFdERcrdgs9lEF9C+vj7C4bDYPB8bGyMej/Pb3/6Wa9eutVwdhslkwmw2s3//fn7v934Pt9st\nstmGh4e5cOECL730EoVCgXw+L+4VrXtoe3s7P/jBD4jFYvT394vsMIDh4WHm5uZ49dVXuXjxYlNT\nUneFkMO9G7tcLpNMJpmfn2dubo56vS4S+rVCIO1ghdu3b4s86Eql0nLHUrlcLjo6OoQnrm0Aa8UH\nCwsLa+J6D4v2ENjomLztQju96dOuUa/Xr8laMhgM4uSo1Z69Xq/HZrOJA0ksFgt9fX3CIw0EAmIe\naa+Xy+VIJBIt0w3zQdBaWHg8HlEApKValstlisUiMzMzTE1NiW5+reSNw70Vl16vx2q14vF4MJvN\nwL18eK3gR3s4aXphsViwWq2i90pXVxft7e1rNr0rlQrLy8vMz8+ztLTU9BX/rhFyuGfMhYUFGo0G\nP/zhD+nu7uYP/uAPCAQCnDp1SvyM1h/h1q1bXL58WaTkbWUJ7WZz7Ngxvvvd79Lb24uiKGK1MTQ0\nxI0bN8SScDNYXUW5k0TLZDLhcDg+tWWx1jNcG7vWykG7iTWhdzqdPPbYY2sOHNFKzxOJBOVyWaS6\naqGXxcVFUTi1W3A6nRw+fJi+vj6+//3vEwqFcLlcVCoV0dfmL//yLxkeHhaVv622utPOMrBarWvS\nBUdGRvjRj35EJpPBbrcTiUQ4deoUgUCAQ4cO4XK56OzsxGazEYlEMJlMa+oYarUaH330EVeuXCEe\njze9ZcOuEnKtNa1WTguIyk8tZU1rCtXZ2SlK+UulkjjSbad75tqpOH6/X8Tq4F7mztLSEul0mmKx\nuKXeQKPREAdQNJvV6ZEul4tQKCRqBu5Hs5H2IPL5fPT29goh125ih8NBNBrFbrcLD19LZ02n0yL2\nqc0NVVUplUoivbPV0R5cDoeDrq4uuru7RfGXXq+nVquJ9g3z8/PE4/FNdRS2g/tbPOh0OoxGIy6X\nC5vNRjQapbe3l2AwSH9/v2gYp1X2rt4j0u6HVCpFIpGgWCw2XUN2lZBrFAoFLl++zJ07d0gkEkSj\nUb797W/T3t7OwMAAdrudZ599lscff5xYLMbo6Chvvvkmt27dIp/P7+hUqlgsRltbG/v376enp0d4\no4lEgg8//JDx8fFNv8E08dImZyaTYWJioumnJWlFKI1GA51Ox1e+8hWOHDnCwsLChv097hdyvV4v\n7KXtqyQSCUqlEhcvXqRYLDI9PU0ul2NsbIxMJiNaOPy7f/fvCIVCGAwGsQKamZnZFYVl2ilQhw4d\n4s/+7M8IhUIEg0EMBoNYmaRSKWHnjfKpWwVNdFdWVshkMiJscvbsWQYGBsQekrba0w6q0XLntcZy\ner1edEMsFAqk02lu3rwpDqVpNrtSyLXuh+Vymdu3b5NOpzl27Bhw7+ZWVRWXy4Xb7aarq4t6vS5E\n//5io52G1WoVnehsNptIm1pZWWF+fp5MJrNp4mo2mzGbzWvyh2u1GplMhkQiseVpf6tRVZV8Pi96\nv+TzeXESlNVqJRAIrPsdj8dDNBoVfcu19MtarSZ6RheLRfL5vOjap1Xw3b59m0wmQ7lcxmg0UiqV\nxPmu2uqtXC437fq3EqPRiMfjEWezaj3GNRqNBplMhnQ6LbK+WhVtHmsdCbXmXx6PB6/XK76vzRXt\nga+tzgDRa0ZLvy0Wi+RyObLZ7Lb13NmVQq6h9SNPpVL86Ec/wuPxcPz4cdra2vj2t79NT08Phw8f\npr+/n1AoxFNPPcUrr7zC+fPnt3von4rmVWohllKpRLlcZmJignfffZelpaWH9si12PETTzzB4OCg\n6PSnnV36xhtv8LOf/YxUKtW05XWlUuFXv/oV7777LrOzswwODhKJRHC73esOBdFIp9PMzc2RTCa5\nc+cO+XxetCGYnJwUYqy1SV79fmVlBUVROHLkiNjgcjqd5HI5VlZWWmo/5fPQjgEcHBzEZDKtcwQK\nhQLnz59naGio5ZvMVSoVqtUq58+fZ2lpiXPnzvHNb34Tk8mExWJZ08Z5eHiYZDLJ5cuXRaqr3+/n\ne9/7Hm1tbXg8HtGATysc2i52tZBr1XelUomxsTGsVitms5l0Os2zzz4LII7qyufz2O123nrrLVGG\nvBNj5Vo8U4vRab1n8vk8iUSCQqHw0OPWJnVHRweDg4PCU8nn8ywsLDAzM8Pdu3ebGh9uNBri7MPh\n4WHgXrviYDD4ub87NzfH0NAQ2WyWubk5MpkMY2Njn+tZmkwmkaevLbO1Hvi7JTauKIqIjUciETGn\nNLRVy9zcHDMzMy3/ANMcj3g8DkBXVxdLS0uYzWZsNpvoQbSwsMDY2Bhzc3MiXFIsFolGo6KaWLOT\nFnbazhXarhZyDU3QK5UKV65cYWpqiu9///si1qooCtFoVCT579u3j0QiQSKR2O6hfy6rY34PW4Cg\nNcV6/PHH6e7u5oUXXuDo0aNYLBaWlpa4du0ab7zxBtevX9+WDR24Fza7dOkSIyMjD3zIdqlUIpvN\nrvG4HyTGq9fr6e7uZv/+/SLfeGJigjt37jA/P78Zl7Ot2O12AoEA/f39HDp0SOwBaBSLRVGmPjk5\nSSKR2BUPMIClpSUKhQI//elPeeutt0RaYr1eF6EU7cCQVCqFyWSira2NWCzG4OCgaCCmefCLi4tS\nyLcSzevQPI1MJoOiKOsmpCYK2oG6rdJfenXZvZYm90XR7KN54p2dnezbt4++vj46OztJpVIiTDE6\nOir2ErYDLSWwGQ9ZRVFwOp34fD4RckilUszOzu6KIiCz2UwgECAYDBIKhdZk8mgrvfn5eWZnZ8lm\ns7uqMdhqsR4ZGfncn3c4HFitVhwOB16vF4/HIxIAtGrX7dwA3tVCbjAYxGHBg4ODuFwuIpEIHo+H\n3t7eNSlE2ibaxMQEt2/fbpkbNZFIcOPGDaanp7+Uh6zT6QiFQjgcDh577DHa2tp45pln6OnpoVwu\nc/nyZV577TU++OADZmZmmJ+fbxnbbDaqqjI1NcXly5d3Rf/xffv28S/+xb+gra1NHAmo0+kolUrM\nz88zMzPDT37yE+7evcvi4uJ2D3db0Ur0tRXL6pbaN2/e5OLFi2QymW0b364U8tUeptfrxefzsW/f\nPoLBIH19fXi9Xvx+/5o8Um0zQ+tNshPj4xtRKBSIx+NfahIpioLBYMDj8YhWuNoyu62tjatXrzI5\nOcm7777L3//932/B6FsPrSXwbniYBYNBnn76aZxOJ3a7XXy9Xq+LDoBXrlwRp3I9ymhasXrFAvds\npe0bbWe22+cKuaIoMeC/AWFABV5SVfU/KYriA34GdAN3gX+iquq2lrkZjUa8Xi8ul4sjR47g9/s5\ncuQIHo+H7u5u7Ha7OFlGO+1ayxvV8ko3qSLroKIor7EFNlldyKAoCqFQiKNHjzI5OfmFSua1U1/C\n4TDPPPMMbW1t9Pf343a7WVpaYnp6ml/+8pdcunSJ0dHRzRj6ltmkhRlQFOUO23T/GI1G7Hb7unbE\nuVyOd999d03jp2YW/2ynTT6NYrHI6Ogoer1+R6adPohHXgP+b1VVLyuK4gQufXxD/h/A71RV/Q+K\novwF8BfA/7N1Q/18tMqstrY2Tp06RTQa5fTp0yLuff9pQXAvxry6Uf4m9Y64AfyOLbSJJtoul4uu\nrq51K4zP+h24Fx8dGBigp6eHZ599VpQfA+Im/uCDD3jzzTc3a8hbbpMWJKeq6sB23D9agdTqOgHN\ngSmVSty5c4fx8fEtP9lmI7bLJp+FdvamlnK40/hcIVdVNQ7EP/44pyjKLaAd+BZw7uMf+0vgDZps\ndK1ngtvtFmXFJ0+eFP0RtANVtdjfqmsS501ev36deDwu4sw3btzYrOFtmU1W33Ra24F9+/bx7W9/\nm0Qiwfj4uMiRtlgsBAIBXC4X/f392Gw2sWrROkQ6HA5RFLOyssJ7773H6OjoVsSBt2WebAbaSkib\nU5u02aolZTfVLv39/Zw8eZInnnhCVKrCJw/6SqXC9PQ0c3Nz29kUa0fNFYvFQnt7+5pq6p3EF4qR\nK4rSDRwD3gfCH4s8wDz3Qi9NxWAwYLPZCIfDHDt2jO7ubp5//nlcLhfhcPhTjwJrNBqk02mSySQX\nLlxgaGiIjz76aLOPq9oSm6wO+6iqKmzQ3d3NM888w9jYGIVCQVQsOp1O+vv7iUQinDt3TvRh1o7w\n0s7qXFlZYWZmhsXFRYaGhrh9+/aGZe8PybbMk83EZrPh8Xg262bWVLKpduno6OC5556jv79/TRdI\nbW5Vq1Vx0Pk2ep87aq6YTJ2wS9AAACAASURBVCY6Ojpob2/flkO/P48HFnJFURzAXwH/WlXV7Opl\nuqqqqqIoGwaWFUX5AfCDhx0ofHLQsJYG1NXVxalTp8ThqV6vl0AgIJraaNTrdZE3euvWLZaXlxka\nGiKRSHD79m1xBNpm0iybaAVCoVCIU6dO0d3dLRqCFQoF7Ha7qErs7u4WDaMURSGZTIo+7qlUirfe\neovp6WnGx8eFvTaTZtlkq9ns9r3NsovdbsfpdNLR0UFvb684Y1ILL2azWe7cucPw8DDLy8vb2t1w\np80Vg8GA2+3G7XY39azYB+WBhFxRFCP3RPwnqqq+8vGXFxRFiaqqGlcUJQpsuNZUVfUl4KWPX+eh\ndhG1zRntUIWTJ0/yz/7ZP8Pr9dLW1rZGvFdTr9eFh/Gb3/yGiYkJLl68SCKR2LKJ2iybfPwahEIh\nQqEQxWKREydOUK1WKRaLWCwWotHouslXLpeZmppieXmZmzdvEo/H+fu///vN2tj8tHE2zSabjRbO\n0kIsnzbXviBGaJ5dtNWrJuRarxCtv0g6nRab29o5lduVvbXT5oper8fj8YhGWTuNB8laUYAfAbdU\nVf2Pq771S+CPgf/w8ftfbPrgDAYMBgN9fX309vbidrvx+/14PB7C4TCxWEw0d1/tJWkHqBYKBTEp\nb968yeLiItevX2dxcXFTStk/hy2xSSaTYW5ujng8TjweX3NCCXzysKvX6+IgBK2DnVY0NDMzQyqV\n4vXXXyeRSDA/P08ul2tGr4gtsUmz0A67bmtrExvDD4n/4/dNsYt2cITVasVut4vwkHZU4tzcHG+/\n/bZIr9yMYwEfgh01VyqVCvF4HLvdTq1W2/Cgle3kQTzyJ4E/BK4rinL146/9W+4J+M8VRfk/gUng\nn2z24LRKw5MnT/IP/+E/JBQK0dbWhsvlIhAIfKoRK5UKMzMzxONx/vqv/5p4PM6VK1fIZrOUy+Vm\nLBcPAmm2wCbLy8vUajVxQnc4HF4n5FoMb/VNuPoEoUuXLnH37l1+/OMfMzs726ybdcts0kz8fj+d\nnZ2iD/xD4vo41W5L7p/70U67sdlsOBwOkcVVrVbJZrNMT0/z61//eiv2Rr4QzbTJg6JtANtsNur1\n+o4RcI0HyVp5G/i0UT+7mYNxuVyiWZPP58Pn8+FyuXjsscfo7e3F6XTidruxWCxrDKnlghcKBaam\nplhcXOTChQskk0nRxnZlZYVqtdos0bqhqurXtuKFK5UKhUKBmzdv8pvf/Iaenh727t1LIBAgFout\n+VlFUcQhG7lcjjt37pBOp7l69SrJZLLZx5RtmU2axf3ZHZvAbVVVT27Wi20GO6EQTlXVge0ew/3U\najXRFTGdTuPz+UQkwGaz4XQ6hcZsBzumslPbsAsEAjz33HMcOnSIWCwmWpW6XC5g45tIVVWx037+\n/HnGx8f5q7/6qzWnYO+ECboZFItFisUi77zzDh999BGHDx/mzJkzHDt2jPb29nWx8OXlZd555x0m\nJyd57bXXRMVes4s8Wp2d5oFJmouWR26320kkErjdbsLhMDqdDo/HQyAQYHl5eduqO3eMkCuKgt/v\nJxaL0dnZKYpcnE6nODC3UqmII7ay2awQomw2y9TUFIlEgkuXLrGwsEChUGjJQ5UfFM0zn52d5dq1\na2SzWRHj1jZjGo0Gy8vLXLlyhaWlJZLJpCh6kiL++TQaDVKplDjarNWpVCrigIhkMonD4RAOkuTB\nKJfL3LlzB1VVcbvdGAwG+vv7qVar5HI50USr2WmbO0bIdTodvb29HDlyhBMnTnDo0KF15+oVCgUS\niQRTU1PcuHFDpE1NTEzw6quvUigURBeyVj2K6kEplUqiPevNmzdFpd79aGEnLTNB+5rk82k0Gty9\nexdFUXj88ce3ezgPTaFQYGFhgampKcbGxohEImv2VySfTz6f54033mBqaoo9e/aIFhd79uwRjtTc\n3FxTT8+CHSTk2sEB2mbMzMzMup/RMisSiQSTk5NCoOLxuNjI3C39kh8U7WEGbGcV3q6k0WiI6ta3\n3nqLubk5bt68KQqnWg3tkIjp6WkuXLiAx+NhZGREeOrDw8NyDn0O2qljZrOZRCKB2WzG6XSiqip9\nfX3i8HctB79ZTpPSTO/s83I+tVJ6g8HwwN4lfHK4wg7yNC896CbWTsyZ3iJa0iZ6vR6dTofJZBIn\nyms94Ddh1ffANoHNqzkwGAziXtNSU7V7aieEkFRVfeANiWbPFZ1Oh9PpJBKJ8Od//ufiKESr1cp7\n773H1NQU//W//lcuX75MpVLZzBDLZ86VHeORwyce5U7sLiZ5NNHCdLvFU9USA3bL9TQbVVUpl8vk\n83lGRkaoVqs4HA6RYdfe3i4y67TDypvBjhJyiUQi2cloQr6wsMDLL7+M1+tlfn6e/v5+nnjiCdFR\n9M6dO+LYuGYghVwikUi+AFoYKpvNUq/XRbdRj8eD1+tlaWmJSqXS1ISLHRUj30W0ZDx4i5E2WU/T\nY+StwE6Okd/3t1EUBbvdLqpmdTqdOKBmk1sctE6MXCKRSFoFbZO42amGG9FsIV8ECh+/3w0E2Pha\nur7Aa+w2m8DGdpE2eTibwO6zi7TJer6UpjQ1tAKgKMrFndZf4suyWdeym2wCm3M90iZb+zo7AWmT\n9XzZa9l5jXUlEolE8oWQQi6RSCQtznYI+Uvb8De3is26lt1kE9ic65E22drX2QlIm6znS11L02Pk\nEolEItlcZGhFIpFIWhwp5BKJRNLiNE3IFUV5XlGUEUVRRhVF+Ytm/d3NQlGUmKIo5xVFuakoypCi\nKH/28df/vaIos4qiXP347YUv+Lotaxdpk/VIm2zMVthF2mQVWnXSVr4BemAM6AVMwDVgfzP+9iZe\nQxQ4/vHHTuA2sB/498CfP4p2kTaRNtkuu0ibrH1rlkd+ChhVVXVcVdUK8N+BbzXpb28KqqrGVVW9\n/PHHOeAW0P6QL9vSdpE2WY+0ycZsgV2kTVbRLCFvB6ZXfT7Dw0/ubUNRlG7gGPD+x1/6U0VRPlIU\n5ceKoni/wEvtGrtIm6xH2mRjNsku0iarkJudXxBFURzAXwH/WlXVLPD/AX3AUSAO/L/bOLxtQdpk\nPdImGyPtsp7NsEmzhHwWiK36vOPjr7UUiqIYuWfwn6iq+gqAqqoLqqrWVVVtAD/k3pLvQWl5u0ib\nrEfaZGM22S7SJqtolpB/CAwoitKjKIoJ+C7wyyb97U1BURQF+BFwS1XV/7jq69FVP/YicOMLvGxL\n20XaZD3SJhuzBXaRNllFU9rYqqpaUxTlT4Ffc2+3+ceqqg41429vIk8CfwhcVxTl6sdf+7fA9xRF\nOQqowF3g/3rQF9wFdpE2WY+0ycZsql2kTdYiS/QlEomkxZGbnRKJRNLiSCGXSCSSFkcKuUQikbQ4\nUsglEomkxZFCLpFIJC2OFHKJRCJpcaSQSyQSSYsjhVwikUhaHCnkEolE0uJIIZdIJJIWRwq5RCKR\ntDhSyCUSiaTFkUIukUgkLY4UcolEImlxpJBLJBJJiyOFXCKRSFocKeQSiUTS4kghl0gkkhZHCrlE\nIpG0OFLIJRKJpMWRQi6RSCQtjhRyiUQiaXGkkEskEkmLI4VcIpFIWhwp5BKJRNLiSCGXSCSSFkcK\nuUQikbQ4UsglEomkxZFCLpFIJC2OFHKJRCJpcaSQSyQSSYsjhVwikUhaHCnkEolE0uJIIZdIJJIW\nRwq5RCKRtDhSyCUSiaTFkUIukUgkLY4UcolEImlxpJBLJBJJiyOFXCKRSFocKeQSiUTS4kghl0gk\nkhbnoYRcUZTnFUUZURRlVFGUv9isQbUy0iYbI+2yHmmT9UibfDkUVVW/3C8qih64DTwHzAAfAt9T\nVfXm5g2vtZA22Rhpl/VIm6xH2uTLY3iI3z0FjKqqOg6gKMp/B74FfKrRFUX5ck+N1uN9VVWD0iZr\nqD7oXJE22ZhHxS7SJhuyqKpq8NO++TChlXZgetXnMx9/bQ2KovxAUZSLiqJcfIi/1WpMfvxe2uQT\nMqs+XmcXaRM5VzZA2uQTJj/rmw/jkT8Qqqq+BLwEj9TT8zORNlmPtMnGSLusR9pkPQ/jkc8CsVWf\nd3z8NcknSJt8gmnVx9Iu95A2+WykTR6QhxHyD4EBRVF6FEUxAd8Ffrk5w2p5TNIm67DIubIOaZMN\nkDb54nzp0IqqqjVFUf4U+DWgB36squrQpo2stdkD3ELaZDVTyLlyP9ImGyNt8gV5qBi5qqqvAq9u\n0lh2EzdUVT253YPYYWSkTdYhbbIBqqru2e4xtBpbvtkp2V4URcFgMKAoCnq9HgBVVVFVlWq1Kj6W\nSCTNQa/XYzB8Ir31ep1arfZQrymFfJcTCoX4yle+Qjgc5vjx4+h0OpaWllhaWuKVV14hHo+TzWYf\neiJJJJIH47HHHuO5555DURQArl+/zt/8zd881D34yAq5oijCkMCu9UydTidHjhyht7eXF154AZPJ\nxNTUFFNTU7z99ttks1kKhcIjKeTa/19RFHQ63Zr58Hlo86XRaOzKeaPZQqe7lw9Rr9e3czi7Ak1z\nurq6OHfunLBxvV7n7/7u7x7qtR8JIVcUBaPRiE6nw2g0YjQa6ezsxOVyYbFY0Ov13Lx5k9nZWer1\n+q64MY1GI06nk1gsxsmTJ4lEIsIGgUCAarVKJBIhk8mQTqcpl8vbPeQtx2Qy4fF40Ol06HQ6LBYL\nbW1t+P1+zp07h8vleqD/vaqqzM7Okkwm+fDDD7l06ZIQ9VZHs0skEsHn83HmzBlqtRqvvPIKCwsL\n2z28lubAgQMcPHiQs2fP0tfXRyKRYHx8nGw2+9Ca80gJucFgwGKxYLFY6O7uJhQK4XK5MBqNJJNJ\n5ufnUVV1V3gfBoMBh8OBx+Ohp6eHQCAgYuUul4tisYjH48HlconY+W5GURRMJhMulwuDwSDsMzg4\nSFdXF9///vcJhUIPvDK7ceMG4+PjpFIprl27titWNNrKxGw2EwqFiMVifO1rX6NWq/G73/1OCvlD\noCgKsViMxx9/nH379hEOh8lkMmSzWVZWVh769XelkOv1epxOJ06nk8OHD+NyuWhra8NmsxEIBLDZ\nbASDQWw2GyaTSdzk3d3dXL16ldHR0e2+hIemXq9TKBRIp9PcvXuXcrlMX18fRqNxu4fWNKxWKy6X\ni0AgQE9PD8FgkP3792MwGNDr9VgsFmKxGG63G5vN9oXCa5FIBKvVyle+8hV0Oh0jIyPcuHGDWq3W\nsqKueePt7e38yZ/8CdFolK6uLpaWlnC5XDgcDorF4q5wdJrJgQMH6Onp4dy5czzxxBP4/X5qtRrp\ndJo7d+4wPz//0Ku5XS3k4XCYJ554gmg0yuDgIC6Xi66uLmw225qfbzQaVCoVfD4fi4uLu0bIi8Ui\n2WyWubk59Ho93d3dj5SQWywW/H4/PT09PPXUU8RiMU6fPo3RaBQPb5/PtyabZ/X7z8Lv9+P3+8nl\nchgMBnQ6Hbdv3wZoWSHXbBIOh3nhhReIRCIsLy9TrVax2+1YrVbK5bIU8i+Aoihi/p08eZJDhw7R\naDSo1+tks1mmp6dZWlqSoRW4F/t0Op243W727NmD1+vl4MGDeL1eDhw4gNPpJBAIYDKZqFarIkuj\n0WiI0Ep7ezuKonDp0iUCgQArKyubsuTZLhqNBtVqlVKpRCaTwePx7IoY7hehvb2dr3zlK3R3d/PY\nY4/hdrvxeDzo9fovtcG5EZFIBEVRmJ6eJhQKkc1mKZVKm3QF24tOp8NqtYrVrd1uJ5fLUa1Wt3to\nLYXH4yEWi4m5l8vlWFpaYnx8nBs3brC8vCyFHO5t7Hm9Xrq7u/kH/+Af0NHRwTPPPIPD4RAbfHBP\n3BYXFymXyxSLRRqNBmazGZPJRFtbG16vl87OTgKBAMlksqWFXMsTL5VKpNNp/H7/IynkX/3qV4nF\nYhw+fFjMg80kHA4TDoe5desWkUgEVVVJJBKb/ne2A0VRsNvtOBwO7HY7drv9kdhP2Ww8Hg/t7e24\n3W50Oh2FQoHZ2VkmJia4devWpqxwWlLIdTodBoOBUCjEwMAAoVCIwcFBgsEghw4dwuPxiGwURVFo\nNBoUCgVyuRyvv/468/Pz1Ot19Ho93/jGNxgcHMRkute/yGq1YrFYWj4EodnIbrcTCoXwer1bImQ7\nmXw+z9TUFFardY3H02g0xMN8cnKSarUqPPPVP6fT6fB6vVgsFnw+H1artenXsJ2sXq087MqlVTGZ\nTPj9flwuF3v27KFSqXDx4kXy+TyVSuUzPelIJCK8cb/fj06nI5PJcOfOHd58801GRkY2LUOuJYVc\nyz7p7e3lO9/5Dp2dnZw5cwaLxYLVal036er1OsvLyywsLPA//sf/4MaNGxgMBmw2GwcPHhRCbjQa\nhQeyG4RcWxZHIhGCweAj501lMhlu3769Lq1Q2whOJpO88847FAqFDYVcr9ezZ88efD4fZrP5kRNy\nyT3Hrquri87OTn7/93+ffD7PzMwMMzMz1Gq1T/WmFUWhs7OTvr4+ent7iUajFItFlpaWuH79On/7\nt3/L4uLipq2SW1LInU4n7e3t9PX1sXfvXoLBoPCiV4t4o9GgWCySyWR48803mZqaYmZmhlwuh8Ph\n2NWhBq0k32QyiY2q1R65TqfD5/MRDAZxu90iDLObNrKWl5e5efMmlUpF5I7DPSFfWVkhnU5z7dq1\ndTn0mpjrdDpmZmbweDzYbDZsNhtms3nNAzGdTpPJZEReeT6fb94FNgmtkEVLX32UcLvdnDlzho6O\nDrq7u0mlUni9XnK53Gdm8GiFP8ePH6etrQ2z2czExATDw8OMjIywtLREoVDYtHG2pJD7/X6OHDnC\nyZMnefLJJ9cJuEa9XieVSjEzM8PLL7/MyMgIyWSSSqWC2WzehpE3D51Oh8lkwmKx4PV6cblca4Tc\nYDAQiUQolUpMTExQr9dZXFzcVUIej8dJJBJcvnyZ3/zmN2u87lqtRrVaZXl5+TNvRqvVisPhoKen\nh2g0isfjWeOZLywsMDIywsjICJOTky2bsbIRqqoKm5lMpkdSyEOhEN/5zndob2+nra2N2dlZOjo6\nKJVKLC0tUalUNvw9nU7HwYMHef7554lEIjgcDsbGxvjrv/5r7ty5w8zMzKYWHrakkDscDrq6uggG\ng2syDxqNBqVSiVqtRi6XI5vNcvXqVWZmZpifnyefz1Or1VAUBZvNJjJWdiOaLbLZrBAYv98vvEmT\nycSePXtwOBxMTk6i0+nI5/O7JuMCEMVd5XKZXC635nv1el00K/q0G0qn0+HxePD5fNjtdkwm07p9\nhkqlIuxWr9d33SpPVVV0Oh0ulwuv17um2dOjhPZQ0+l06PV6sf+2EdoKWMu91/bfgA1DeJtBS/5X\ngsEgJ06cIBaLrbmxqtUqqVSKXC7H2NgYs7OzvPzyy8Tjcebn58US2mAw4PP5CIfDWCyW7bqMLaVa\nrZJOp5mdneW9996jv7+fPXv2iJWIzWbj3LlzZLNZFhcXcbvdxONx0un0No9889AKfEql0pd6QOn1\nehEfDYVC2Gy2dUJWLBZZXl4WTsJuaO9wPwaDgba2NnK5HENDj1Z7cC37q1qtioe0Vhm8kZArikIg\nEMDv9xMOh/H7/ZhMJhqNhmgRshVJBy0p5FrsM5/PY7VaqdVqZLNZisUiCwsLFAoFJicnWVpaIplM\nksvl1iyftcIHLbNlN1MoFLhz5w56vX7dsl9rp6nX6zclp3q3oO0f2O12uru76ezsxOl0buiFLS4u\nMjw8TDKZ3BUirvWM0VYs2jXbbDYcDseuv1807HY70WiUnp4e3G43FouFYrFIPp8nm82Sz+fXheS0\nTLFoNEpnZyderxe9Xk+5XBab6/F4fN3qcDNoSSEfHh5mcXGRvr4+7t69SzqdZmhoiHw+z/z8vMid\nrtVqIkVo9U2m5ce63e5dv1RcXFzk7/7u74jH4/zzf/7P8fl82z2kHY/RaOTAgQO0tbVx7tw5urq6\naGtrW7NE1hgaGuJ//s//2dI1B6up1+tUq1WKxSKlUgmr1Yper8fn84l+PY8C0WiUF198kT179tDb\n24ter2d+fp7Z2Vmmp6eZnZ1dFx83Go1YrVZOnz7NiRMn6O3txWQykUgkWFhY4Pr167z//vtbso/S\nkv+VcrlMJpNhYWGBO3fuiDL0lZUVUqkU1WqVQqHwqR6S5mG43e4Nb87dhKqqVCoVUY2nKMqaTSzp\nhd9bmXg8HsxmM8FgEIfDwZEjRwgGg3R0dIisqNVUKhUqlQorKysUi8VdscmpeeLVapVMJoPL5cJs\nNq9pabvbaxE0r9rtdtPZ2UkkEsFgMFAqlRgfH2d8fJxcLke5XF5XcxCNRvH7/XR2dtLe3o7ZbKZS\nqXD37l1u3LjB9PT05+aef1laVsi1UvvR0VGRhbC6P/RnGUun09HR0cHAwAAOh6OJI99eVvdgv3+F\n8ihjs9k4efIkbW1tfOMb3yAajdLe3o7NZsNisYiY6Gq0wzlSqdSu2eRsNBqsrKywvLzMnTt3qFar\nuFyu7R5WUzGbzXg8Hnp7e/nqV78qwiOJRIKf/vSnTE5OMjs7u85RNJlMPPfccxw8eJCvfvWr9Pb2\nCsfylVde4eWXX97S9N6WFHK4N+m0fiL3oz1Rtdjv/axOydOyVsrlMpVKhUKhQD6ff6T6SeyG2O5G\naN6V2WzGbreLpe/9Dy6n08nevXuJRqPEYjGCwSBerxez2Sz2DrQMmHQ6TaFQYGpqivn5eVHUsRuE\nHD6JkWsbfI8Ker0eo9FIIBCgv7+fnp4e0fJ4aWmJRCIh0lm1IxLh3hxzu93Cg+/q6hIr/fn5eZLJ\nJMlkcsuTCFpWyD8Lt9vNs88+i9PpFCljqz1Rk8nE4cOHhdelqirz8/MkEgmRsL8bJ7G2Ulkt3LtV\nxOFeVZ7X66Wjo4OTJ08SDAYZHBxct2GnVfg6HA6xAX7/xmapVKJarfK73/2Oq1evirzx+fn5LVsu\nS5qH1i318ccf50/+5E8IBoMEAgEWFxd54403uH37NteuXRPdIDXMZjNPP/00PT09PPfccwwODoqW\nEBcvXuTDDz/kzp07Wz7+lhdyLbdTO/nH5XIRDAbp6+vD6XSKGN9qITcYDPj9fhwOBzqdjlqtRiKR\n4O7du6RSqXXxr93EF2nV2qpo2Tg+n4+uri66urro6+vD7/fT3d29TsgtFgvBYFCkZm5kG221Fo/H\nmZiYYGZmhng8Tj6f39W21NCym3YbZrNZnIg0MDBAb28vHR0dOJ1OEU7TwiF2u13UJGjphCaTiVAo\nRFtbm6gALpVKlMtl4vE4U1NTW5Klcj8tLeSaIe12O5FIhI6ODp5//nmi0SinT58Wy+iNYsOa51Uq\nlUilUrz66qv87ne/4+7du7tmmfyooj3Mn3nmGb7//e/j8XiIRqMizHI/2jz6NBqNBtPT08Tjcd5+\n+21ef/11sYH8KMwVnU4nehDtls1OzQHs6uriwIEDnDlzhu985zs4HA7RYE677oGBAVwuF/l8nng8\nzm9/+1tyuRx2ux2fz8exY8c4cOAAXq+XRqPB0NAQ09PT/Pa3v+Wtt96iWCxu+fW0pJBrS1/NkFrv\nlVgsRl9fH6FQSPQf1/psaPHO+8MKWiXoysoK2Wz2U0tuJa2D1mPG7XYTi8VwOp34/f7P3NRd/aD/\nrJ+r1Wpis303tTP4PGw2G3a7fdekH5pMJnGkXXd3Nz09PXR1da3pmKpVgTudTmq1Gj09PeKc12w2\ni8vlEv2KfD4fBoOBWq3GwsICExMTzM/Pk8lkmnI9LfVf0RpBeTwewuEwx48f57vf/a64UY1Go/Aa\ntN33iYkJFEVh79696/qTa6+p0+no7+8nk8nw4Ycfkk6nd9UG1qPKg6ZYrl6xwfrQil6vp6+vj/b2\ndk6cOEEqlWJiYoK5ubnNH/QOxGKxCI9zt2R5dXV1MTAwwFe/+lVefPFFnE6n2NQGRC8Vrbo3FovR\n399PPp/n5MmTlMtlvF4vNptNZL9p1dS//OUv+e1vf0sqlWra9bSUkBsMBnESekdHB729vRw4cACr\n1YrZbBZd7bQ4lpZdYDAYiMVionPd6ptWy2zw+Xx0dHQwOjoqjrTajd75RnnkRqNxwz4irYrmTWkP\n80ajIUIn2kbV/emX94feNHtYLBZ0Op3ofOh2u3G5XLu6/mB11orWa8XhcKyJG7cqWippIBAQWSax\nWAz4pOaiWCxSKBRIJBKiZ4rZbMbpdOLxeKhUKtTrdZHZpBUWammo2ms0Gg30er1Iid7S69rSV98k\ntHhVJBKhq6uLJ598kt///d/H6/USDAaZm5vjnXfeIZVKMTY2RjabZXx8XNy0fr8fn89HX18f4XB4\nTfc67SFw9uxZjh8/LsR+cnKSycnJXZMjDBsLlnaWp6qqvPHGG9s4us1DO27t17/+NXfv3iUWi3Hk\nyBHRg0e7ET8Nk8nEiRMnCIfDnD59mkgkAuzuDeLVlEolhoeHqdVqHD9+HLvdvt1D2jS6u7uJxWJ8\n4xvf4Pnnn8fj8YgiqEqlwvXr1/nFL34hQq0dHR1885vfFGe/mkwmurq6UFVVrO4159BqtWIwGPiX\n//Jf8q1vfYuf/exnvPvuu2QymU1tWbsRLSHkRqMRs9ksvOb+/n6OHDmCoijCCx8bGyORSDA0NCRO\np240GgQCAdGXfHX+p7ZRpaUmBgIB8ZTu7OykUCiIDJaNsli0fNtWR6/X43a78fv9wvts9evSTrKP\nx+OiXYPFYmF5eZmhoaF1TZDux2az4XQ6KRaLHD58+JFJ19TQTnjPZDJrbLT6nNNWtYO2mtdCK+Vy\nmZWVFUqlErlcjqmpKa5evUqxWGRlZYVCocDc3ByqqtLe3o5er8dms60L12nZcDqdTuzTnT9/HpPJ\n1JRsnx0t5NqkOXjwIIcPH+bo0aOcOXNGtK9NpVJMTk5y9epVfvOb35BOp0VPbS2l7I/+6I/o6uri\nyJEj+Hw+jEYjxWKRAWIYjgAAEQlJREFU999/n0QiIdIQe3t7CQaDPP300+zdu5fR0VFGR0cZGRnh\n2rVrVKvVNQcQFAoFFhYWWmpCb5RHbjQa2bdvH+FwmL6+PhYWFlhaWmrKTvtWUyqVWFxcZGVlhXg8\nTrVaFaljn/Ww0kqyQ6EQjz32GN3d3c0b9A5AE/LVvdq1ghmv14vf72/JxABFURgYGOBrX/sa4XBY\nNDy7cuUKU1NT3Lhxg6WlJWZmZkTTMG2zsru7G4PBQDAYFOX3G6GqqujFMjU1xcLCQlNqUna0kGuT\nJxKJsH//fg4dOsSxY8dEa8lcLsfs7CwzMzPcvXuXQqFAsVjEYDDg9XoJBAIcPXqUnp4eAoEAFouF\nUqnEysqKCJ20t7eLntNut5tQKEQ4HF6ThpRMJimVSkLctKdvIpFoGSHX2hjUarV1m71erxej0YjP\n58Pj8YjTT1od7WbUUkwflNUrvd1ghy+K1st+9UpU88ZNJhNWq7VlT0LyeDyiAVo6nWZqaoqPPvqI\nkZER3n///XX3c61WEwWCS0tLWK1W6vX6urYgmmPQaDRIpVIiY6VZ82dHCrkWyz1y5Ah79+7l6aef\n5qmnnsLlclGpVBgdHeXixYtMTk5y6dIlFhYWyGQyYlMzEonw/PPP097ezoEDB3C5XORyORYWFnjt\ntdeYmprigw8+YGFhAbfbjc1mY3BwkGg0Sm9vL21tbbjdbvr7+wkGg5w+fVqIQrlcJp/Pc/nyZf7z\nf/7PLXMQQzab5cKFC3R3d3PkyBFsNtua7+v1eg4fPoyiKLz22mtNS5vaiRgMBjEPnE7ndg9nW9DE\nSROo1ce9fVrri1ZgZmaGK1eusLCwQDKZZHp6momJCXK53IZOmZbm7PF4aG9vJxQKYTQayeVy/O3f\n/i0LCwssLi6K1Xqj0WBycpJUKsXo6GjTrmtHCrm2gdDR0cHhw4fZv38/g4ODVCoVSqUS8Xicixcv\ncvfuXS5evChywS0Wi4hznzlzhkgkQigUQqfTEY/HSSaTXLx4kZGREYaGhlheXsZqtWI0Gkkmk4TD\nYfL5POVymb1799LZ2bmu812hUBDn7bVSpZvWhU2n07F///5139fr9UQiEQqFQktvbj1s/FYTq2Aw\nSFtb2649eOSz0PrKbLQh/FmHKux0VFVleXmZ6elpbt26xfDwMLlc7jOdFr1ej8ViESeKOZ1OdDod\npVKJa9euMTo6yvT09JrNzFQqJdoAN4vPFXJFUWLAfwPCgAq8pKrqf1IUxQf8DOgG7gL/RP3/2zu7\nmDiv9I7/DvYADmZwIWCGGYYvEwiGmK3sWsaRXKu1Uvtm1VxU7cWqlSJtb1bqSq3UaK/2cm+6F7mp\nlGo36sUqTaVdpSslUeOsUyyH2LjGJmA+DDPGIeP5HsbzAcPAzOkF856CgRjsd77w+UmjmXkZ3o+/\n3vO855znOc8j5dKLnpAxj2W327l48SIXLlygoaFBeZRHR0d58OABt2/fRkqJ0+nk2LFjagl2f38/\n9fX19Pb2ks1muXHjBpFIhLGxMYLBIHfu3FHzprBh4NbX11lcXCQUCuHz+RgZGaGvr4++vj4cDgcd\nHR3q/FwuF59++ikff/zx93mi+4UQV83SxAzW1tYIBoNYrdZipVzNmybGsL+xsZGmpiaV5MhIy7pX\nqqqqlL/g7bffVr3yPNIthJjDxPZjBisrK0xPT5NOp/F4PFgsFurq6qipqeH06dPU1NTw+eef5y0S\nI5+azM/PE4lElDN3t/lrYxrJqA/c3d2tiotEo1F8Ph8TExPMzMyQTCZVm5JSsrq6uuuDMF/spUe+\nDvyjlHJMCFEL3Mk1yL8D/iCl/IUQ4l3gXeCfX/SEhBDYbDZ6e3s5efIk/f39Ksrgu+++Y2RkhMXF\nRRYWFqivr6ezs5O2tjYuXLigFglVVVVhsVgIhUIqudG1a9cIBoOEQqEtThpD8HA4DGwMvWAjTenS\n0hKvv/76ltjzyclJhoeH1VBzl97fJPAHszQxA6OKUiwW29XRl+cc5XnTxBjBHTt2DKfTCWz0itbX\n1/fVmCorK2lvb6e9vZ2zZ8/S1dWV7yLdcSllt5ntxwwMA15VVUU4HKahoYG6ujqqqqro6OhASsmN\nGzfydvx8auLz+fD5fM/8nbEavLa2lo6ODhwOh0rrkUgkWFpaUn62UuCZhlxK6QW8uc9xIcQ0YAd+\nCPxp7mf/DvwPJhnykydPcunSJdra2tQ2IQROp5OLFy+qGM+6ujrsdjtWqxWHw6EWbSSTScbHx/F4\nPFy9ehWfz6cKT+y1YRtRDgsLC4yNjantfr9/r4m1TNPEDBKJBKOjo4TDYa5cuYLFYtmywOPw4cMM\nDAzQ3NysHpbGNJOJmKpJY2MjtbW19PT00N7eTldXF52dnVy9ehWv16tCTnfD6HVVV1fjcDg4fvw4\nly9fpq2tjePHj2OxWNRDLZPJqPwqJuZYCefeS+peMUin07jdbg4dOoTVaiWbzXL9+nXGx8cJBoP5\nPnxRNamvr2dwcJDu7m7OnTtHQ0MD6+vrBINBPvzwQ9xut+r8lQL7miMXQrQDPwBuAcdzRh7Ax8bU\ny07/82Pgx/s4Bp2dnZw9e1bN1RpD5+bmZk6fPq1WeNbW1tLU1LRjHcWxsTHcbrcyXvtteEbhgBfA\nNE3MYGVlhampKRWSZ9SkNDh8+LBahm6326mrqyOdTpttyE29T+rq6mhpaeHcuXOcOXNGGfJAIMAn\nn3zyzCkkIQTV1dVYrVa6u7txOp0MDQ3hdDq31XM1Rm5G5I9Jhtx4ypTUvWKwtrbG48ePOXLkCD09\nPWQyGe7evcvNmzcLEVJXVE2sVisDAwP09PRw6tQpKisricViBINBPvvsM9xud0kVKt+zIRdCHAV+\nC/xUShl7KjeFFELs2D2VUr4PvJ/bx568UMYSeyPUyVhGbzgajO+pVAqXy8Xy8jLBYJBEIoHH4yEc\nDnPr1i1CodD3lnzLJ2ZrYhaZTIZIJKKGzMZS80KktzVTEyOq6cyZM7zxxht0dXWpeqR2u53z58+r\nYrm7UV1dTVtbG1arlb6+Pl599VUaGxvV8vxsNkswGFQFrA1n+cOHD4nFYs+lwU6U6r3y1HlscYLm\nu00VSxOLxcIrr7xCS0sLg4ODtLa2cujQIZLJJN988w0ul4tQKLRj8eVisidDLoSwsGHEfyOl/F1u\ns18IYZNSeoUQNiBg1kmlUimSyaQqM2Ukyzp69OiWpD1erxeXy0UwGGR6ehq/38/o6CjxeJxAILDv\nOVIzMVsTs9hsyI04WPj/aI9nlcl7EczUpKKigoGBAS5fvkxzczMNDQ3qby0tLQwNDbG2tva9kQO1\ntbUMDg4qZ/nT+ciz2Sx+v59AIMDw8DBzc3PcvXvXzHlRC5TuvbIThgM534a8WJoYjl2bzcbg4KDK\nari8vMzExAQul4twOFxycfR7iVoRwK+AaSnlLzf96ffA3wK/yL3/lxknlM1mGR0dZXV1lc7OTux2\nOzU1NdTU1LC+vk46nSYSieD1egmHwywuLhKPx/H5fMTjccLhsPIaF3mpuWmamImRR2NtbY3+/v5C\n12QsiCaNjY0MDg6qqZDdjI6RktSoFG9EHKTTabU24dq1azx69IjZ2VmCwaDZ8fXG06ck7xUDI7Ga\nxWKhtbUVv9+P1+tVkV95oiiaOJ1O3nrrLXp7e9UI78GDBywuLvL111/j8XhKcpHYXnrk54EfARNC\niHu5bT9jw4D/pxDiHeAR8FdmnFA2m+XLL7/kq6++YmBggK6uLmw2GzabjeXlZeLxODMzM9y8eZPV\n1VWSyeSutTuLSD8QxSRNzGRlZYXx8XHi8ThXrlwp5KELpklLS8u+wwaNHNRGfo3JyUk8Hg8fffSR\nevDlYXRnzYXamdZ+8oEQgsrKSiorKzlx4gTJZJJ4PJ43Q15MTU6cOME777yjEvKFQiHu37/PzMxM\nwVPT7oe9RK3cAHaLR/szc09nA8NJ5fP5yGQyKlG74Xzzer0qdvP7elxFZFJK+efFPomdyGazxGIx\nlpaWiEajxGKxQhUMMFWTbDbL7Owsw8PDvPbaa7S1taksl88ik8moNKNGr31paYnl5WXm5uaIRqPc\nv3+fYDCopqDyNLp7IKU8nY8dm4HFYsHpdNLa2orFYlEhrNFoNK9rEaSU3Xnb+TOIx+PMz8/jcDho\namoilUqp6lBFWn+xJ0pyZadhoF0uF263G9ga25zPedyDjhFCVVlZidfrxWq1Yrfbyy7PtJSSkZER\n5ubmePPNNzl16pRKjPYs0uk04XBYJUJLJpNMT08TCAT44osv8Pv9PHr0SI32XlaMghLd3d0q2Vwo\nFCIQCJgdzVQyhEIhbt26RSKRoL+/n0QiwdTUFAsLC6U26t9Cybfel6FYcCExpg/C4TDXr19nbm5O\nZYWsqKggnU4zNzdHIpEo6RtXSkkikUAIwezsrJrbTqfTNDY2YrPZVEnAlZUVIpEIqVSKJ0+eqHz1\nhhFPpVJ4PB5isRher5doNMrq6upLbcQBNVIJhUJks1ni8bhaCX1Q22M0GmVqakotnPP7/SpSpZSi\nVJ6m5A25xlwymYyaVnnvvfdUquDNGAteSr2xRiIR5fi2WCxMTEwwMTHB0NAQly5dorq6miNHjhCJ\nRLh37x6BQIDJyUkCgQC3b98mmUwSjUZVFIaRyU6P+DZYW1vj4cOHZLNZHA6HytF9kItOP378GJ/P\nR0VFBR988MG2LIelijbkLylGKuBy5ukiIcFgkPn5eSwWi6rgYiwzd7lcRCIRvv32W5aWlnjy5Amp\nVIpUKqWN9i6srKwwOTmJ3+/H7XaTTqcJBALKv3AQ2RwrX07tQxTyJi7mgoYCc2evTiytyXaeVxMj\nvaqRoS+3ry2xz0ZqVmOYXGQjvmdNoPD3ihCCqqoqtbLaqGmZ7zhyKeWek/3o9rOB7pFrDgzl2JMq\nZaSUZZNv/2WnPLPDazQajUahDblGo9GUOdqQazQaTZmjDblGo9GUOYV2doaAZO79IPAqO19L2z72\ncdA0gZ110Zq8mCZw8HTRmmznuWxKQcMPAYQQ/1vK+SX2g1nXcpA0AXOuR2uS3/2UAlqT7Tzvteip\nFY1GoylztCHXaDSaMqcYhvz9IhwzX5h1LQdJEzDnerQm+d1PKaA12c5zXUvB58g1Go1GYy56akWj\n0WjKnIIZciHEXwghZoUQ80KIdwt1XLMQQrQKIb4UQkwJIe4LIf4ht/3nQgiPEOJe7rWv+mnlrIvW\nZDtak53Jhy5ak01srpyerxdwCHABnUAlMA70FeLYJl6DDfjj3Oda4AHQB/wc+KeXURetidakWLpo\nTba+CtUj/xNgXkrpllKmgf8AfligY5uClNIrpRzLfY4D04D9BXdb1rpoTbajNdmZPOiiNdlEoQy5\nHVjc9P07XvzmLhpCiHbgB8Ct3KafCCG+EUL8WgjxR/vY1YHRRWuyHa3Jzpiki9ZkE9rZuU+EEEeB\n3wI/lVLGgH8FuoBBwAv8SxFPryhoTbajNdkZrct2zNCkUIbcA7Ru+u7IbSsrhBAWNgT/jZTydwBS\nSr+UMiOlzAL/xsaQb6+UvS5ak+1oTXbGZF20JpsolCG/DXQLITqEEJXAXwO/L9CxTUFsVCj+FTAt\npfzlpu22TT/7S2ByH7sta120JtvRmuxMHnTRmmyiINkPpZTrQoifAP/Nhrf511LK+4U4tomcB34E\nTAgh7uW2/Qz4GyHEICCBBeDv97rDA6CL1mQ7WpOdMVUXrclW9MpOjUajKXO0s1Oj0WjKHG3INRqN\npszRhlyj0WjKHG3INRqNpszRhlyj0WjKHG3INRqNpszRhlyj0WjKHG3INRqNpsz5P9pMsMzESwq5\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 5)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    ax[i].imshow(mnist.data[i].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZZTXAV2Bub7"
   },
   "source": [
    "The correct labels for these numbers are the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "IpvCtzOw7MAX",
    "outputId": "a291d28c-9687-4a9c-f234-bcc249334048"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '0', '4', '1', '9', '2', '1', '3', '1', '4'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asihMx7gBxpj"
   },
   "source": [
    "As always, we first split into training and testing sets. We cast the data into floats to preserve memory, and the targets into integers since they're currently strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "pPB7dMPx7C_S",
    "outputId": "bc7d926e-9d31-4f59-d48e-0acec0e59b45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52500, 784), (17500, 784), (52500,), (17500,))"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data.astype(np.float32), \n",
    "                                                    mnist.target.astype(np.int), \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mqbv_78T6Dkm"
   },
   "source": [
    "# Minibatch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48JTkvD2B4zL"
   },
   "source": [
    "From this point, we will implement *Minibatch* stochastic gradient descent. Minibatching refers to dividing a training set into multiple small batches, then iterating over these batches every epoch. Since we apply gradient descent per batch, we essentially allow the model more room to \"learn\" by giving it multiple batches instead of feeding it the entire dataset at once.\n",
    "\n",
    "This function yields a random batch from given inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMaOsSfAvme9"
   },
   "outputs": [],
   "source": [
    "def batchify(inputs, targets, batch_size):\n",
    "    indices = np.random.permutation(len(inputs))\n",
    "\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        batch = indices[start_idx:start_idx + batch_size]\n",
    "        yield inputs[batch], targets[batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUmjGCVdCQQq"
   },
   "source": [
    "Let's create dataloaders for the training and testing sets so we can iterate over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGc6E3O0vmhS"
   },
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_loader = [b for b in batchify(X_train, y_train, bs)]\n",
    "test_loader = [b for b in batchify(X_test, y_test, bs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jh2NUBoXCULc"
   },
   "source": [
    "We can check the first batch in a loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "p5VlBhup0PMJ",
    "outputId": "b622d804-a270-43e5-eb22-a51cdef6a44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32,)\n"
     ]
    }
   ],
   "source": [
    "x, y = train_loader[0]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "GJp8GCwEgXAG",
    "outputId": "aec2ca24-fdd6-49b6-f252-6dc95dc76e85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7, 8, 3, 0, 8, 9, 9, 7, 4, 2, 4, 7, 9, 2, 4, 5, 7, 9, 2, 2, 3,\n",
       "       6, 7, 9, 2, 8, 7, 4, 1, 9, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlL76uJ66Ffy"
   },
   "source": [
    "# Basic Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGVKDn9WCbcB"
   },
   "source": [
    "Let's implement a class for a neural network layer.\n",
    "\n",
    "We include the weights and biases of the layer and give it functions for forward and backward propagation.\n",
    "\n",
    "Forward propagation is straightforward. We apply a matrix multiplication then return the pre activation. We do not include the activation function within the class so we can do with the pre activations as we please.\n",
    "\n",
    "Backward propagation, while a little more involved, is also quite simple. We compute for the weighted errors (new deltas), then get the gradients by applying a dot product with the activations of the previous layer.\n",
    "\n",
    "We then apply a gradient descent step and return the new delta errors so we can pass this to the previous layer in the chain while backpropagating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8jIjZO8UkAX"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=3e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros(output_dim)\n",
    "        \n",
    "    def forward(self, activations):\n",
    "        return np.matmul(activations, self.weights) + self.biases\n",
    "      \n",
    "    def backward(self, grad_activations, delta):\n",
    "        # Compute weighted errors\n",
    "        new_delta = np.dot(delta, np.transpose(self.weights))\n",
    "\n",
    "        # Dot product with gradient of activations\n",
    "        grad_weights = np.transpose(np.dot(np.transpose(delta), grad_activations))\n",
    "        grad_biases = np.sum(delta, axis=0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return new_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2P0vOHPYDkNL"
   },
   "source": [
    "We implement only one activation function, which is the rectified linear unit. The structure follows the structure of our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zfvVYqCU2E3"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_activations, delta):\n",
    "        relu_grad = grad_activations > 0\n",
    "        new_delta = delta * relu_grad \n",
    "        \n",
    "        return new_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0VqhLd2Duvf"
   },
   "source": [
    "We can test out getting the pre activations using our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "nSr2uu3W0ijn",
    "outputId": "478b3b00-2d45-47bb-fc4c-dd7e54cdb356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n",
      "[[ 14.69895215  -5.61666568  -3.37829084 ...  12.41353805   0.59881915\n",
      "   16.86491563]\n",
      " [ 16.00085381 -15.25398307  -0.17245634 ... -36.91130359 -23.10192512\n",
      "  -11.97116324]\n",
      " [ -9.83528596 -30.70019511 -11.91590519 ...   3.92389971  -4.05055383\n",
      "   12.99285493]\n",
      " ...\n",
      " [ 24.08741678 -15.22455756  22.05522407 ... -14.21536057  19.9951475\n",
      "   11.62332957]\n",
      " [ -6.27261123  35.85141086   1.938583   ...  -5.74902129  -4.25501623\n",
      "   21.33150014]\n",
      " [  8.64883446 -28.72850325 -16.14439628 ...  15.24779078  21.82864814\n",
      "    6.29347996]]\n"
     ]
    }
   ],
   "source": [
    "fc1 = Linear(784, 128)\n",
    "relu = ReLU()\n",
    "\n",
    "z = fc1.forward(x)\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrOyr92gDyV8"
   },
   "source": [
    "And as expected, applying a relu will bound our pre activations between 0 and infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "moI3JcjA0il3",
    "outputId": "853905e4-cc9b-4304-be19-b799d135ade3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n",
      "[[14.69895215  0.          0.         ... 12.41353805  0.59881915\n",
      "  16.86491563]\n",
      " [16.00085381  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  3.92389971  0.\n",
      "  12.99285493]\n",
      " ...\n",
      " [24.08741678  0.         22.05522407 ...  0.         19.9951475\n",
      "  11.62332957]\n",
      " [ 0.         35.85141086  1.938583   ...  0.          0.\n",
      "  21.33150014]\n",
      " [ 8.64883446  0.          0.         ... 15.24779078 21.82864814\n",
      "   6.29347996]]\n"
     ]
    }
   ],
   "source": [
    "a = relu.forward(z)\n",
    "\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoYdblsK6IcK"
   },
   "source": [
    "# Softmax Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P19PLjXMD3F0"
   },
   "source": [
    "As for softmax, we do not implement it as a layer, but instead we incorporate it into our loss function. It is convention that we leave the final pre activations untouched so that we can use them should we need to (for example, chaining to another neural network or layer).\n",
    "\n",
    "Anyhow, we implement a cross entropy loss function like this. Since we're handling multiple labels, we take the predicted logits for each correct label, then compute for the entropy. We introduce an epsilon value to prevent the logarithm from encountering a zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXQv2u7TU7Dk"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, y):\n",
    "    # Get activations for correct answers (softmax), then compute cross-entropy \n",
    "    logits_for_labels = logits[np.arange(len(logits)), y]\n",
    "    entropy = -logits_for_labels + np.log(np.sum(np.exp(logits) + 1e-18, axis=-1))\n",
    "    \n",
    "    return np.mean(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDRCRVP-Ee7W"
   },
   "source": [
    "Likewise, we need to implement a version of the loss function for backpropagation. \n",
    "\n",
    "We subtract the softmaxed logits with the correct answers (which are expanded into a one-hot vector). This is our initial delta error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ma0Kr7s06MeV"
   },
   "outputs": [],
   "source": [
    "def grad_cross_entropy_loss(logits, y):\n",
    "    # One-hot encode the correct answers\n",
    "    one_hot = np.zeros_like(logits)\n",
    "    one_hot[np.arange(len(logits)), y] = 1\n",
    "    \n",
    "    # Get the softmax\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Initial delta is the output - correct answer. Divide to get the mean.\n",
    "    delta = (softmax - one_hot) / logits.shape[0]\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOTe-1VK6NHD"
   },
   "source": [
    "# Foward and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPHw1_2nEuQt"
   },
   "source": [
    "Let's write convenience functions for forward and back propagation.\n",
    "\n",
    "Forward propagation is simple. We apply the forward function for all layers in our network, which is a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFPUeCVUVNfd"
   },
   "outputs": [],
   "source": [
    "def forwardprop(network, x):\n",
    "    activations = []\n",
    "\n",
    "    # Pass through first layer\n",
    "    out = network[0].forward(x)\n",
    "    activations.append(out)\n",
    "\n",
    "    # Forward prop through all layers 1 to L\n",
    "    for i in range(1, len(network)):\n",
    "        out = network[i].forward(out)\n",
    "        activations.append(out)\n",
    "        \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tpwj1RBE2dI"
   },
   "source": [
    "For backprop, we iterate from the last layer. Each layer and the activations of the previous layer are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJlSyz_K1lC4"
   },
   "outputs": [],
   "source": [
    "def backprop(network, logits, y):\n",
    "    # Calculate the loss and initial delta\n",
    "    loss = cross_entropy_loss(logits, y)\n",
    "    delta = grad_cross_entropy_loss(logits, y)\n",
    "\n",
    "    # Propagate delta backwards\n",
    "    for i in range(1, len(network))[::-1]:\n",
    "        delta = network[i].backward(activations[i - 1], delta)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNf43Ek2E7hD"
   },
   "source": [
    "We also implement an accuracy function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7jc2DIz19eP"
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, y):\n",
    "    return np.sum(np.argmax(logits, 1) == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgdNdugjE-SR"
   },
   "source": [
    "Let's instantiate a simple one hidden layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwnJ8yc2VhBE"
   },
   "outputs": [],
   "source": [
    "network = [\n",
    "    Linear(784, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 10),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKJGsrCxFCKg"
   },
   "source": [
    "Then sample a batch from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bc_JSFxX2i0u"
   },
   "outputs": [],
   "source": [
    "x, y = train_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2QxaljMFEoR"
   },
   "source": [
    "This gives us one run of forward and back propagation (with one step of gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "r8ueduzHVhkR",
    "outputId": "d4156049-bba6-4465-f057-893f01881e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits: (32, 10)\n",
      "Loss 3.5702 | Accuracy 18.75%\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "activations = forwardprop(network, x)\n",
    "logits = activations[-1]\n",
    "print(\"Shape of logits:\", logits.shape)\n",
    "\n",
    "# Backpropagation\n",
    "loss = backprop(network, logits, y)\n",
    "acc = accuracy(logits, y)\n",
    "print(\"Loss {:.4f} | Accuracy {:.2f}%\".format(loss, acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZTeJzNmFJWw"
   },
   "source": [
    "Another forward and backward pass shows us that our network is improving as the loss is going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "KdF9cHdP7rUo",
    "outputId": "59c70247-db96-45db-ea2a-1c6cfcca5707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits: (32, 10)\n",
      "Loss 2.5923 | Accuracy 28.12%\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "activations = forwardprop(network, x)\n",
    "logits = activations[-1]\n",
    "print(\"Shape of logits:\", logits.shape)\n",
    "\n",
    "# Backpropagation\n",
    "loss = backprop(network, logits, y)\n",
    "acc = accuracy(logits, y)\n",
    "print(\"Loss {:.4f} | Accuracy {:.2f}%\".format(loss, acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TI4eZ5Xu6QB_"
   },
   "source": [
    "# Putting it all Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N5zu7B-8FNbh"
   },
   "source": [
    "Let's reinstantiate our neural network and set a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYEFcq_GgI-o"
   },
   "outputs": [],
   "source": [
    "network = [\n",
    "    Linear(784, 128, learning_rate=3e-5),\n",
    "    ReLU(),\n",
    "    Linear(128, 10, learning_rate=3e-5),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWgercG7FQPv"
   },
   "source": [
    "We then train for 10 epochs. We iterate through each minibatch. For each minibatch, we perform forward and backward propagation, including one step of gradient descent to update our parameters. We log our loss and average per batch. We then take the average of the losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "q5EQ-wcbnAPB",
    "outputId": "d9539c03-6c2b-48cb-b17c-9ee71927bba0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1226.51it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1729.08it/s]\n",
      "  8%|▊         | 130/1640 [00:00<00:01, 1298.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 1.1660 | Test Loss 0.7475 | Train Acc 63.60% | Test Acc 77.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1175.50it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1746.40it/s]\n",
      "  7%|▋         | 117/1640 [00:00<00:01, 1165.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 0.6588 | Test Loss 0.6145 | Train Acc 80.25% | Test Acc 81.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1219.39it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1719.55it/s]\n",
      "  8%|▊         | 130/1640 [00:00<00:01, 1299.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 0.5745 | Test Loss 0.5594 | Train Acc 82.83% | Test Acc 83.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1235.74it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1741.71it/s]\n",
      "  8%|▊         | 130/1640 [00:00<00:01, 1293.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 0.5326 | Test Loss 0.5275 | Train Acc 84.12% | Test Acc 84.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1218.18it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1714.64it/s]\n",
      "  8%|▊         | 130/1640 [00:00<00:01, 1292.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 0.5062 | Test Loss 0.5061 | Train Acc 84.87% | Test Acc 85.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1206.64it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1685.96it/s]\n",
      "  8%|▊         | 129/1640 [00:00<00:01, 1283.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   6 | Train Loss 0.4876 | Test Loss 0.4906 | Train Acc 85.43% | Test Acc 85.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1224.46it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1729.68it/s]\n",
      "  7%|▋         | 120/1640 [00:00<00:01, 1195.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Train Loss 0.4737 | Test Loss 0.4787 | Train Acc 85.84% | Test Acc 85.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1224.66it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1497.61it/s]\n",
      "  7%|▋         | 119/1640 [00:00<00:01, 1184.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   8 | Train Loss 0.4627 | Test Loss 0.4692 | Train Acc 86.14% | Test Acc 86.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1220.76it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1743.57it/s]\n",
      "  8%|▊         | 128/1640 [00:00<00:01, 1271.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   9 | Train Loss 0.4538 | Test Loss 0.4615 | Train Acc 86.39% | Test Acc 86.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1225.86it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 1719.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Train Loss 0.4464 | Test Loss 0.4551 | Train Acc 86.63% | Test Acc 86.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    # Training Loop\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "\n",
    "        activations = forwardprop(network, x)\n",
    "        logits = activations[-1]\n",
    "        loss = backprop(network, logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    # Testing Loop\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    for batch in tqdm(test_loader):\n",
    "        x, y = batch\n",
    "\n",
    "        activations = forwardprop(network, x)\n",
    "        logits = activations[-1]\n",
    "        loss = cross_entropy_loss(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL-ea3wlFe9P"
   },
   "source": [
    "We can test some predictions after we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNy0KwzG48w2"
   },
   "outputs": [],
   "source": [
    "x, y = test_loader[0]\n",
    "\n",
    "activations = forwardprop(network, x)\n",
    "logits = activations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEEC7wNPFgqp"
   },
   "source": [
    "Here are the predicted numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Nhj4g5t45022",
    "outputId": "810e2220-891e-45ea-bccf-100b0121acf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4, 9, 2, 1, 7, 0, 3, 0, 6, 3, 1, 1, 5, 7, 1, 1, 6, 7, 4, 4,\n",
       "       1, 5, 1, 5, 3, 9, 5, 7, 3, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(logits, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrjjctJbFiQL"
   },
   "source": [
    "And here are the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "IhpfVK-v51SG",
    "outputId": "9c0277b1-1c18-4e74-e81a-691cd1fbd0ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 4, 9, 2, 1, 7, 0, 5, 0, 6, 3, 1, 8, 5, 7, 1, 2, 6, 7, 7, 4,\n",
       "       1, 5, 1, 9, 3, 9, 5, 7, 3, 8])"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4y7QRGnG9Ld2"
   },
   "source": [
    "# From Numpy to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tv06OUAFm2I"
   },
   "source": [
    "*Adapted from the PyTorch Tutorials*\n",
    "\n",
    "While Numpy works well as a linear algebra library, we are starting to see its limits. For one, it is tedious to create larger and more complicated neural networks because we have to take note of each and every activation per layer, then handle backpropagation likewise.\n",
    "\n",
    "Here's a simple one hidden layer neural network that fits random toy data. We train for 500 epochs using simple MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "puDeiUWM54tl",
    "outputId": "a136a7d5-88a5-4f9f-dbca-9fd80ab41fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 207636.1345\n",
      "Epoch 100 | Loss 53474.6256\n",
      "Epoch 150 | Loss 18069.1752\n",
      "Epoch 200 | Loss 7089.4898\n",
      "Epoch 250 | Loss 3061.2206\n",
      "Epoch 300 | Loss 1408.1601\n",
      "Epoch 350 | Loss 678.4998\n",
      "Epoch 400 | Loss 338.8360\n",
      "Epoch 450 | Loss 174.0719\n",
      "Epoch 500 | Loss 91.5951\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "bs, input_dim, hidden_dim, output_dim = 32, 100, 128, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(bs, input_dim)\n",
    "y = np.random.randn(bs, output_dim)\n",
    "\n",
    "# Randomly initialize weights\n",
    "theta_l1 = np.random.randn(input_dim, hidden_dim)\n",
    "theta_l2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "epochs = 500\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    # Forward propagation\n",
    "    z_l1 = x.dot(theta_l1)\n",
    "    a_l1 = np.maximum(z_l1, 0)\n",
    "    y_pred = a_l1.dot(theta_l2)\n",
    "\n",
    "    # Compute for loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if e % 50 == 0:\n",
    "        print(\"Epoch {:3d} | Loss {:.4f}\".format(e, loss))\n",
    "\n",
    "    # Back propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_theta_l2 = a_l1.T.dot(grad_y_pred)\n",
    "    grad_a_l1 = grad_y_pred.dot(theta_l2.T)\n",
    "    grad_z_l1 = grad_a_l1.copy()\n",
    "    grad_z_l1[z_l1 < 0] = 0\n",
    "    grad_theta_l1 = x.T.dot(grad_z_l1)\n",
    "\n",
    "    # Update weights\n",
    "    theta_l1 -= learning_rate * grad_theta_l1\n",
    "    theta_l2 -= learning_rate * grad_theta_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rW8kXxOF5Yv"
   },
   "source": [
    "To improve upon this, we will use **PyTorch**.\n",
    "\n",
    "PyTorch is a neural networks library that essentially works like Numpy on steroids. Like numpy, it has linear algebra capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrQCKbbABr8E"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnkaP3YHGHwF"
   },
   "source": [
    "Here is the same example as the one above, but using PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "aon6XveV9tHE",
    "outputId": "743e3024-626b-4c9d-acf1-4370e1fc3faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 182762.8906\n",
      "Epoch 100 | Loss 43167.3906\n",
      "Epoch 150 | Loss 13680.2207\n",
      "Epoch 200 | Loss 5027.7749\n",
      "Epoch 250 | Loss 2028.1030\n",
      "Epoch 300 | Loss 874.4384\n",
      "Epoch 350 | Loss 396.3244\n",
      "Epoch 400 | Loss 186.7557\n",
      "Epoch 450 | Loss 90.7950\n",
      "Epoch 500 | Loss 45.2445\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "bs, input_dim, hidden_dim, output_dim = 32, 100, 128, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(bs, input_dim)\n",
    "y = torch.randn(bs, output_dim)\n",
    "\n",
    "# Randomly initialize weights\n",
    "theta_l1 = torch.randn(input_dim, hidden_dim)\n",
    "theta_l2 = torch.randn(hidden_dim, output_dim)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "epochs = 500\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    # Forward propagation\n",
    "    z_l1 = x.mm(theta_l1)\n",
    "    a_l1 = z_l1.clamp(min=0)\n",
    "    y_pred = a_l1.mm(theta_l2)\n",
    "\n",
    "    # Compute for loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if e % 50 == 0:\n",
    "        print(\"Epoch {:3d} | Loss {:.4f}\".format(e, loss))\n",
    "\n",
    "    # Back propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_theta_l2 = a_l1.t().mm(grad_y_pred)\n",
    "    grad_a_l1 = grad_y_pred.mm(theta_l2.t())\n",
    "    grad_z_l1 = grad_a_l1.clone()\n",
    "    grad_z_l1[z_l1 < 0] = 0\n",
    "    grad_theta_l1 = x.t().mm(grad_z_l1)\n",
    "\n",
    "    # Update weights\n",
    "    theta_l1 -= learning_rate * grad_theta_l1\n",
    "    theta_l2 -= learning_rate * grad_theta_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygK5IhhQGMEP"
   },
   "source": [
    "The main advantage of using PyTorch is that it has an **autodiff** engine. When we mark a tensor with ```requires_grad=True```, we are telling PyTorch to record all the operations that the tensor undergoes. This creates a computational graph in the background that records the operational history.\n",
    "\n",
    "Instead of manually taking note of every single activation and gradient ourselves, we can let PyTorch do this for us. When we reach our loss value, we can have PyTorch automatically backprop from that point, giving us the gradients with respect to the tensors we marked as  ```requires_grad=True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "pFJn_7GjA-OD",
    "outputId": "120b3ba8-c61d-471d-c0ee-6b327205cdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 182762.8906\n",
      "Epoch 100 | Loss 43167.3906\n",
      "Epoch 150 | Loss 13680.2207\n",
      "Epoch 200 | Loss 5027.7749\n",
      "Epoch 250 | Loss 2028.1030\n",
      "Epoch 300 | Loss 874.4384\n",
      "Epoch 350 | Loss 396.3244\n",
      "Epoch 400 | Loss 186.7557\n",
      "Epoch 450 | Loss 90.7950\n",
      "Epoch 500 | Loss 45.2445\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "bs, input_dim, hidden_dim, output_dim = 32, 100, 128, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(bs, input_dim)\n",
    "y = torch.randn(bs, output_dim)\n",
    "\n",
    "# Randomly initialize weights\n",
    "theta_l1 = torch.randn(input_dim, hidden_dim, requires_grad=True)\n",
    "theta_l2 = torch.randn(hidden_dim, output_dim, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "epochs = 500\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    # Forward propagation, chain all the operations\n",
    "    y_pred = x.mm(theta_l1).clamp(min=0).mm(theta_l2)\n",
    "\n",
    "    # Compute for loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if e % 50 == 0:\n",
    "        print(\"Epoch {:3d} | Loss {:.4f}\".format(e, loss))\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    with torch.no_grad():\n",
    "        theta_l1 -= learning_rate * theta_l1.grad\n",
    "        theta_l2 -= learning_rate * theta_l2.grad\n",
    "\n",
    "        # Zero-out gradients\n",
    "        theta_l1.grad.zero_()\n",
    "        theta_l2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKxH0Fd1G3zn"
   },
   "source": [
    "In combination with an autodiff engine, PyTorch also has a neural networks toolkit called ```torch.nn```. This includes wrappers for many different neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7tPIBMu9rfI"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULQEqP9HHF29"
   },
   "source": [
    "Instead of manually creating theta parameters and operations, we can instantiate a model with one hidden layer using the neural networks toolkit.\n",
    "\n",
    "This submodule also includes many widely-used loss functions.\n",
    "\n",
    "Let's instantiate the same network using the toolkit and add a loss function that we can use. For forward propagation, we only have to call our model as a function to get our activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "dh65JQfACEg8",
    "outputId": "b302b641-34b3-44ea-be2b-6c3897bb6f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 178.6263\n",
      "Epoch 100 | Loss 89.6067\n",
      "Epoch 150 | Loss 41.4231\n",
      "Epoch 200 | Loss 19.0851\n",
      "Epoch 250 | Loss 9.0797\n",
      "Epoch 300 | Loss 4.4719\n",
      "Epoch 350 | Loss 2.2714\n",
      "Epoch 400 | Loss 1.1820\n",
      "Epoch 450 | Loss 0.6270\n",
      "Epoch 500 | Loss 0.3390\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "bs, input_dim, hidden_dim, output_dim = 32, 100, 128, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(bs, input_dim)\n",
    "y = torch.randn(bs, output_dim)\n",
    "\n",
    "# Initialize layers and a loss function\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, output_dim)\n",
    ")\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 500\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    # Forward propagation\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute for loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if e % 50 == 0:\n",
    "        print(\"Epoch {:3d} | Loss {:.4f}\".format(e, loss))\n",
    "\n",
    "    # Zero-out the gradients then backpropagate\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58G9qAObHW1S"
   },
   "source": [
    "The last component is the optimizers submodule, which contains implementations of many different optimizers, such as gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYI2RlNo-VsE"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe1zXVzVHdJi"
   },
   "source": [
    "Here is the same example, but with an optimizer handling all the gradient descent upkeep. We instantiate a gradient descent optimizer and we pass the parameter list of our model. This tells the optimizer which parameters to update during every step of gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "SPfYZvsX-n6k",
    "outputId": "65e91d6a-892a-4e7c-e8fd-de6e23eab559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 178.6263\n",
      "Epoch 100 | Loss 89.6067\n",
      "Epoch 150 | Loss 41.4231\n",
      "Epoch 200 | Loss 19.0851\n",
      "Epoch 250 | Loss 9.0797\n",
      "Epoch 300 | Loss 4.4719\n",
      "Epoch 350 | Loss 2.2714\n",
      "Epoch 400 | Loss 1.1820\n",
      "Epoch 450 | Loss 0.6270\n",
      "Epoch 500 | Loss 0.3390\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "bs, input_dim, hidden_dim, output_dim = 32, 100, 128, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(bs, input_dim)\n",
    "y = torch.randn(bs, output_dim)\n",
    "\n",
    "# Initialize layers and a loss function\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, output_dim)\n",
    ")\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 500\n",
    "for e in range(1, epochs + 1):\n",
    "    # Forward propagation\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute for loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if e % 50 == 0:\n",
    "        print(\"Epoch {:3d} | Loss {:.4f}\".format(e, loss))\n",
    "\n",
    "    # Zero-out the gradients, backpropagate, then update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmpS4vltHiNN"
   },
   "source": [
    "Finally, we can create custom modules using PyTorch. This is the most common way that we create advanced neural networks in PyTorch.\n",
    "\n",
    "Here is our neural network as a module. We only have to add the layers as members and the module will keep track of the necessary parameters. We also have to implement a forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULLXgeiY_DBZ"
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4ApP7tRH0Oq"
   },
   "source": [
    "We can instantiate the model and use it like a sequential model, but with more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "0HVKQfp7-4MC",
    "outputId": "47914bc9-d45c-4c7c-bcc2-81830d0718af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 178.6263\n",
      "Epoch 100 | Loss 89.6067\n",
      "Epoch 150 | Loss 41.4231\n",
      "Epoch 200 | Loss 19.0851\n",
      "Epoch 250 | Loss 9.0797\n",
      "Epoch 300 | Loss 4.4719\n",
      "Epoch 350 | Loss 2.2714\n",
      "Epoch 400 | Loss 1.1820\n",
      "Epoch 450 | Loss 0.6270\n",
      "Epoch 500 | Loss 0.3390\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "bs, input_dim, hidden_dim, output_dim = 32, 100, 128, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(bs, input_dim)\n",
    "y = torch.randn(bs, output_dim)\n",
    "\n",
    "# Initialize layers and a loss function\n",
    "model = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 500\n",
    "for e in range(1, epochs + 1):\n",
    "    # Forward propagation\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute for loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if e % 50 == 0:\n",
    "        print(\"Epoch {:3d} | Loss {:.4f}\".format(e, loss))\n",
    "\n",
    "    # Zero-out the gradients, backpropagate, then update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JuNOupl_z3B"
   },
   "source": [
    "# MNIST in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIz_ktSgH-SZ"
   },
   "source": [
    "Let's train a neural network for the MNIST dataset using PyTorch.\n",
    "\n",
    "First, we resplit the data, then convert the numpy arrays into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "4arbEze6_Xgu",
    "outputId": "9fbf29ac-7424-4ce7-b437-7cc015cc8ecd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52500, 784]),\n",
       " torch.Size([17500, 784]),\n",
       " torch.Size([52500]),\n",
       " torch.Size([17500]))"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data.astype(np.float32), \n",
    "                                                    mnist.target.astype(np.int), \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test  = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test  = torch.from_numpy(y_test)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxjIfIK0IEpp"
   },
   "source": [
    "We import the data utilities submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLmrv2vSADb1"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKa7UFq6IG_f"
   },
   "source": [
    "This submodule allows easy batching. There are many ways to implement custom dataloaders for minibatching, but here is the simplest way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKJy-a7n_4WT"
   },
   "outputs": [],
   "source": [
    "# Dataloading\n",
    "bs = 32\n",
    "\n",
    "# Create tensor datasets\n",
    "train_data = data_utils.TensorDataset(X_train, y_train)\n",
    "test_data  = data_utils.TensorDataset(X_test,  y_test)\n",
    "\n",
    "# Pass the datasets and a batch size to create a loader\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=bs)\n",
    "test_loader  = data_utils.DataLoader(test_data,  batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv3BWQ1eIRZE"
   },
   "source": [
    "This functions the same way as a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "xE1LkedJACHN",
    "outputId": "fc947a80-27af-4893-fbbc-9d9eac658b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) torch.Size([32])\n",
      "1641\n"
     ]
    }
   ],
   "source": [
    "train_list = [b for b in train_loader]\n",
    "\n",
    "x, y = train_list[0]\n",
    "print(x.shape, y.shape)\n",
    "print(len(train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeFUimJmIURM"
   },
   "source": [
    "Here is the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "bVFdWjoRAKXV",
    "outputId": "686db1e3-35ec-4330-e5a9-b1568e4b4eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([3, 2, 8, 2, 3, 0, 3, 9, 9, 7, 3, 9, 5, 8, 7, 3, 1, 0, 3, 8, 7, 0, 8, 4,\n",
      "        1, 1, 0, 1, 0, 7, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V58NAKm8IVhb"
   },
   "source": [
    "Let's write our model again as a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aSDW3PVAMOJ"
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqOxXX3cIZKF"
   },
   "source": [
    "And write a new accuracy helper function the works with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuE5fbZWAVRe"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return torch.sum(torch.max(y_pred, 1)[1] == y).item() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t25US1nIceI"
   },
   "source": [
    "We instantiate a model, a loss function, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5LGtli4AX44"
   },
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(input_dim=784, hidden_dim=128, output_dim=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=6e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8-CUpnhIpUj"
   },
   "source": [
    "Then we train for 10 epochs again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "86Jc2Bc2AZaz",
    "outputId": "0e3cb865-9162-4705-b7b7-01db0df7067f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 724.82it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1637.44it/s]\n",
      "  5%|▍         | 75/1641 [00:00<00:02, 743.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 1.6859 | Test Loss 0.8272 | Train Acc 79.85% | Test Acc 86.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 737.75it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1671.26it/s]\n",
      "  5%|▍         | 75/1641 [00:00<00:02, 748.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 0.6125 | Test Loss 0.5884 | Train Acc 88.75% | Test Acc 89.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 754.46it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1677.58it/s]\n",
      "  5%|▍         | 76/1641 [00:00<00:02, 754.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 0.4395 | Test Loss 0.4819 | Train Acc 90.88% | Test Acc 90.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 748.51it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1601.24it/s]\n",
      "  5%|▍         | 75/1641 [00:00<00:02, 744.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Train Loss 0.3496 | Test Loss 0.4205 | Train Acc 92.18% | Test Acc 91.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 737.62it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1651.85it/s]\n",
      "  4%|▍         | 71/1641 [00:00<00:02, 703.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   5 | Train Loss 0.2936 | Test Loss 0.3788 | Train Acc 93.21% | Test Acc 91.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 746.59it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1671.67it/s]\n",
      "  5%|▍         | 76/1641 [00:00<00:02, 753.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   6 | Train Loss 0.2550 | Test Loss 0.3497 | Train Acc 93.89% | Test Acc 92.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 744.28it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1650.77it/s]\n",
      "  4%|▍         | 73/1641 [00:00<00:02, 724.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Train Loss 0.2260 | Test Loss 0.3286 | Train Acc 94.45% | Test Acc 92.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 734.83it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1610.40it/s]\n",
      "  5%|▍         | 78/1641 [00:00<00:02, 779.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   8 | Train Loss 0.2037 | Test Loss 0.3123 | Train Acc 94.88% | Test Acc 93.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 737.39it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1601.65it/s]\n",
      "  5%|▍         | 76/1641 [00:00<00:02, 752.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   9 | Train Loss 0.1859 | Test Loss 0.2998 | Train Acc 95.22% | Test Acc 93.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1641/1641 [00:02<00:00, 743.78it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 1486.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Train Loss 0.1712 | Test Loss 0.2893 | Train Acc 95.54% | Test Acc 93.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(y_pred, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    for batch in tqdm(test_loader):\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy(y_pred, y)\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3d} | Train Loss {:.4f} | Test Loss {:.4f} | Train Acc {:.2f}% | Test Acc {:.2f}%\".format(e, train_loss, test_loss, train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YV21A7nQIsOh"
   },
   "source": [
    "We can test a sample batch of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "kippptogAceY",
    "outputId": "2c8a6ba7-5742-45d6-83e1-32e8fb0a3d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.3389 | Accuracy 90.62%\n"
     ]
    }
   ],
   "source": [
    "x, y = train_list[10]\n",
    "\n",
    "y_pred = model(x)\n",
    "loss = criterion(y_pred, y)\n",
    "acc = accuracy(y_pred, y)\n",
    "\n",
    "print(\"Loss {:.4f} | Accuracy {:.2f}%\".format(loss.item(), acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdPKVJOXItQa"
   },
   "source": [
    "Here are the network's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "suOjFOkgAmlQ",
    "outputId": "fe54fa13-89ae-420d-8d71-f6dd8816861a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 7, 6, 0, 1, 0, 9, 5, 7, 9, 3, 0, 8, 6, 1, 3, 3, 7, 7, 9, 0, 0, 9, 3,\n",
       "        7, 7, 9, 3, 6, 7, 7, 5])"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(y_pred, 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E--gJNkQIynJ"
   },
   "source": [
    "And the correct labels for each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "TkqzVmbwAvUy",
    "outputId": "03e76636-5031-4ebb-8250-ac6e5f4df4d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 7, 6, 0, 1, 0, 9, 5, 7, 9, 5, 0, 8, 6, 1, 3, 3, 7, 7, 4, 0, 0, 9, 3,\n",
       "        7, 7, 9, 3, 6, 7, 7, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nONgrDO3AwgE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "07 - Introduction to Neural Networks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
