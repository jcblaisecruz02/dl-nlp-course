{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgcEZxT1u55K"
   },
   "source": [
    "# 13 - Introduction to Transfer Learning\n",
    "Prepared by Jan Christian Blaise Cruz\n",
    "\n",
    "DLSU Machine Learning Group\n",
    "\n",
    "In this simple demo notebook, we'll see how Transformers are used for Transfer Learning to downstream tasks after pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ck1-TZUu-tY"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHwCRh0mvG03"
   },
   "source": [
    "Make sure that you have a GPU. This notebook was tested with a Tesla P100 (16GB GPU). If you use a smaller GPU, make sure to adjust your batch sizes later to ensure that your data will fit in the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "e5XRghMdjOFg",
    "outputId": "720d6cb9-647e-4bc6-e4ad-2ee88075bd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 21 14:14:15 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVUfhMpDvRQT"
   },
   "source": [
    "Download the iMDB dataset, then install the HuggingFace Transformers package. This gives us a lot of prewritted wrappers and helper functions to load pretrained Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Udy77LIynHlP"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/imdb/imdb.zip\n",
    "!unzip imdb.zip && rm imdb.zip\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pj87bRUAvaK-"
   },
   "source": [
    "We'll use our usual imports, in addition to some new modules from the Transformers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6usRWbAkAt0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as datautils\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForMaskedLM, DistilBertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FS1-OIQnveEv"
   },
   "source": [
    "# Data Processing and Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTU4c0nrvgDE"
   },
   "source": [
    "Tokenization for Transformers is mainly done using a variant of BPE or byte-pair encoding. BERT in particular uses a variant called WordPiece. To load a pretrained tokenizer for a specific pretrained model, we'll use the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGyOehbWkZ69"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yl0GAVFrvsxX"
   },
   "source": [
    "Here's a sample sentence to display how the tokenizer handles sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnPkns-akc-M"
   },
   "outputs": [],
   "source": [
    "s = \"I liked all the jokes hahaha! Taadasafaunknowntoken.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xecl8uLmvwaF"
   },
   "source": [
    "Encode it into the vocabulary's indexes. Each tokenizer in HuggingFace Transformers has its own vocabulary that it uses internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PcCfMk9Gkevk",
    "outputId": "354e7a07-6f36-4a88-a29b-8054099bc2b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 146, 3851, 1155, 1103, 13948, 5871, 2328, 2328, 106, 22515, 7971, 3202, 8057, 12660, 2728, 6540, 18290, 1424, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(s)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjzhviIPv3Hi"
   },
   "source": [
    "Here's the tokenized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pjm7BOpfkiCd",
    "outputId": "78160d0c-3b9a-4145-92cb-fae38b649876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'I', 'liked', 'all', 'the', 'jokes', 'ha', '##ha', '##ha', '!', 'Ta', '##ada', '##sa', '##fa', '##unk', '##no', '##wn', '##tok', '##en', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode([idx]) for idx in out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktjtdzUUv5pD"
   },
   "source": [
    "BERT uses a pretraining scheme called Masked Language Modeling (MLM). We won't pretrained BERT in this notebook (it'd take months in one GPU to perform). Instead, we'll illustrate how MLM works.\n",
    "\n",
    "To load a pretrained model, we simply do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "FgBwIDPTkoYN",
    "outputId": "10bf8c6b-01e8-471c-a9ae-2ab984d88319"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Yq6aZYmwIJ-"
   },
   "source": [
    "MLM's task is to predict words under the [MASK] tokens. We'll encode a sentence here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Au-t1eyxlUsu",
    "outputId": "b0d740e9-b1e1-4144-9ec4-4c3ae2b4be38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'The', 'quick', 'brown', 'fox', '[MASK]', 'over', 'the', 'lazy', 'dog', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "s = \"The quick brown fox [MASK] over the lazy dog.\"\n",
    "input_tensor = tokenizer.encode(s)\n",
    "\n",
    "print([tokenizer.decode([idx]) for idx in input_tensor])\n",
    "\n",
    "input_tensor = torch.LongTensor(input_tensor).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAI04H4JwN95"
   },
   "source": [
    "Then pass it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOPi8vxplcwH"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(input_tensor)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL9hP0rKwPe7"
   },
   "source": [
    "This gives us our logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wZ9n6mZLlrk_",
    "outputId": "339a1880-41b1-4337-af39-12a40c4143f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 28996])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJcR9XlzwRSn"
   },
   "source": [
    "Let's see the predictions for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSys-767luo3"
   },
   "outputs": [],
   "source": [
    "preds = list(out.argmax(2).squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1AbrahPwTEM"
   },
   "source": [
    "We can see that the model filled in the mask with a verb that it thinks is a likely word for that mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a4JY4fy5lsjG",
    "outputId": "3e1aebd4-151a-4b67-fd84-72248b8ace43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'The', 'quick', 'brown', 'fox', 'loomed', 'over', 'the', 'lazy', 'dog', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode([idx]) for idx in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4ZhiatKwXkY"
   },
   "source": [
    "To train MLM, we simply have to optimize a cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsvEssr5mgpY"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z1DGYCHwbZW"
   },
   "source": [
    "We calculate loss the same way that we do for normal language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQBUNScXmzq5"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss = criterion(out.flatten(0, 1), input_tensor.flatten(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atKNsiyxwehs"
   },
   "source": [
    "Here's our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WcW_auphm9Bp",
    "outputId": "683ae74a-dda0-4383-88cb-4c21e0e3fbff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.252188205718994"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUyG4GzIyxmh"
   },
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHCX25uGwhV0"
   },
   "source": [
    "For this section, we'll finetune a pretrained DistilBERT model for sentiment classification on the iMDB dataset. Let's load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0Cq0-Uym-nZ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb/train.csv').sample(frac=1.0, random_state=42)\n",
    "text, labels = list(df['text']), list(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rovoXQ5awm6S"
   },
   "source": [
    "HuggingFace Transformers has a way to make tokenization and encoding simple in just one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KJM9021nWWS"
   },
   "outputs": [],
   "source": [
    "out = tokenizer(text[0], padding='max_length', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2J3d7YSwsM4"
   },
   "source": [
    "This gives us our input ids padded to a maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "9pytJRbWnb3B",
    "outputId": "909248bf-31bc-4cf8-e914-f78179706dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "[101, 2038, 1376, 11826, 119, 146, 1108, 7805, 1199, 2076, 1104, 10729, 5367, 2523, 1133, 1184, 146, 1400, 1108, 3600, 1603, 11826, 1115, 5671, 3839, 1104, 1412, 1159, 119, 2082, 10008, 1292, 5558, 1195, 1138, 1106, 1243, 1154, 1103, 1171, 2650, 2801, 1177, 1195, 1209, 1719, 1631, 12775, 1111, 1172, 1137, 11571, 1165, 1234, 1838, 2033, 1841, 119, 184, 1216, 14723, 1757, 1303, 119, 2160, 1128, 1267, 170, 1374, 2650, 1133, 1152, 1541, 1178, 12254, 1114, 1103, 3981, 1116, 119, 5723, 1112, 1103, 2252, 1676, 1120, 1103, 18976, 2133, 1395, 1108, 13825, 119, 1284, 1486, 1172, 1177, 1195, 1180, 1198, 1293, 7856, 1103, 6516, 1959, 1108, 1105, 1293, 1107, 11470, 19568, 1103, 1207, 4556, 9477, 1108, 119, 1284, 1267, 1103, 1376, 1873, 2566, 1272, 1131, 1209, 1138, 170, 1304, 1353, 1133, 1696, 1648, 1224, 1107, 1103, 2523, 1165, 1155, 26913, 7610, 5768, 119, 157, 3048, 1162, 6945, 1335, 5123, 26977, 1116, 1272, 1195, 1444, 1113, 1107, 2440, 1106, 1815, 1103, 4928, 3075, 119, 1109, 2213, 2564, 1107, 2440, 2993, 1123, 1107, 1103, 2150, 1104, 1103, 3043, 119, 1109, 14708, 2564, 1107, 1103, 3871, 1108, 1696, 1106, 1103, 2523, 1315, 119, 1109, 1178, 123, 2650, 1115, 1127, 1198, 126, 12119, 1116, 1114, 1185, 1329, 1106, 1103, 4928, 1127, 1103, 1160, 1685, 3713, 1113, 1103, 4261, 119, 157, 3048, 2980, 1108, 13336, 1272, 146, 1354, 1152, 1156, 1138, 1380, 1106, 1202, 1114, 4928, 119, 1622, 1103, 1148, 2741, 1106, 1103, 1314, 1103, 1590, 1959, 170, 1685, 3415, 3275, 1417, 6516, 1110, 1107, 2965, 119, 2431, 1165, 3160, 2196, 1117, 2276, 5769, 1131, 2144, 112, 189, 7011, 119, 1153, 6191, 1184, 1131, 1169, 1202, 1106, 15523, 1159, 119, 6291, 1168, 2523, 1103, 6866, 3275, 1535, 1156, 1129, 3176, 1176, 10696, 1116, 119, 1252, 1136, 1142, 1141, 119, 1135, 1108, 170, 1304, 1603, 2523, 1105, 146, 1108, 2613, 1111, 1103, 4400, 4928, 5197, 1106, 5642, 1107, 1272, 1103, 2523, 1882, 1106, 1129, 1909, 1106, 1157, 6593, 2698, 119, 4514, 2365, 1183, 3839, 1104, 1172, 1127, 1215, 119, 1109, 1207, 3415, 9477, 1225, 1136, 1202, 1103, 4400, 1270, 1105, 1500, 1123, 1184, 1106, 1202, 117, 1134, 1110, 7011, 3968, 1103, 2179, 1105, 1576, 1149, 1104, 1103, 3415, 1443, 2157, 1625, 117, 1137, 2304, 1240, 6054, 1105, 1587, 1123, 1131, 1125, 1106, 1277, 1106, 3668, 1105, 1198, 21728, 1123, 119, 3982, 24422, 1431, 1202, 1167, 1104, 1292, 3322, 1104, 5558, 119, 2907, 1141, 1314, 7368, 119, 3579, 9604, 1110, 1107, 1103, 2523, 1133, 146, 1125, 1136, 1141, 9956, 1150, 1119, 1108, 119, 146, 1125, 1106, 1435, 1166, 1303, 1106, 1267, 1115, 1119, 1110, 6516, 112, 188, 1401, 119, 1124, 1110, 2423, 8362, 1874, 2528, 22152, 22505, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(out['input_ids']))\n",
    "print(out['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKM1ygL8wvAW"
   },
   "source": [
    "This also gives us an attention mask as well as token type ids.\n",
    "\n",
    "The mask is used to remove attention values to the padding tokens. Token type ids are used to identify which sequence a token belongs to. In tasks like entailment where there are two input sequences, this helps the model identify which is sentence 1 and sentence 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "uzrfugIKoL_U",
    "outputId": "35475c70-9beb-4801-c01b-05a0667145a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(out['attention_mask'])\n",
    "print(out['token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7L1m-5IxCzy"
   },
   "source": [
    "We'll tokenize and encode the entire dataset. The backend for the tokenizers are written in Rust and are guaranteed to be much faster than writing it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rCJO9WgnpoqO",
    "outputId": "ec23a453-aab7-425f-b97c-a2708fc672f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 54.74s\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=512)\n",
    "print(\"Time elapsed: {:.2f}s\".format(time.time() - stime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fb5Rfvz4xIng"
   },
   "source": [
    "Here's the number of samples that are tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0J8AdwXTqpjb",
    "outputId": "87ea3b3f-7bcf-4100-aef8-7a28f731c1ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yDSAUh6xKmc"
   },
   "source": [
    "As usual, we'll split them into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGQXXByIq9Lc"
   },
   "outputs": [],
   "source": [
    "tr_sz = int(len(text) * 0.7) \n",
    "\n",
    "X_train_input, X_train_mask, X_train_types = tokenized['input_ids'][:tr_sz], tokenized['attention_mask'][:tr_sz], tokenized['token_type_ids'][:tr_sz]\n",
    "X_valid_input, X_valid_mask, X_valid_types = tokenized['input_ids'][tr_sz:], tokenized['attention_mask'][tr_sz:], tokenized['token_type_ids'][tr_sz:]\n",
    "y_train, y_valid = labels[:tr_sz], labels[tr_sz:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKD2aOD8xNE2"
   },
   "source": [
    "Convert them to PyTorch tensors afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd3qVBOZrUs0"
   },
   "outputs": [],
   "source": [
    "X_train_input, X_train_mask, X_train_types = torch.LongTensor(X_train_input), torch.LongTensor(X_train_mask), torch.LongTensor(X_train_types)\n",
    "X_valid_input, X_valid_mask, X_valid_types = torch.LongTensor(X_valid_input), torch.LongTensor(X_valid_mask), torch.LongTensor(X_valid_types)\n",
    "y_train, y_valid = torch.LongTensor(y_train), torch.LongTensor(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbkygKIMxO_y"
   },
   "source": [
    "Then make dataloaders.\n",
    "\n",
    "Make sure to adjust your batch size depending on the memory capacity of your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0R5j_cB_u6Nu"
   },
   "outputs": [],
   "source": [
    "bs = 16\n",
    "\n",
    "train_set = datautils.TensorDataset(X_train_input, X_train_mask, X_train_types, y_train)\n",
    "valid_set = datautils.TensorDataset(X_valid_input, X_valid_mask, X_valid_types, y_valid)\n",
    "\n",
    "train_sampler = datautils.RandomSampler(train_set)\n",
    "train_loader = datautils.DataLoader(train_set, batch_size=bs, sampler=train_sampler)\n",
    "valid_loader = datautils.DataLoader(valid_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKyWHTcSxV4Z"
   },
   "source": [
    "Here's one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X_RiJW5ex-Dm",
    "outputId": "d9e2d8c6-2646-4a02-aca8-c75c34a9fecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "x_in, x_ma, x_ty, y = next(iter(train_loader))\n",
    "print(x_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWS3Jbk_xX17"
   },
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0t7ipf5xZ_D"
   },
   "source": [
    "Finetuning for a task is simple, we simply have to affix a head to the pretrained model. HuggingFace Transformers provides prewritten modules that are essentially pretrained model + head. We'll use a ```BertForSequenceClassification``` for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ssCLCgX-yNwB",
    "outputId": "ead74874-26f3-450c-9ad0-69c1529610df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12jwlQ2lxm8x"
   },
   "source": [
    "Passing our inputs to our model will give us our logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuCoLppRyTkf"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(input_ids=x_in, attention_mask=x_ma, token_type_ids=x_ty)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGzhHVMzxpdm"
   },
   "source": [
    "Here's the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NxOJD6SVydyK",
    "outputId": "4e392678-5ba1-444a-8468-fc9769d0aaf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMot0DJ2xs2q"
   },
   "source": [
    "Argmaxing on the first dimension gives us the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hRM-QCEZyeJ5",
    "outputId": "e37480bc-6118-4177-d483-447de7b48b0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QiinsdIrxvnW"
   },
   "source": [
    "For context, here are the correct answers.\n",
    "\n",
    "At this stage, the model's head still isn't trained, even if the transformer itself has been pretrained. We need to finetune this model in order to induce the correct biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TZ3ZdbWDzSsz",
    "outputId": "6264a00c-e378-4960-a5ad-a6f5f62c0bd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XlDG1cbx3dB"
   },
   "source": [
    "Loss is similar to how we compute for loss in the RNN-based sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Em8SCwybzWSr",
    "outputId": "8aa59915-2e1b-428d-b3ad-2d84ea30397f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6767595410346985\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = criterion(out, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgODig4P1Buq"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5_cxWgGx833"
   },
   "source": [
    "Here's an accuracy function to help us calculate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZOzKc6M1PcU"
   },
   "outputs": [],
   "source": [
    "def accuracy(out, y): \n",
    "    with torch.no_grad():\n",
    "        return torch.mean((out.argmax(1) == y).float()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YYNlE9tyBD9"
   },
   "source": [
    "To conserve on GPU space, we'll use a DistilBERT model instead of a full BERT model. This is a compressed version of BERT that's smaller and lighter. We'll discuss model compression in a future session. For all intents and purposes, this performs the same role as standard BERT, but it's just smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "l5z0MJmy0SNx",
    "outputId": "632a1502-7c6c-42bd-895f-0e44fda7fa19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFQqX5lJyM01"
   },
   "source": [
    "We'll set the layers that will have weight decay, set the epochs, optimizer settings, and scheduler settings. \n",
    "\n",
    "We're using a variant of Adam that accepts weight decay, and use a linear schedule with warmup. This warms up the scheduler from 0 to the top learning rate for the first 10% of training steps, then linearly decay to 0 from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEUlnN9dzcN7"
   },
   "outputs": [],
   "source": [
    "weight_decay = 1e-8\n",
    "learning_rate = 5e-5\n",
    "epochs = 3\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": weight_decay},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * steps), num_training_steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DV6Hiqveyaih"
   },
   "source": [
    "Then train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "cYnpVCQ11ELb",
    "outputId": "75f5d4a1-db44-4e6f-85b2-5e04b1a2d2f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [07:53<00:00,  2.31it/s]\n",
      "100%|██████████| 469/469 [01:06<00:00,  7.06it/s]\n",
      "  0%|          | 0/1094 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Train Loss 0.3226 | Train Acc 0.8561 | Valid Loss 0.2269 | Valid Acc 0.9147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [07:54<00:00,  2.31it/s]\n",
      "100%|██████████| 469/469 [01:06<00:00,  7.06it/s]\n",
      "  0%|          | 0/1094 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   2 | Train Loss 0.1479 | Train Acc 0.9452 | Valid Loss 0.2260 | Valid Acc 0.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [07:54<00:00,  2.31it/s]\n",
      "100%|██████████| 469/469 [01:06<00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   3 | Train Loss 0.0435 | Train Acc 0.9874 | Valid Loss 0.2888 | Valid Acc 0.9221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, epochs + 1):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for x_in, x_ma, x_ty, y in tqdm(train_loader):\n",
    "        x_in, x_ma, y = x_in.to(device), x_ma.to(device), y.to(device)\n",
    "\n",
    "        out = model(input_ids=x_in, attention_mask=x_ma)[0]\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(out, y)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_in, x_ma, x_ty, y in tqdm(valid_loader):\n",
    "            x_in, x_ma, y = x_in.to(device), x_ma.to(device), y.to(device)\n",
    "\n",
    "            out = model(input_ids=x_in, attention_mask=x_ma)[0]\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += accuracy(out, y)\n",
    "    valid_loss /= len(valid_loader)\n",
    "    valid_acc /= len(valid_loader)\n",
    "\n",
    "    print(\"\\nEpoch {:3} | Train Loss {:.4f} | Train Acc {:.4f} | Valid Loss {:.4f} | Valid Acc {:.4f}\".format(e, train_loss, train_acc, valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBOoLGOEyb9H"
   },
   "source": [
    "Our final validation accuracy is at 92%, which is way higher than our previous benchmark using RNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IG23GU23H7z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "10 - Transfer Learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
